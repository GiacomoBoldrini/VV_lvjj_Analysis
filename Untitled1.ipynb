{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.1\n",
    "\n",
    "def f(x, noise_level=noise_level):\n",
    "    return np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2)) + np.random.randn() * noise_level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot f(x) + contours\n",
    "x = np.linspace(-2, 2, 400).reshape(-1, 1)\n",
    "fx = [f(x_i, noise_level=0.0) for x_i in x]\n",
    "plt.plot(x, fx, \"r--\", label=\"True (unknown)\")\n",
    "plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate(([fx_i - 1.9600 * noise_level for fx_i in fx], \n",
    "                         [fx_i + 1.9600 * noise_level for fx_i in fx[::-1]])),\n",
    "         alpha=.2, fc=\"r\", ec=\"None\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "\n",
    "res = gp_minimize(f,                  # the function to minimize\n",
    "                  [(-2.0, 2.0)],      # the bounds on each dimension of x\n",
    "                  acq_func=\"EI\",      # the acquisition function\n",
    "                  n_calls=15,         # the number of evaluations of f \n",
    "                  n_random_starts=5,  # the number of random initialization points\n",
    "                  noise=0.1**2,       # the noise level (optional)\n",
    "                  random_state=123)   # the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x^*=-0.2573, f(x^*)=-1.0715'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"x^*=%.4f, f(x^*)=%.4f\" % (res.x[0], res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          fun: -1.0714985913973778\n",
      "    func_vals: array([-0.18788762, -0.96229886, -0.34643484, -0.46587165, -0.1773319 ,\n",
      "       -0.80809029, -1.07149859, -0.92120939, -0.1608395 , -0.80998886,\n",
      "       -0.32494081,  0.03291131, -0.02279617,  0.05849308,  0.69296707])\n",
      "       models: [GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5) + WhiteKernel(noise_level=0.01),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734)]\n",
      " random_state: <mtrand.RandomState object at 0x7f8623425a20>\n",
      "        space: Space([Real(low=-2.0, high=2.0, prior='uniform', transform='normalize', name='X_0')])\n",
      "        specs: {'args': {'n_jobs': 1, 'kappa': 1.96, 'xi': 0.01, 'n_restarts_optimizer': 5, 'n_points': 10000, 'callback': None, 'verbose': False, 'random_state': <mtrand.RandomState object at 0x7f8623425a20>, 'y0': None, 'x0': None, 'acq_optimizer': 'auto', 'acq_func': 'EI', 'n_random_starts': 5, 'n_calls': 15, 'base_estimator': GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * Matern(length_scale=1, nu=2.5),\n",
      "             n_restarts_optimizer=2, noise=0.010000000000000002,\n",
      "             normalize_y=True, optimizer='fmin_l_bfgs_b',\n",
      "             random_state=843828734), 'dimensions': Space([Real(low=-2.0, high=2.0, prior='uniform', transform='normalize', name='X_0')]), 'func': <function f at 0x7f863188f840>}, 'function': 'base_minimize'}\n",
      "            x: [-0.25728223287645147]\n",
      "      x_iters: [[0.8518212820929092], [-0.2861162952526968], [0.7635394201074472], [0.8766012406190926], [-0.03552426626961047], [-0.30626086081540094], [-0.25728223287645147], [-0.22378966624613694], [1.0553495088893112], [-0.25562401609058916], [-0.5244265425661685], [-1.9991519430001499], [1.999866281741757], [-1.2339044799440428], [0.45960221844046245]]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(res);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giacomo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/giacomo/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten, Dropout\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn import metrics\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interp\n",
    "import ModelTemplates as MT\n",
    "from keras.callbacks import *\n",
    "from copy import deepcopy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xy(df, target):\n",
    "    y = df[:,target]\n",
    "    x = np.delete(df, target, 1)\n",
    "    return x,y\n",
    "\n",
    "def mkdir_p(mypath):\n",
    "    #crea una directory\n",
    "\n",
    "    from errno import EEXIST\n",
    "    from os import makedirs,path\n",
    "\n",
    "    try:\n",
    "        makedirs(mypath)\n",
    "    except OSError as exc: \n",
    "        if exc.errno == EEXIST and path.isdir(mypath):\n",
    "            pass\n",
    "        else: raise\n",
    "    \n",
    "def model_score(y_test, pred, th):\n",
    "    print(\">>>>Computing score th = {}\".format(th))\n",
    "    i=0\n",
    "    while i<len(pred):\n",
    "        if pred[i]>th:\n",
    "            pred[i]=1;\n",
    "        else:\n",
    "            pred[i]=0;\n",
    "        i=i+1\n",
    "    return metrics.accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### DIMENSION OF SEARCHING SPACE ############################\n",
    "\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "\n",
    "dim_num_dense_layers = Integer(low=1, high=8, name='num_dense_layers')\n",
    "\n",
    "dim_num_dense_nodes = Integer(low=1, high=250, name='num_dense_nodes')\n",
    "\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid', 'tanh'],\n",
    "                             name='activation')\n",
    "\n",
    "dim_dropout = Real(low= 0.0, high= 1.0, name='dropout_rate')\n",
    "\n",
    "dim_decay = Real(low= 1e-9, high= 10, name='decay_rate')\n",
    "\n",
    "dim_batch = Integer(low= 100, high= 50000, name='batch_dimension')\n",
    "\n",
    "\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation,\n",
    "              dim_dropout,\n",
    "              dim_decay,\n",
    "              dim_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 16)\n",
      "(100000, 15)\n",
      "(100000,)\n",
      ">>>count0 15526\n",
      ">>>count1 84474\n",
      ">>>sum 100000\n",
      ">>>rateo 0.18379619764661315\n"
     ]
    }
   ],
   "source": [
    "###################### INPUT DATA, ACC, PATHS ############################\n",
    "\n",
    "data = np.load(\"/home/giacomo/tesi1/VBSAnalysis/Dataset4.npy\")\n",
    "random.shuffle(data)\n",
    "data = data[:100000][:]\n",
    "print(data.shape)\n",
    "input_shape = data.shape[1] -1 \n",
    "x,y = to_xy(data, 15)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "for i in y:\n",
    "    if i == 0:\n",
    "        count0 +=1\n",
    "    else:\n",
    "        count1 +=1\n",
    "print(\">>>count0 {}\".format(count0))\n",
    "print(\">>>count1 {}\".format(count1))\n",
    "print(\">>>sum {}\".format(count0+count1))\n",
    "print(\">>>rateo {}\".format(count0/count1))\n",
    "    \n",
    "path_best_model = '/home/giacomo/tesi1/DNN_test/best_model'\n",
    "best_accuracy = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modeloptimizer(learning_rate, num_dense_layers, num_dense_nodes, activation, dropout_rate, decay_rate, batch_dimension):\n",
    "    print(\">>> Creating model...\")\n",
    "    model = Sequential()\n",
    "    print(\">>> input dim {}\".format(input_shape))\n",
    "    model.add(Dense(units = 30 ,input_dim=input_shape, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(num_dense_layers):\n",
    "        # Name of the layer. This is not really necessary\n",
    "        # because Keras should give them unique names.\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        # Add the dense / fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes and the activation function.\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=activation,\n",
    "                        name=name))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    #last layer for categorical crossentropy must be one, activation sigmoid\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    optimizer = Adam(lr=learning_rate, decay = decay_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### FITNESS FUNCTION FOR BEST ACC ############################\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers,\n",
    "            num_dense_nodes, activation, dropout_rate, decay_rate, batch_dimension):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print('dropout_rate:', dropout_rate)\n",
    "    print('decay rate:', decay_rate)\n",
    "    print('batch dimension:', batch_dimension)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(y),y)\n",
    "    kf = KFold(3)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=20, verbose=1, mode='auto', baseline=None)\n",
    "    fold = 0\n",
    "    oos_y = []\n",
    "    oos_pred = []\n",
    "    print(kf)\n",
    "    for train,test in kf.split(x):\n",
    "        fold+=1\n",
    "        print(\" Fold #{}\".format(fold))\n",
    "        #csv_logger = CSVLogger(output_dir1 +'/training_{0}.log'.format(fold))\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "        model = Modeloptimizer(learning_rate=learning_rate, num_dense_layers=num_dense_layers,num_dense_nodes=num_dense_nodes,activation=activation, dropout_rate = dropout_rate, decay_rate=decay_rate, batch_dimension= batch_dimension)\n",
    "        history = model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=1, epochs = 1000, batch_size=batch_dimension, class_weight=class_weights,shuffle=True,\n",
    "                            callbacks=[early_stop])\n",
    "        predi = model.predict(x_test)\n",
    "        oos_y.append(y_test)\n",
    "        oos_pred.append(predi)\n",
    "        \n",
    "    oos_y = np.concatenate(oos_y)\n",
    "    os_pred = np.concatenate(oos_pred)\n",
    "    oos_pred = np.concatenate(oos_pred)\n",
    "    \n",
    "    th = 0.3\n",
    "    score = []\n",
    "    while th < 1:\n",
    "        toscore = deepcopy(oos_pred)\n",
    "        z = model_score(oos_y, toscore, th)\n",
    "        score.append([z, th])\n",
    "        th = th + 0.1\n",
    "        th = round(th, 1)\n",
    "    \n",
    "    #We define max score for some threshold. It's not important which threshold it is\n",
    "    score = sorted(score, reverse = True)\n",
    "\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = score[0][0]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print(score)\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print(\"Threshold: {0}\".format(score[0][1]))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        #model.save(path_best_model+ \"/best_model_{0:.4f}\".format(accuracy))\n",
    "        \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 10\n",
      "activation: relu\n",
      "dropout_rate: 0.0\n",
      "decay rate: 1e-05\n",
      "batch dimension: 1024\n",
      "\n",
      "KFold(n_splits=3, random_state=None, shuffle=False)\n",
      " Fold #1\n",
      ">>> Creating model...\n",
      ">>> input dim 15\n",
      "Train on 66666 samples, validate on 33334 samples\n",
      "Epoch 1/1000\n",
      "66666/66666 [==============================] - 1s 10us/step - loss: 0.8494 - acc: 0.7853 - val_loss: 0.5425 - val_acc: 0.8091\n",
      "Epoch 2/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.4870 - acc: 0.8157 - val_loss: 0.4374 - val_acc: 0.8432\n",
      "Epoch 3/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.4181 - acc: 0.8305 - val_loss: 0.3914 - val_acc: 0.8444\n",
      "Epoch 4/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3938 - acc: 0.8365 - val_loss: 0.3742 - val_acc: 0.8457\n",
      "Epoch 5/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3817 - acc: 0.8400 - val_loss: 0.3655 - val_acc: 0.8473\n",
      "Epoch 6/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3745 - acc: 0.8407 - val_loss: 0.3608 - val_acc: 0.8466\n",
      "Epoch 7/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3682 - acc: 0.8423 - val_loss: 0.3551 - val_acc: 0.8483\n",
      "Epoch 8/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3674 - acc: 0.8434 - val_loss: 0.3622 - val_acc: 0.8492\n",
      "Epoch 9/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3636 - acc: 0.8438 - val_loss: 0.3534 - val_acc: 0.8492\n",
      "Epoch 10/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3601 - acc: 0.8440 - val_loss: 0.3498 - val_acc: 0.8483\n",
      "Epoch 11/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3595 - acc: 0.8443 - val_loss: 0.3479 - val_acc: 0.8484\n",
      "Epoch 12/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3583 - acc: 0.8444 - val_loss: 0.3472 - val_acc: 0.8480\n",
      "Epoch 13/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3572 - acc: 0.8450 - val_loss: 0.3479 - val_acc: 0.8484\n",
      "Epoch 14/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3541 - acc: 0.8455 - val_loss: 0.3579 - val_acc: 0.8456\n",
      "Epoch 15/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3545 - acc: 0.8449 - val_loss: 0.3396 - val_acc: 0.8510\n",
      "Epoch 16/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3508 - acc: 0.8459 - val_loss: 0.3410 - val_acc: 0.8508\n",
      "Epoch 17/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3502 - acc: 0.8465 - val_loss: 0.3382 - val_acc: 0.8508\n",
      "Epoch 18/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3482 - acc: 0.8475 - val_loss: 0.3458 - val_acc: 0.8497\n",
      "Epoch 19/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3501 - acc: 0.8471 - val_loss: 0.3424 - val_acc: 0.8511\n",
      "Epoch 20/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3495 - acc: 0.8470 - val_loss: 0.3385 - val_acc: 0.8531\n",
      "Epoch 21/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3473 - acc: 0.8470 - val_loss: 0.3361 - val_acc: 0.8545\n",
      "Epoch 22/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3457 - acc: 0.8481 - val_loss: 0.3294 - val_acc: 0.8547\n",
      "Epoch 23/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3412 - acc: 0.8500 - val_loss: 0.3347 - val_acc: 0.8535\n",
      "Epoch 24/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3408 - acc: 0.8495 - val_loss: 0.3433 - val_acc: 0.8520\n",
      "Epoch 25/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3432 - acc: 0.8503 - val_loss: 0.3314 - val_acc: 0.8542\n",
      "Epoch 26/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3370 - acc: 0.8513 - val_loss: 0.3227 - val_acc: 0.8577\n",
      "Epoch 27/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3364 - acc: 0.8523 - val_loss: 0.3209 - val_acc: 0.8582\n",
      "Epoch 28/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3363 - acc: 0.8525 - val_loss: 0.3243 - val_acc: 0.8608\n",
      "Epoch 29/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3327 - acc: 0.8543 - val_loss: 0.3176 - val_acc: 0.8610\n",
      "Epoch 30/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3321 - acc: 0.8548 - val_loss: 0.3171 - val_acc: 0.8606\n",
      "Epoch 31/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3303 - acc: 0.8547 - val_loss: 0.3368 - val_acc: 0.8586\n",
      "Epoch 32/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3285 - acc: 0.8561 - val_loss: 0.3168 - val_acc: 0.8610\n",
      "Epoch 33/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3267 - acc: 0.8572 - val_loss: 0.3122 - val_acc: 0.8631\n",
      "Epoch 34/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3248 - acc: 0.8578 - val_loss: 0.3129 - val_acc: 0.8646\n",
      "Epoch 35/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3235 - acc: 0.8578 - val_loss: 0.3113 - val_acc: 0.8640\n",
      "Epoch 36/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3264 - acc: 0.8569 - val_loss: 0.3224 - val_acc: 0.8585\n",
      "Epoch 37/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8579 - val_loss: 0.3073 - val_acc: 0.8659\n",
      "Epoch 38/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3233 - acc: 0.8579 - val_loss: 0.3124 - val_acc: 0.8635\n",
      "Epoch 39/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3239 - acc: 0.8578 - val_loss: 0.3129 - val_acc: 0.8630\n",
      "Epoch 40/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3182 - acc: 0.8599 - val_loss: 0.3059 - val_acc: 0.8655\n",
      "Epoch 41/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3191 - acc: 0.8595 - val_loss: 0.3156 - val_acc: 0.8599\n",
      "Epoch 42/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3190 - acc: 0.8594 - val_loss: 0.3204 - val_acc: 0.8649\n",
      "Epoch 43/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3170 - acc: 0.8605 - val_loss: 0.3028 - val_acc: 0.8672\n",
      "Epoch 44/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3133 - acc: 0.8629 - val_loss: 0.3023 - val_acc: 0.8673\n",
      "Epoch 45/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3145 - acc: 0.8620 - val_loss: 0.3348 - val_acc: 0.8551\n",
      "Epoch 46/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3159 - acc: 0.8608 - val_loss: 0.3191 - val_acc: 0.8640\n",
      "Epoch 47/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3144 - acc: 0.8624 - val_loss: 0.3029 - val_acc: 0.8659\n",
      "Epoch 48/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3137 - acc: 0.8619 - val_loss: 0.3105 - val_acc: 0.8678\n",
      "Epoch 49/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3131 - acc: 0.8629 - val_loss: 0.3032 - val_acc: 0.8668\n",
      "Epoch 50/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3135 - acc: 0.8616 - val_loss: 0.3034 - val_acc: 0.8701\n",
      "Epoch 51/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3111 - acc: 0.8625 - val_loss: 0.3094 - val_acc: 0.8648\n",
      "Epoch 52/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3126 - acc: 0.8621 - val_loss: 0.2981 - val_acc: 0.8696\n",
      "Epoch 53/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3093 - acc: 0.8633 - val_loss: 0.2954 - val_acc: 0.8715\n",
      "Epoch 54/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3071 - acc: 0.8645 - val_loss: 0.2973 - val_acc: 0.8696\n",
      "Epoch 55/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3078 - acc: 0.8636 - val_loss: 0.3041 - val_acc: 0.8710\n",
      "Epoch 56/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3091 - acc: 0.8640 - val_loss: 0.2933 - val_acc: 0.8726\n",
      "Epoch 57/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3084 - acc: 0.8643 - val_loss: 0.2936 - val_acc: 0.8725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3076 - acc: 0.8630 - val_loss: 0.3048 - val_acc: 0.8655\n",
      "Epoch 59/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3083 - acc: 0.8631 - val_loss: 0.2938 - val_acc: 0.8715\n",
      "Epoch 60/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3056 - acc: 0.8638 - val_loss: 0.2966 - val_acc: 0.8729\n",
      "Epoch 61/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3043 - acc: 0.8648 - val_loss: 0.2947 - val_acc: 0.8728\n",
      "Epoch 62/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3104 - acc: 0.8626 - val_loss: 0.2940 - val_acc: 0.8697\n",
      "Epoch 63/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3031 - acc: 0.8653 - val_loss: 0.3176 - val_acc: 0.8658\n",
      "Epoch 64/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3074 - acc: 0.8630 - val_loss: 0.2900 - val_acc: 0.8728\n",
      "Epoch 65/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.3026 - acc: 0.8646 - val_loss: 0.2927 - val_acc: 0.8719\n",
      "Epoch 66/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.3049 - acc: 0.8643 - val_loss: 0.2908 - val_acc: 0.8713\n",
      "Epoch 67/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3020 - acc: 0.8646 - val_loss: 0.2934 - val_acc: 0.8709\n",
      "Epoch 68/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3050 - acc: 0.8640 - val_loss: 0.3113 - val_acc: 0.8674\n",
      "Epoch 69/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3036 - acc: 0.8653 - val_loss: 0.3089 - val_acc: 0.8642\n",
      "Epoch 70/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.3020 - acc: 0.8661 - val_loss: 0.2919 - val_acc: 0.8727\n",
      "Epoch 71/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3019 - acc: 0.8652 - val_loss: 0.2873 - val_acc: 0.8730\n",
      "Epoch 72/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2994 - acc: 0.8660 - val_loss: 0.2875 - val_acc: 0.8746\n",
      "Epoch 73/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2991 - acc: 0.8657 - val_loss: 0.2982 - val_acc: 0.8673\n",
      "Epoch 74/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.3023 - acc: 0.8650 - val_loss: 0.2947 - val_acc: 0.8717\n",
      "Epoch 75/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3031 - acc: 0.8647 - val_loss: 0.2921 - val_acc: 0.8690\n",
      "Epoch 76/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2993 - acc: 0.8656 - val_loss: 0.2861 - val_acc: 0.8751\n",
      "Epoch 77/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2984 - acc: 0.8656 - val_loss: 0.2895 - val_acc: 0.8701\n",
      "Epoch 78/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3003 - acc: 0.8651 - val_loss: 0.3015 - val_acc: 0.8693\n",
      "Epoch 79/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2986 - acc: 0.8660 - val_loss: 0.2875 - val_acc: 0.8734\n",
      "Epoch 80/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3007 - acc: 0.8651 - val_loss: 0.2871 - val_acc: 0.8757\n",
      "Epoch 81/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2997 - acc: 0.8660 - val_loss: 0.2918 - val_acc: 0.8703\n",
      "Epoch 82/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2964 - acc: 0.8669 - val_loss: 0.2861 - val_acc: 0.8731\n",
      "Epoch 83/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3016 - acc: 0.8657 - val_loss: 0.2847 - val_acc: 0.8737\n",
      "Epoch 84/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2965 - acc: 0.8664 - val_loss: 0.2886 - val_acc: 0.8737\n",
      "Epoch 85/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2972 - acc: 0.8667 - val_loss: 0.2857 - val_acc: 0.8741\n",
      "Epoch 86/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2954 - acc: 0.8672 - val_loss: 0.2883 - val_acc: 0.8749\n",
      "Epoch 87/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2957 - acc: 0.8668 - val_loss: 0.2840 - val_acc: 0.8730\n",
      "Epoch 88/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3001 - acc: 0.8646 - val_loss: 0.2882 - val_acc: 0.8736\n",
      "Epoch 89/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2964 - acc: 0.8669 - val_loss: 0.2847 - val_acc: 0.8746\n",
      "Epoch 90/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2945 - acc: 0.8673 - val_loss: 0.2838 - val_acc: 0.8746\n",
      "Epoch 91/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2961 - acc: 0.8661 - val_loss: 0.2850 - val_acc: 0.8747\n",
      "Epoch 92/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2946 - acc: 0.8668 - val_loss: 0.3009 - val_acc: 0.8664\n",
      "Epoch 93/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2988 - acc: 0.8656 - val_loss: 0.2826 - val_acc: 0.8755\n",
      "Epoch 94/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2936 - acc: 0.8668 - val_loss: 0.2925 - val_acc: 0.8709\n",
      "Epoch 95/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2958 - acc: 0.8664 - val_loss: 0.2837 - val_acc: 0.8755\n",
      "Epoch 96/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2955 - acc: 0.8667 - val_loss: 0.2953 - val_acc: 0.8681\n",
      "Epoch 97/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2965 - acc: 0.8663 - val_loss: 0.2979 - val_acc: 0.8704\n",
      "Epoch 98/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2949 - acc: 0.8666 - val_loss: 0.2845 - val_acc: 0.8745\n",
      "Epoch 99/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.3015 - acc: 0.8643 - val_loss: 0.2867 - val_acc: 0.8712\n",
      "Epoch 100/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2947 - acc: 0.8675 - val_loss: 0.2819 - val_acc: 0.8754\n",
      "Epoch 101/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2932 - acc: 0.8669 - val_loss: 0.2805 - val_acc: 0.8761\n",
      "Epoch 102/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2915 - acc: 0.8675 - val_loss: 0.2977 - val_acc: 0.8723\n",
      "Epoch 103/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2942 - acc: 0.8668 - val_loss: 0.2902 - val_acc: 0.8701\n",
      "Epoch 104/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2946 - acc: 0.8666 - val_loss: 0.2804 - val_acc: 0.8759\n",
      "Epoch 105/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2927 - acc: 0.8670 - val_loss: 0.2888 - val_acc: 0.8718\n",
      "Epoch 106/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2937 - acc: 0.8672 - val_loss: 0.2822 - val_acc: 0.8740\n",
      "Epoch 107/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2931 - acc: 0.8671 - val_loss: 0.2805 - val_acc: 0.8756\n",
      "Epoch 108/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2937 - acc: 0.8663 - val_loss: 0.2796 - val_acc: 0.8748\n",
      "Epoch 109/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2931 - acc: 0.8669 - val_loss: 0.2795 - val_acc: 0.8767\n",
      "Epoch 110/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2913 - acc: 0.8682 - val_loss: 0.2802 - val_acc: 0.8775\n",
      "Epoch 111/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2939 - acc: 0.8663 - val_loss: 0.2797 - val_acc: 0.8775\n",
      "Epoch 112/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2918 - acc: 0.8670 - val_loss: 0.2832 - val_acc: 0.8759\n",
      "Epoch 113/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2915 - acc: 0.8674 - val_loss: 0.2799 - val_acc: 0.8747\n",
      "Epoch 114/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2918 - acc: 0.8670 - val_loss: 0.2795 - val_acc: 0.8739\n",
      "Epoch 115/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2905 - acc: 0.8684 - val_loss: 0.2911 - val_acc: 0.8735\n",
      "Epoch 116/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2930 - acc: 0.8678 - val_loss: 0.2816 - val_acc: 0.8719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2923 - acc: 0.8670 - val_loss: 0.2833 - val_acc: 0.8771\n",
      "Epoch 118/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2919 - acc: 0.8676 - val_loss: 0.2840 - val_acc: 0.8745\n",
      "Epoch 119/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2915 - acc: 0.8673 - val_loss: 0.2950 - val_acc: 0.8753\n",
      "Epoch 120/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2936 - acc: 0.8672 - val_loss: 0.2803 - val_acc: 0.8730\n",
      "Epoch 121/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2932 - acc: 0.8659 - val_loss: 0.2800 - val_acc: 0.8778\n",
      "Epoch 122/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2911 - acc: 0.8680 - val_loss: 0.2780 - val_acc: 0.8747\n",
      "Epoch 123/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2899 - acc: 0.8671 - val_loss: 0.2811 - val_acc: 0.8732\n",
      "Epoch 124/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2900 - acc: 0.8683 - val_loss: 0.2794 - val_acc: 0.8766\n",
      "Epoch 125/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2885 - acc: 0.8678 - val_loss: 0.2834 - val_acc: 0.8758\n",
      "Epoch 126/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2936 - acc: 0.8663 - val_loss: 0.2907 - val_acc: 0.8739\n",
      "Epoch 127/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2887 - acc: 0.8679 - val_loss: 0.2800 - val_acc: 0.8753\n",
      "Epoch 128/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2884 - acc: 0.8678 - val_loss: 0.2888 - val_acc: 0.8750\n",
      "Epoch 129/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2919 - acc: 0.8662 - val_loss: 0.2808 - val_acc: 0.8776\n",
      "Epoch 130/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2903 - acc: 0.8675 - val_loss: 0.2863 - val_acc: 0.8742\n",
      "Epoch 131/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2884 - acc: 0.8678 - val_loss: 0.2855 - val_acc: 0.8704\n",
      "Epoch 132/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2891 - acc: 0.8683 - val_loss: 0.2822 - val_acc: 0.8718\n",
      "Epoch 133/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2909 - acc: 0.8665 - val_loss: 0.2857 - val_acc: 0.8701\n",
      "Epoch 134/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2890 - acc: 0.8673 - val_loss: 0.2782 - val_acc: 0.8744\n",
      "Epoch 135/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2879 - acc: 0.8676 - val_loss: 0.2837 - val_acc: 0.8714\n",
      "Epoch 136/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2921 - acc: 0.8669 - val_loss: 0.2772 - val_acc: 0.8756\n",
      "Epoch 137/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2876 - acc: 0.8679 - val_loss: 0.2941 - val_acc: 0.8673\n",
      "Epoch 138/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2911 - acc: 0.8671 - val_loss: 0.2762 - val_acc: 0.8753\n",
      "Epoch 139/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2930 - acc: 0.8654 - val_loss: 0.2836 - val_acc: 0.8716\n",
      "Epoch 140/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2890 - acc: 0.8677 - val_loss: 0.2801 - val_acc: 0.8734\n",
      "Epoch 141/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2891 - acc: 0.8675 - val_loss: 0.2798 - val_acc: 0.8728\n",
      "Epoch 142/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2889 - acc: 0.8677 - val_loss: 0.2811 - val_acc: 0.8722\n",
      "Epoch 143/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2883 - acc: 0.8682 - val_loss: 0.2783 - val_acc: 0.8764\n",
      "Epoch 144/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2862 - acc: 0.8684 - val_loss: 0.2828 - val_acc: 0.8749\n",
      "Epoch 145/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2894 - acc: 0.8674 - val_loss: 0.2793 - val_acc: 0.8758\n",
      "Epoch 146/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2945 - acc: 0.8666 - val_loss: 0.2813 - val_acc: 0.8750\n",
      "Epoch 147/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2872 - acc: 0.8684 - val_loss: 0.2770 - val_acc: 0.8760\n",
      "Epoch 148/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2866 - acc: 0.8687 - val_loss: 0.2767 - val_acc: 0.8779\n",
      "Epoch 149/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2852 - acc: 0.8690 - val_loss: 0.2779 - val_acc: 0.8772\n",
      "Epoch 150/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2875 - acc: 0.8680 - val_loss: 0.2920 - val_acc: 0.8683\n",
      "Epoch 151/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2888 - acc: 0.8676 - val_loss: 0.2784 - val_acc: 0.8742\n",
      "Epoch 152/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2852 - acc: 0.8683 - val_loss: 0.2802 - val_acc: 0.8730\n",
      "Epoch 153/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2852 - acc: 0.8693 - val_loss: 0.2820 - val_acc: 0.8762\n",
      "Epoch 154/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2846 - acc: 0.8693 - val_loss: 0.2755 - val_acc: 0.8764\n",
      "Epoch 155/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2848 - acc: 0.8683 - val_loss: 0.2750 - val_acc: 0.8778\n",
      "Epoch 156/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2841 - acc: 0.8694 - val_loss: 0.2812 - val_acc: 0.8757\n",
      "Epoch 157/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2881 - acc: 0.8678 - val_loss: 0.2800 - val_acc: 0.8759\n",
      "Epoch 158/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2853 - acc: 0.8685 - val_loss: 0.2783 - val_acc: 0.8735\n",
      "Epoch 159/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2857 - acc: 0.8685 - val_loss: 0.2781 - val_acc: 0.8749\n",
      "Epoch 160/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2858 - acc: 0.8686 - val_loss: 0.2738 - val_acc: 0.8772\n",
      "Epoch 161/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2850 - acc: 0.8690 - val_loss: 0.2876 - val_acc: 0.8745\n",
      "Epoch 162/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2868 - acc: 0.8683 - val_loss: 0.2760 - val_acc: 0.8732\n",
      "Epoch 163/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2844 - acc: 0.8687 - val_loss: 0.2732 - val_acc: 0.8758\n",
      "Epoch 164/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2854 - acc: 0.8686 - val_loss: 0.2886 - val_acc: 0.8719\n",
      "Epoch 165/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2874 - acc: 0.8678 - val_loss: 0.2758 - val_acc: 0.8777\n",
      "Epoch 166/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2835 - acc: 0.8690 - val_loss: 0.2792 - val_acc: 0.8766\n",
      "Epoch 167/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2829 - acc: 0.8686 - val_loss: 0.2767 - val_acc: 0.8742\n",
      "Epoch 168/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2826 - acc: 0.8697 - val_loss: 0.2740 - val_acc: 0.8775\n",
      "Epoch 169/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2832 - acc: 0.8694 - val_loss: 0.2729 - val_acc: 0.8772\n",
      "Epoch 170/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2832 - acc: 0.8688 - val_loss: 0.2753 - val_acc: 0.8749\n",
      "Epoch 171/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2842 - acc: 0.8686 - val_loss: 0.2775 - val_acc: 0.8757\n",
      "Epoch 172/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2837 - acc: 0.8689 - val_loss: 0.2748 - val_acc: 0.8762\n",
      "Epoch 173/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2821 - acc: 0.8687 - val_loss: 0.2885 - val_acc: 0.8727\n",
      "Epoch 174/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2837 - acc: 0.8690 - val_loss: 0.2732 - val_acc: 0.8770\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2838 - acc: 0.8688 - val_loss: 0.2765 - val_acc: 0.8752\n",
      "Epoch 176/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2843 - acc: 0.8679 - val_loss: 0.2712 - val_acc: 0.8771\n",
      "Epoch 177/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2820 - acc: 0.8693 - val_loss: 0.2781 - val_acc: 0.8758\n",
      "Epoch 178/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2862 - acc: 0.8680 - val_loss: 0.2788 - val_acc: 0.8745\n",
      "Epoch 179/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2840 - acc: 0.8692 - val_loss: 0.2820 - val_acc: 0.8723\n",
      "Epoch 180/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2844 - acc: 0.8686 - val_loss: 0.2710 - val_acc: 0.8762\n",
      "Epoch 181/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2802 - acc: 0.8698 - val_loss: 0.2731 - val_acc: 0.8771\n",
      "Epoch 182/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2827 - acc: 0.8693 - val_loss: 0.2724 - val_acc: 0.8788\n",
      "Epoch 183/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2814 - acc: 0.8697 - val_loss: 0.2844 - val_acc: 0.8733\n",
      "Epoch 184/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2834 - acc: 0.8680 - val_loss: 0.2805 - val_acc: 0.8714\n",
      "Epoch 185/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2833 - acc: 0.8696 - val_loss: 0.2710 - val_acc: 0.8779\n",
      "Epoch 186/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2824 - acc: 0.8689 - val_loss: 0.2724 - val_acc: 0.8774\n",
      "Epoch 187/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2819 - acc: 0.8692 - val_loss: 0.2852 - val_acc: 0.8695\n",
      "Epoch 188/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2853 - acc: 0.8684 - val_loss: 0.2700 - val_acc: 0.8780\n",
      "Epoch 189/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2821 - acc: 0.8689 - val_loss: 0.2805 - val_acc: 0.8759\n",
      "Epoch 190/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2817 - acc: 0.8699 - val_loss: 0.2782 - val_acc: 0.8740\n",
      "Epoch 191/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2823 - acc: 0.8691 - val_loss: 0.2792 - val_acc: 0.8730\n",
      "Epoch 192/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2813 - acc: 0.8693 - val_loss: 0.2708 - val_acc: 0.8766\n",
      "Epoch 193/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2806 - acc: 0.8695 - val_loss: 0.2703 - val_acc: 0.8779\n",
      "Epoch 194/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2789 - acc: 0.8702 - val_loss: 0.2856 - val_acc: 0.8703\n",
      "Epoch 195/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2809 - acc: 0.8687 - val_loss: 0.2718 - val_acc: 0.8773\n",
      "Epoch 196/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2790 - acc: 0.8698 - val_loss: 0.2764 - val_acc: 0.8757\n",
      "Epoch 197/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2797 - acc: 0.8696 - val_loss: 0.2698 - val_acc: 0.8770\n",
      "Epoch 198/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2805 - acc: 0.8688 - val_loss: 0.2694 - val_acc: 0.8768\n",
      "Epoch 199/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2813 - acc: 0.8680 - val_loss: 0.2790 - val_acc: 0.8745\n",
      "Epoch 200/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2806 - acc: 0.8695 - val_loss: 0.2689 - val_acc: 0.8783\n",
      "Epoch 201/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2777 - acc: 0.8702 - val_loss: 0.2682 - val_acc: 0.8778\n",
      "Epoch 202/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2776 - acc: 0.8701 - val_loss: 0.2693 - val_acc: 0.8773\n",
      "Epoch 203/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2807 - acc: 0.8692 - val_loss: 0.2680 - val_acc: 0.8778\n",
      "Epoch 204/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2782 - acc: 0.8703 - val_loss: 0.2706 - val_acc: 0.8778\n",
      "Epoch 205/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2819 - acc: 0.8686 - val_loss: 0.2753 - val_acc: 0.8765\n",
      "Epoch 206/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2799 - acc: 0.8691 - val_loss: 0.2732 - val_acc: 0.8780\n",
      "Epoch 207/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2787 - acc: 0.8702 - val_loss: 0.2954 - val_acc: 0.8673\n",
      "Epoch 208/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2870 - acc: 0.8670 - val_loss: 0.2671 - val_acc: 0.8804\n",
      "Epoch 209/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2773 - acc: 0.8707 - val_loss: 0.2709 - val_acc: 0.8772\n",
      "Epoch 210/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2777 - acc: 0.8701 - val_loss: 0.2702 - val_acc: 0.8801\n",
      "Epoch 211/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2783 - acc: 0.8703 - val_loss: 0.2668 - val_acc: 0.8789\n",
      "Epoch 212/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2792 - acc: 0.8696 - val_loss: 0.2890 - val_acc: 0.8701\n",
      "Epoch 213/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2802 - acc: 0.8688 - val_loss: 0.2684 - val_acc: 0.8790\n",
      "Epoch 214/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2797 - acc: 0.8690 - val_loss: 0.2691 - val_acc: 0.8782\n",
      "Epoch 215/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2767 - acc: 0.8700 - val_loss: 0.2663 - val_acc: 0.8797\n",
      "Epoch 216/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2768 - acc: 0.8702 - val_loss: 0.2656 - val_acc: 0.8781\n",
      "Epoch 217/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2763 - acc: 0.8702 - val_loss: 0.2665 - val_acc: 0.8800\n",
      "Epoch 218/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2779 - acc: 0.8690 - val_loss: 0.2754 - val_acc: 0.8754\n",
      "Epoch 219/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2784 - acc: 0.8701 - val_loss: 0.2709 - val_acc: 0.8754\n",
      "Epoch 220/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2782 - acc: 0.8698 - val_loss: 0.2703 - val_acc: 0.8746\n",
      "Epoch 221/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2770 - acc: 0.8691 - val_loss: 0.2721 - val_acc: 0.8745\n",
      "Epoch 222/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2772 - acc: 0.8699 - val_loss: 0.2700 - val_acc: 0.8748\n",
      "Epoch 223/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2766 - acc: 0.8695 - val_loss: 0.2720 - val_acc: 0.8775\n",
      "Epoch 224/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2792 - acc: 0.8689 - val_loss: 0.2661 - val_acc: 0.8770\n",
      "Epoch 225/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2750 - acc: 0.8694 - val_loss: 0.2673 - val_acc: 0.8780\n",
      "Epoch 226/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2757 - acc: 0.8700 - val_loss: 0.2784 - val_acc: 0.8727\n",
      "Epoch 227/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2770 - acc: 0.8702 - val_loss: 0.2675 - val_acc: 0.8772\n",
      "Epoch 228/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2763 - acc: 0.8693 - val_loss: 0.2665 - val_acc: 0.8795\n",
      "Epoch 229/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2753 - acc: 0.8701 - val_loss: 0.2647 - val_acc: 0.8788\n",
      "Epoch 230/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2767 - acc: 0.8695 - val_loss: 0.2713 - val_acc: 0.8781\n",
      "Epoch 231/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2783 - acc: 0.8694 - val_loss: 0.2717 - val_acc: 0.8751\n",
      "Epoch 232/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2788 - acc: 0.8677 - val_loss: 0.2775 - val_acc: 0.8735\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2762 - acc: 0.8695 - val_loss: 0.2656 - val_acc: 0.8759\n",
      "Epoch 234/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2753 - acc: 0.8685 - val_loss: 0.2731 - val_acc: 0.8751\n",
      "Epoch 235/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2768 - acc: 0.8692 - val_loss: 0.2632 - val_acc: 0.8788\n",
      "Epoch 236/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2751 - acc: 0.8694 - val_loss: 0.2746 - val_acc: 0.8766\n",
      "Epoch 237/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2764 - acc: 0.8696 - val_loss: 0.2740 - val_acc: 0.8774\n",
      "Epoch 238/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2761 - acc: 0.8694 - val_loss: 0.2787 - val_acc: 0.8744\n",
      "Epoch 239/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2749 - acc: 0.8702 - val_loss: 0.2643 - val_acc: 0.8777\n",
      "Epoch 240/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2741 - acc: 0.8697 - val_loss: 0.2639 - val_acc: 0.8781\n",
      "Epoch 241/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2746 - acc: 0.8695 - val_loss: 0.2672 - val_acc: 0.8766\n",
      "Epoch 242/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2744 - acc: 0.8695 - val_loss: 0.2637 - val_acc: 0.8785\n",
      "Epoch 243/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2738 - acc: 0.8698 - val_loss: 0.2697 - val_acc: 0.8749\n",
      "Epoch 244/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2759 - acc: 0.8693 - val_loss: 0.2625 - val_acc: 0.8793\n",
      "Epoch 245/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2743 - acc: 0.8697 - val_loss: 0.2669 - val_acc: 0.8778\n",
      "Epoch 246/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2781 - acc: 0.8692 - val_loss: 0.2641 - val_acc: 0.8786\n",
      "Epoch 247/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2763 - acc: 0.8696 - val_loss: 0.2660 - val_acc: 0.8801\n",
      "Epoch 248/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2754 - acc: 0.8692 - val_loss: 0.2637 - val_acc: 0.8772\n",
      "Epoch 249/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2743 - acc: 0.8693 - val_loss: 0.2665 - val_acc: 0.8784\n",
      "Epoch 250/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2732 - acc: 0.8706 - val_loss: 0.2732 - val_acc: 0.8762\n",
      "Epoch 251/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2741 - acc: 0.8710 - val_loss: 0.2668 - val_acc: 0.8745\n",
      "Epoch 252/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2718 - acc: 0.8706 - val_loss: 0.2617 - val_acc: 0.8774\n",
      "Epoch 253/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2745 - acc: 0.8705 - val_loss: 0.2690 - val_acc: 0.8751\n",
      "Epoch 254/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2729 - acc: 0.8707 - val_loss: 0.2625 - val_acc: 0.8783\n",
      "Epoch 255/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2722 - acc: 0.8702 - val_loss: 0.2621 - val_acc: 0.8794\n",
      "Epoch 256/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2713 - acc: 0.8705 - val_loss: 0.2630 - val_acc: 0.8792\n",
      "Epoch 257/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2724 - acc: 0.8706 - val_loss: 0.2646 - val_acc: 0.8786\n",
      "Epoch 258/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2785 - acc: 0.8687 - val_loss: 0.2666 - val_acc: 0.8776\n",
      "Epoch 259/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2729 - acc: 0.8705 - val_loss: 0.2686 - val_acc: 0.8752\n",
      "Epoch 260/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2725 - acc: 0.8705 - val_loss: 0.2636 - val_acc: 0.8781\n",
      "Epoch 261/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2715 - acc: 0.8714 - val_loss: 0.2631 - val_acc: 0.8791\n",
      "Epoch 262/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2738 - acc: 0.8700 - val_loss: 0.2648 - val_acc: 0.8790\n",
      "Epoch 263/1000\n",
      "66666/66666 [==============================] - 0s 4us/step - loss: 0.2704 - acc: 0.8709 - val_loss: 0.2649 - val_acc: 0.8773\n",
      "Epoch 264/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2714 - acc: 0.8702 - val_loss: 0.2608 - val_acc: 0.8788\n",
      "Epoch 265/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2711 - acc: 0.8714 - val_loss: 0.2622 - val_acc: 0.8770\n",
      "Epoch 266/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2701 - acc: 0.8707 - val_loss: 0.2610 - val_acc: 0.8771\n",
      "Epoch 267/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2710 - acc: 0.8711 - val_loss: 0.2748 - val_acc: 0.8727\n",
      "Epoch 268/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2738 - acc: 0.8697 - val_loss: 0.2612 - val_acc: 0.8802\n",
      "Epoch 269/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2716 - acc: 0.8708 - val_loss: 0.2640 - val_acc: 0.8770\n",
      "Epoch 270/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2739 - acc: 0.8700 - val_loss: 0.2645 - val_acc: 0.8761\n",
      "Epoch 271/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2714 - acc: 0.8696 - val_loss: 0.2655 - val_acc: 0.8758\n",
      "Epoch 272/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2712 - acc: 0.8708 - val_loss: 0.2607 - val_acc: 0.8778\n",
      "Epoch 273/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2718 - acc: 0.8703 - val_loss: 0.2598 - val_acc: 0.8785\n",
      "Epoch 274/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2693 - acc: 0.8710 - val_loss: 0.2701 - val_acc: 0.8755\n",
      "Epoch 275/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2695 - acc: 0.8711 - val_loss: 0.2654 - val_acc: 0.8769\n",
      "Epoch 276/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2697 - acc: 0.8711 - val_loss: 0.2650 - val_acc: 0.8761\n",
      "Epoch 277/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2729 - acc: 0.8697 - val_loss: 0.2656 - val_acc: 0.8754\n",
      "Epoch 278/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2703 - acc: 0.8717 - val_loss: 0.2629 - val_acc: 0.8775\n",
      "Epoch 279/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2711 - acc: 0.8698 - val_loss: 0.2683 - val_acc: 0.8777\n",
      "Epoch 280/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2715 - acc: 0.8707 - val_loss: 0.2597 - val_acc: 0.8784\n",
      "Epoch 281/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2715 - acc: 0.8709 - val_loss: 0.2602 - val_acc: 0.8790\n",
      "Epoch 282/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2687 - acc: 0.8713 - val_loss: 0.2665 - val_acc: 0.8770\n",
      "Epoch 283/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2699 - acc: 0.8708 - val_loss: 0.2788 - val_acc: 0.8713\n",
      "Epoch 284/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2713 - acc: 0.8712 - val_loss: 0.2602 - val_acc: 0.8780\n",
      "Epoch 285/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2704 - acc: 0.8710 - val_loss: 0.2608 - val_acc: 0.8778\n",
      "Epoch 286/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2688 - acc: 0.8710 - val_loss: 0.2623 - val_acc: 0.8780\n",
      "Epoch 287/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2692 - acc: 0.8714 - val_loss: 0.2607 - val_acc: 0.8775\n",
      "Epoch 288/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2689 - acc: 0.8720 - val_loss: 0.2647 - val_acc: 0.8766\n",
      "Epoch 289/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2705 - acc: 0.8706 - val_loss: 0.2722 - val_acc: 0.8725\n",
      "Epoch 290/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2711 - acc: 0.8699 - val_loss: 0.2683 - val_acc: 0.8745\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2693 - acc: 0.8714 - val_loss: 0.2639 - val_acc: 0.8771\n",
      "Epoch 292/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2710 - acc: 0.8715 - val_loss: 0.2618 - val_acc: 0.8763\n",
      "Epoch 293/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2700 - acc: 0.8707 - val_loss: 0.2631 - val_acc: 0.8772\n",
      "Epoch 294/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2696 - acc: 0.8710 - val_loss: 0.2617 - val_acc: 0.8779\n",
      "Epoch 295/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2706 - acc: 0.8702 - val_loss: 0.2687 - val_acc: 0.8736\n",
      "Epoch 296/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2702 - acc: 0.8699 - val_loss: 0.2618 - val_acc: 0.8790\n",
      "Epoch 297/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2682 - acc: 0.8715 - val_loss: 0.2598 - val_acc: 0.8775\n",
      "Epoch 298/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8712 - val_loss: 0.2610 - val_acc: 0.8775\n",
      "Epoch 299/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2687 - acc: 0.8711 - val_loss: 0.2609 - val_acc: 0.8767\n",
      "Epoch 300/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2697 - acc: 0.8704 - val_loss: 0.2589 - val_acc: 0.8780\n",
      "Epoch 301/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2699 - acc: 0.8717 - val_loss: 0.2618 - val_acc: 0.8769\n",
      "Epoch 302/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2701 - acc: 0.8714 - val_loss: 0.2610 - val_acc: 0.8772\n",
      "Epoch 303/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2702 - acc: 0.8706 - val_loss: 0.2583 - val_acc: 0.8764\n",
      "Epoch 304/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2689 - acc: 0.8715 - val_loss: 0.2617 - val_acc: 0.8760\n",
      "Epoch 305/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2673 - acc: 0.8717 - val_loss: 0.2830 - val_acc: 0.8669\n",
      "Epoch 306/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2682 - acc: 0.8714 - val_loss: 0.2644 - val_acc: 0.8752\n",
      "Epoch 307/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2685 - acc: 0.8711 - val_loss: 0.2650 - val_acc: 0.8763\n",
      "Epoch 308/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2679 - acc: 0.8715 - val_loss: 0.2613 - val_acc: 0.8757\n",
      "Epoch 309/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2680 - acc: 0.8720 - val_loss: 0.2922 - val_acc: 0.8651\n",
      "Epoch 310/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2732 - acc: 0.8695 - val_loss: 0.2659 - val_acc: 0.8763\n",
      "Epoch 311/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2682 - acc: 0.8715 - val_loss: 0.2631 - val_acc: 0.8771\n",
      "Epoch 312/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2710 - acc: 0.8714 - val_loss: 0.2586 - val_acc: 0.8775\n",
      "Epoch 313/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2673 - acc: 0.8723 - val_loss: 0.2586 - val_acc: 0.8765\n",
      "Epoch 314/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2681 - acc: 0.8716 - val_loss: 0.2638 - val_acc: 0.8768\n",
      "Epoch 315/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2679 - acc: 0.8715 - val_loss: 0.2578 - val_acc: 0.8775\n",
      "Epoch 316/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2676 - acc: 0.8708 - val_loss: 0.2604 - val_acc: 0.8766\n",
      "Epoch 317/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2673 - acc: 0.8719 - val_loss: 0.2574 - val_acc: 0.8780\n",
      "Epoch 318/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2659 - acc: 0.8717 - val_loss: 0.2571 - val_acc: 0.8778\n",
      "Epoch 319/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2661 - acc: 0.8724 - val_loss: 0.2607 - val_acc: 0.8758\n",
      "Epoch 320/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2670 - acc: 0.8713 - val_loss: 0.2788 - val_acc: 0.8736\n",
      "Epoch 321/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2689 - acc: 0.8711 - val_loss: 0.2650 - val_acc: 0.8757\n",
      "Epoch 322/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2664 - acc: 0.8725 - val_loss: 0.2581 - val_acc: 0.8789\n",
      "Epoch 323/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2668 - acc: 0.8724 - val_loss: 0.2982 - val_acc: 0.8592\n",
      "Epoch 324/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2759 - acc: 0.8685 - val_loss: 0.2679 - val_acc: 0.8741\n",
      "Epoch 325/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2691 - acc: 0.8709 - val_loss: 0.2581 - val_acc: 0.8778\n",
      "Epoch 326/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2678 - acc: 0.8717 - val_loss: 0.2592 - val_acc: 0.8772\n",
      "Epoch 327/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2674 - acc: 0.8716 - val_loss: 0.2599 - val_acc: 0.8770\n",
      "Epoch 328/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2673 - acc: 0.8715 - val_loss: 0.2668 - val_acc: 0.8742\n",
      "Epoch 329/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2675 - acc: 0.8710 - val_loss: 0.2616 - val_acc: 0.8760\n",
      "Epoch 330/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2664 - acc: 0.8720 - val_loss: 0.2570 - val_acc: 0.8781\n",
      "Epoch 331/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2674 - acc: 0.8712 - val_loss: 0.2671 - val_acc: 0.8766\n",
      "Epoch 332/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2658 - acc: 0.8723 - val_loss: 0.2580 - val_acc: 0.8773\n",
      "Epoch 333/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2659 - acc: 0.8719 - val_loss: 0.2581 - val_acc: 0.8767\n",
      "Epoch 334/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2667 - acc: 0.8724 - val_loss: 0.2641 - val_acc: 0.8760\n",
      "Epoch 335/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2653 - acc: 0.8722 - val_loss: 0.2559 - val_acc: 0.8778\n",
      "Epoch 336/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2653 - acc: 0.8724 - val_loss: 0.2604 - val_acc: 0.8766\n",
      "Epoch 337/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2672 - acc: 0.8716 - val_loss: 0.2612 - val_acc: 0.8773\n",
      "Epoch 338/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2657 - acc: 0.8720 - val_loss: 0.2567 - val_acc: 0.8763\n",
      "Epoch 339/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2692 - acc: 0.8709 - val_loss: 0.2590 - val_acc: 0.8774\n",
      "Epoch 340/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2649 - acc: 0.8722 - val_loss: 0.2600 - val_acc: 0.8769\n",
      "Epoch 341/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2664 - acc: 0.8708 - val_loss: 0.2633 - val_acc: 0.8758\n",
      "Epoch 342/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2674 - acc: 0.8721 - val_loss: 0.2559 - val_acc: 0.8766\n",
      "Epoch 343/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2675 - acc: 0.8713 - val_loss: 0.2598 - val_acc: 0.8773\n",
      "Epoch 344/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2672 - acc: 0.8715 - val_loss: 0.2614 - val_acc: 0.8740\n",
      "Epoch 345/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2651 - acc: 0.8720 - val_loss: 0.2628 - val_acc: 0.8772\n",
      "Epoch 346/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2671 - acc: 0.8712 - val_loss: 0.2601 - val_acc: 0.8756\n",
      "Epoch 347/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2649 - acc: 0.8720 - val_loss: 0.2631 - val_acc: 0.8766\n",
      "Epoch 348/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2651 - acc: 0.8711 - val_loss: 0.2564 - val_acc: 0.8774\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2652 - acc: 0.8730 - val_loss: 0.2554 - val_acc: 0.8772\n",
      "Epoch 350/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2655 - acc: 0.8717 - val_loss: 0.2762 - val_acc: 0.8710\n",
      "Epoch 351/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2679 - acc: 0.8716 - val_loss: 0.2569 - val_acc: 0.8780\n",
      "Epoch 352/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2653 - acc: 0.8709 - val_loss: 0.2572 - val_acc: 0.8790\n",
      "Epoch 353/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2661 - acc: 0.8716 - val_loss: 0.2595 - val_acc: 0.8778\n",
      "Epoch 354/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2661 - acc: 0.8719 - val_loss: 0.2558 - val_acc: 0.8773\n",
      "Epoch 355/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2643 - acc: 0.8717 - val_loss: 0.2632 - val_acc: 0.8761\n",
      "Epoch 356/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2653 - acc: 0.8721 - val_loss: 0.2560 - val_acc: 0.8779\n",
      "Epoch 357/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2674 - acc: 0.8712 - val_loss: 0.2582 - val_acc: 0.8781\n",
      "Epoch 358/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2662 - acc: 0.8719 - val_loss: 0.2575 - val_acc: 0.8761\n",
      "Epoch 359/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2638 - acc: 0.8723 - val_loss: 0.2573 - val_acc: 0.8763\n",
      "Epoch 360/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2665 - acc: 0.8716 - val_loss: 0.2555 - val_acc: 0.8774\n",
      "Epoch 361/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2629 - acc: 0.8730 - val_loss: 0.2583 - val_acc: 0.8764\n",
      "Epoch 362/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2646 - acc: 0.8722 - val_loss: 0.2569 - val_acc: 0.8794\n",
      "Epoch 363/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2635 - acc: 0.8735 - val_loss: 0.2545 - val_acc: 0.8786\n",
      "Epoch 364/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2637 - acc: 0.8723 - val_loss: 0.2578 - val_acc: 0.8764\n",
      "Epoch 365/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2633 - acc: 0.8730 - val_loss: 0.2560 - val_acc: 0.8774\n",
      "Epoch 366/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2632 - acc: 0.8735 - val_loss: 0.2549 - val_acc: 0.8778\n",
      "Epoch 367/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2634 - acc: 0.8722 - val_loss: 0.2571 - val_acc: 0.8769\n",
      "Epoch 368/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2650 - acc: 0.8717 - val_loss: 0.2556 - val_acc: 0.8750\n",
      "Epoch 369/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2629 - acc: 0.8729 - val_loss: 0.2540 - val_acc: 0.8790\n",
      "Epoch 370/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2638 - acc: 0.8726 - val_loss: 0.2548 - val_acc: 0.8777\n",
      "Epoch 371/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2644 - acc: 0.8717 - val_loss: 0.2556 - val_acc: 0.8775\n",
      "Epoch 372/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2629 - acc: 0.8725 - val_loss: 0.2542 - val_acc: 0.8784\n",
      "Epoch 373/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2636 - acc: 0.8725 - val_loss: 0.2599 - val_acc: 0.8770\n",
      "Epoch 374/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2683 - acc: 0.8702 - val_loss: 0.2622 - val_acc: 0.8764\n",
      "Epoch 375/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2664 - acc: 0.8717 - val_loss: 0.2627 - val_acc: 0.8770\n",
      "Epoch 376/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2651 - acc: 0.8722 - val_loss: 0.2540 - val_acc: 0.8792\n",
      "Epoch 377/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2655 - acc: 0.8717 - val_loss: 0.2655 - val_acc: 0.8748\n",
      "Epoch 378/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2632 - acc: 0.8725 - val_loss: 0.2632 - val_acc: 0.8755\n",
      "Epoch 379/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2630 - acc: 0.8732 - val_loss: 0.2556 - val_acc: 0.8783\n",
      "Epoch 380/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2635 - acc: 0.8727 - val_loss: 0.2589 - val_acc: 0.8762\n",
      "Epoch 381/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2618 - acc: 0.8731 - val_loss: 0.2593 - val_acc: 0.8763\n",
      "Epoch 382/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2645 - acc: 0.8716 - val_loss: 0.2528 - val_acc: 0.8781\n",
      "Epoch 383/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2640 - acc: 0.8722 - val_loss: 0.2767 - val_acc: 0.8713\n",
      "Epoch 384/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2663 - acc: 0.8710 - val_loss: 0.2596 - val_acc: 0.8761\n",
      "Epoch 385/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2634 - acc: 0.8732 - val_loss: 0.2560 - val_acc: 0.8767\n",
      "Epoch 386/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2638 - acc: 0.8723 - val_loss: 0.2536 - val_acc: 0.8784\n",
      "Epoch 387/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2635 - acc: 0.8722 - val_loss: 0.2565 - val_acc: 0.8758\n",
      "Epoch 388/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2626 - acc: 0.8723 - val_loss: 0.2674 - val_acc: 0.8736\n",
      "Epoch 389/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2631 - acc: 0.8719 - val_loss: 0.2550 - val_acc: 0.8770\n",
      "Epoch 390/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2631 - acc: 0.8723 - val_loss: 0.2630 - val_acc: 0.8761\n",
      "Epoch 391/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2628 - acc: 0.8732 - val_loss: 0.2538 - val_acc: 0.8796\n",
      "Epoch 392/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2624 - acc: 0.8721 - val_loss: 0.2558 - val_acc: 0.8795\n",
      "Epoch 393/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2627 - acc: 0.8724 - val_loss: 0.2525 - val_acc: 0.8788\n",
      "Epoch 394/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2615 - acc: 0.8724 - val_loss: 0.2527 - val_acc: 0.8781\n",
      "Epoch 395/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2608 - acc: 0.8734 - val_loss: 0.2543 - val_acc: 0.8792\n",
      "Epoch 396/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2673 - acc: 0.8716 - val_loss: 0.2750 - val_acc: 0.8726\n",
      "Epoch 397/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2637 - acc: 0.8729 - val_loss: 0.2609 - val_acc: 0.8770\n",
      "Epoch 398/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2664 - acc: 0.8719 - val_loss: 0.2573 - val_acc: 0.8775\n",
      "Epoch 399/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2632 - acc: 0.8724 - val_loss: 0.2539 - val_acc: 0.8780\n",
      "Epoch 400/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2628 - acc: 0.8723 - val_loss: 0.2541 - val_acc: 0.8788\n",
      "Epoch 401/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2612 - acc: 0.8733 - val_loss: 0.2550 - val_acc: 0.8771\n",
      "Epoch 402/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2641 - acc: 0.8729 - val_loss: 0.2560 - val_acc: 0.8760\n",
      "Epoch 403/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2609 - acc: 0.8733 - val_loss: 0.2553 - val_acc: 0.8768\n",
      "Epoch 404/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2644 - acc: 0.8725 - val_loss: 0.2572 - val_acc: 0.8766\n",
      "Epoch 405/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2618 - acc: 0.8735 - val_loss: 0.2530 - val_acc: 0.8790\n",
      "Epoch 406/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2633 - acc: 0.8721 - val_loss: 0.2611 - val_acc: 0.8767\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2630 - acc: 0.8725 - val_loss: 0.2521 - val_acc: 0.8788\n",
      "Epoch 408/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2614 - acc: 0.8728 - val_loss: 0.2601 - val_acc: 0.8766\n",
      "Epoch 409/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2630 - acc: 0.8728 - val_loss: 0.2559 - val_acc: 0.8768\n",
      "Epoch 410/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2608 - acc: 0.8732 - val_loss: 0.2562 - val_acc: 0.8774\n",
      "Epoch 411/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2599 - acc: 0.8735 - val_loss: 0.2537 - val_acc: 0.8780\n",
      "Epoch 412/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2620 - acc: 0.8724 - val_loss: 0.2557 - val_acc: 0.8767\n",
      "Epoch 413/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2616 - acc: 0.8724 - val_loss: 0.2535 - val_acc: 0.8788\n",
      "Epoch 414/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2630 - acc: 0.8727 - val_loss: 0.2620 - val_acc: 0.8766\n",
      "Epoch 415/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2630 - acc: 0.8726 - val_loss: 0.2587 - val_acc: 0.8775\n",
      "Epoch 416/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2610 - acc: 0.8731 - val_loss: 0.2627 - val_acc: 0.8766\n",
      "Epoch 417/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2634 - acc: 0.8722 - val_loss: 0.2537 - val_acc: 0.8791\n",
      "Epoch 418/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2596 - acc: 0.8732 - val_loss: 0.2523 - val_acc: 0.8784\n",
      "Epoch 419/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2614 - acc: 0.8730 - val_loss: 0.2621 - val_acc: 0.8777\n",
      "Epoch 420/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2623 - acc: 0.8721 - val_loss: 0.2706 - val_acc: 0.8725\n",
      "Epoch 421/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2608 - acc: 0.8730 - val_loss: 0.2529 - val_acc: 0.8789\n",
      "Epoch 422/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2599 - acc: 0.8735 - val_loss: 0.2575 - val_acc: 0.8763\n",
      "Epoch 423/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2595 - acc: 0.8742 - val_loss: 0.2519 - val_acc: 0.8781\n",
      "Epoch 424/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2588 - acc: 0.8744 - val_loss: 0.2653 - val_acc: 0.8739\n",
      "Epoch 425/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2612 - acc: 0.8723 - val_loss: 0.2523 - val_acc: 0.8791\n",
      "Epoch 426/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2592 - acc: 0.8736 - val_loss: 0.2515 - val_acc: 0.8773\n",
      "Epoch 427/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2594 - acc: 0.8726 - val_loss: 0.2538 - val_acc: 0.8782\n",
      "Epoch 428/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2603 - acc: 0.8729 - val_loss: 0.2523 - val_acc: 0.8778\n",
      "Epoch 429/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2607 - acc: 0.8730 - val_loss: 0.2557 - val_acc: 0.8771\n",
      "Epoch 430/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2622 - acc: 0.8727 - val_loss: 0.2555 - val_acc: 0.8799\n",
      "Epoch 431/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2604 - acc: 0.8730 - val_loss: 0.2504 - val_acc: 0.8797\n",
      "Epoch 432/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2615 - acc: 0.8729 - val_loss: 0.2518 - val_acc: 0.8816\n",
      "Epoch 433/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2592 - acc: 0.8737 - val_loss: 0.2505 - val_acc: 0.8800\n",
      "Epoch 434/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2589 - acc: 0.8729 - val_loss: 0.2520 - val_acc: 0.8797\n",
      "Epoch 435/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2605 - acc: 0.8729 - val_loss: 0.2565 - val_acc: 0.8771\n",
      "Epoch 436/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2593 - acc: 0.8739 - val_loss: 0.2551 - val_acc: 0.8781\n",
      "Epoch 437/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2597 - acc: 0.8742 - val_loss: 0.2602 - val_acc: 0.8774\n",
      "Epoch 438/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2621 - acc: 0.8732 - val_loss: 0.2858 - val_acc: 0.8656\n",
      "Epoch 439/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2627 - acc: 0.8721 - val_loss: 0.2588 - val_acc: 0.8772\n",
      "Epoch 440/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2594 - acc: 0.8734 - val_loss: 0.2514 - val_acc: 0.8787\n",
      "Epoch 441/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2590 - acc: 0.8730 - val_loss: 0.2510 - val_acc: 0.8795\n",
      "Epoch 442/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2594 - acc: 0.8738 - val_loss: 0.2525 - val_acc: 0.8800\n",
      "Epoch 443/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2599 - acc: 0.8732 - val_loss: 0.2514 - val_acc: 0.8795\n",
      "Epoch 444/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2584 - acc: 0.8741 - val_loss: 0.2495 - val_acc: 0.8802\n",
      "Epoch 445/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2588 - acc: 0.8738 - val_loss: 0.2485 - val_acc: 0.8802\n",
      "Epoch 446/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2597 - acc: 0.8730 - val_loss: 0.2576 - val_acc: 0.8769\n",
      "Epoch 447/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2595 - acc: 0.8735 - val_loss: 0.2549 - val_acc: 0.8772\n",
      "Epoch 448/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2618 - acc: 0.8731 - val_loss: 0.2528 - val_acc: 0.8785\n",
      "Epoch 449/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2598 - acc: 0.8734 - val_loss: 0.2613 - val_acc: 0.8774\n",
      "Epoch 450/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2607 - acc: 0.8734 - val_loss: 0.2607 - val_acc: 0.8794\n",
      "Epoch 451/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2593 - acc: 0.8736 - val_loss: 0.2504 - val_acc: 0.8807\n",
      "Epoch 452/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2598 - acc: 0.8735 - val_loss: 0.2542 - val_acc: 0.8784\n",
      "Epoch 453/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2591 - acc: 0.8732 - val_loss: 0.2532 - val_acc: 0.8799\n",
      "Epoch 454/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2594 - acc: 0.8736 - val_loss: 0.2599 - val_acc: 0.8745\n",
      "Epoch 455/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2595 - acc: 0.8736 - val_loss: 0.2545 - val_acc: 0.8791\n",
      "Epoch 456/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2593 - acc: 0.8740 - val_loss: 0.2494 - val_acc: 0.8829\n",
      "Epoch 457/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2614 - acc: 0.8733 - val_loss: 0.2515 - val_acc: 0.8792\n",
      "Epoch 458/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2567 - acc: 0.8735 - val_loss: 0.2502 - val_acc: 0.8817\n",
      "Epoch 459/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2590 - acc: 0.8738 - val_loss: 0.2512 - val_acc: 0.8809\n",
      "Epoch 460/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2575 - acc: 0.8736 - val_loss: 0.2568 - val_acc: 0.8795\n",
      "Epoch 461/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2600 - acc: 0.8732 - val_loss: 0.2508 - val_acc: 0.8803\n",
      "Epoch 462/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2579 - acc: 0.8741 - val_loss: 0.2511 - val_acc: 0.8818\n",
      "Epoch 463/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2590 - acc: 0.8733 - val_loss: 0.2509 - val_acc: 0.8812\n",
      "Epoch 464/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2579 - acc: 0.8736 - val_loss: 0.2515 - val_acc: 0.8799\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2578 - acc: 0.8742 - val_loss: 0.2481 - val_acc: 0.8819\n",
      "Epoch 466/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2583 - acc: 0.8729 - val_loss: 0.2499 - val_acc: 0.8787\n",
      "Epoch 467/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2578 - acc: 0.8741 - val_loss: 0.2538 - val_acc: 0.8767\n",
      "Epoch 468/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2608 - acc: 0.8726 - val_loss: 0.2584 - val_acc: 0.8772\n",
      "Epoch 469/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2578 - acc: 0.8734 - val_loss: 0.2499 - val_acc: 0.8796\n",
      "Epoch 470/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2596 - acc: 0.8737 - val_loss: 0.2559 - val_acc: 0.8798\n",
      "Epoch 471/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2582 - acc: 0.8737 - val_loss: 0.2496 - val_acc: 0.8816\n",
      "Epoch 472/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2578 - acc: 0.8743 - val_loss: 0.2537 - val_acc: 0.8802\n",
      "Epoch 473/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2589 - acc: 0.8732 - val_loss: 0.2492 - val_acc: 0.8792\n",
      "Epoch 474/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2624 - acc: 0.8729 - val_loss: 0.2731 - val_acc: 0.8717\n",
      "Epoch 475/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2599 - acc: 0.8728 - val_loss: 0.2580 - val_acc: 0.8797\n",
      "Epoch 476/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2571 - acc: 0.8734 - val_loss: 0.2493 - val_acc: 0.8816\n",
      "Epoch 477/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2568 - acc: 0.8743 - val_loss: 0.2552 - val_acc: 0.8772\n",
      "Epoch 478/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2573 - acc: 0.8739 - val_loss: 0.2486 - val_acc: 0.8814\n",
      "Epoch 479/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2573 - acc: 0.8742 - val_loss: 0.2474 - val_acc: 0.8810\n",
      "Epoch 480/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2564 - acc: 0.8746 - val_loss: 0.2532 - val_acc: 0.8797\n",
      "Epoch 481/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2580 - acc: 0.8740 - val_loss: 0.2509 - val_acc: 0.8811\n",
      "Epoch 482/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2576 - acc: 0.8737 - val_loss: 0.2484 - val_acc: 0.8808\n",
      "Epoch 483/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2573 - acc: 0.8737 - val_loss: 0.2485 - val_acc: 0.8788\n",
      "Epoch 484/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2561 - acc: 0.8740 - val_loss: 0.2590 - val_acc: 0.8788\n",
      "Epoch 485/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2594 - acc: 0.8734 - val_loss: 0.2477 - val_acc: 0.8811\n",
      "Epoch 486/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2559 - acc: 0.8745 - val_loss: 0.2486 - val_acc: 0.8804\n",
      "Epoch 487/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2570 - acc: 0.8739 - val_loss: 0.2543 - val_acc: 0.8778\n",
      "Epoch 488/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2583 - acc: 0.8737 - val_loss: 0.2594 - val_acc: 0.8752\n",
      "Epoch 489/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2581 - acc: 0.8734 - val_loss: 0.2587 - val_acc: 0.8788\n",
      "Epoch 490/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2596 - acc: 0.8725 - val_loss: 0.2505 - val_acc: 0.8808\n",
      "Epoch 491/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2559 - acc: 0.8740 - val_loss: 0.2471 - val_acc: 0.8819\n",
      "Epoch 492/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2550 - acc: 0.8745 - val_loss: 0.2464 - val_acc: 0.8817\n",
      "Epoch 493/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2572 - acc: 0.8743 - val_loss: 0.2479 - val_acc: 0.8823\n",
      "Epoch 494/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2581 - acc: 0.8739 - val_loss: 0.2469 - val_acc: 0.8818\n",
      "Epoch 495/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2552 - acc: 0.8751 - val_loss: 0.2581 - val_acc: 0.8784\n",
      "Epoch 496/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2575 - acc: 0.8734 - val_loss: 0.2524 - val_acc: 0.8805\n",
      "Epoch 497/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2563 - acc: 0.8746 - val_loss: 0.2476 - val_acc: 0.8809\n",
      "Epoch 498/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2549 - acc: 0.8740 - val_loss: 0.2499 - val_acc: 0.8808\n",
      "Epoch 499/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2581 - acc: 0.8733 - val_loss: 0.2479 - val_acc: 0.8797\n",
      "Epoch 500/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2568 - acc: 0.8737 - val_loss: 0.2546 - val_acc: 0.8796\n",
      "Epoch 501/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2574 - acc: 0.8734 - val_loss: 0.2492 - val_acc: 0.8806\n",
      "Epoch 502/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2550 - acc: 0.8744 - val_loss: 0.2497 - val_acc: 0.8825\n",
      "Epoch 503/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2556 - acc: 0.8740 - val_loss: 0.2467 - val_acc: 0.8823\n",
      "Epoch 504/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2574 - acc: 0.8733 - val_loss: 0.2511 - val_acc: 0.8823\n",
      "Epoch 505/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2564 - acc: 0.8739 - val_loss: 0.2552 - val_acc: 0.8796\n",
      "Epoch 506/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2558 - acc: 0.8738 - val_loss: 0.2523 - val_acc: 0.8820\n",
      "Epoch 507/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2564 - acc: 0.8744 - val_loss: 0.2546 - val_acc: 0.8798\n",
      "Epoch 508/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2578 - acc: 0.8732 - val_loss: 0.2467 - val_acc: 0.8822\n",
      "Epoch 509/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2559 - acc: 0.8739 - val_loss: 0.2501 - val_acc: 0.8796\n",
      "Epoch 510/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2575 - acc: 0.8740 - val_loss: 0.2532 - val_acc: 0.8781\n",
      "Epoch 511/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2576 - acc: 0.8732 - val_loss: 0.2548 - val_acc: 0.8801\n",
      "Epoch 512/1000\n",
      "66666/66666 [==============================] - 0s 3us/step - loss: 0.2564 - acc: 0.8740 - val_loss: 0.2525 - val_acc: 0.8808\n",
      "Epoch 00512: early stopping\n",
      " Fold #2\n",
      ">>> Creating model...\n",
      ">>> input dim 15\n",
      "Train on 66667 samples, validate on 33333 samples\n",
      "Epoch 1/1000\n",
      "66667/66667 [==============================] - 1s 12us/step - loss: 2.5963 - acc: 0.8349 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 2/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 3/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 4/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 5/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 6/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 7/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 8/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 9/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 10/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 12/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 13/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 14/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 15/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 16/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 17/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 18/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 19/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 20/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 21/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 2.4504 - acc: 0.8463 - val_loss: 2.5248 - val_acc: 0.8416\n",
      "Epoch 00021: early stopping\n",
      " Fold #3\n",
      ">>> Creating model...\n",
      ">>> input dim 15\n",
      "Train on 66667 samples, validate on 33333 samples\n",
      "Epoch 1/1000\n",
      "66667/66667 [==============================] - 1s 12us/step - loss: 1.6942 - acc: 0.8059 - val_loss: 0.7975 - val_acc: 0.7779\n",
      "Epoch 2/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.5765 - acc: 0.8185 - val_loss: 0.4925 - val_acc: 0.8276\n",
      "Epoch 3/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.4592 - acc: 0.8354 - val_loss: 0.4491 - val_acc: 0.8370\n",
      "Epoch 4/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.4290 - acc: 0.8403 - val_loss: 0.4302 - val_acc: 0.8374\n",
      "Epoch 5/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.4152 - acc: 0.8412 - val_loss: 0.4213 - val_acc: 0.8406\n",
      "Epoch 6/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4039 - acc: 0.8422 - val_loss: 0.4135 - val_acc: 0.8421\n",
      "Epoch 7/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3956 - acc: 0.8433 - val_loss: 0.3974 - val_acc: 0.8397\n",
      "Epoch 8/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3849 - acc: 0.8443 - val_loss: 0.4005 - val_acc: 0.8415\n",
      "Epoch 9/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3782 - acc: 0.8439 - val_loss: 0.3853 - val_acc: 0.8394\n",
      "Epoch 10/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3713 - acc: 0.8444 - val_loss: 0.3775 - val_acc: 0.8408\n",
      "Epoch 11/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3651 - acc: 0.8470 - val_loss: 0.3707 - val_acc: 0.8437\n",
      "Epoch 12/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3617 - acc: 0.8485 - val_loss: 0.3681 - val_acc: 0.8417\n",
      "Epoch 13/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3576 - acc: 0.8489 - val_loss: 0.3669 - val_acc: 0.8458\n",
      "Epoch 14/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3556 - acc: 0.8484 - val_loss: 0.3621 - val_acc: 0.8450\n",
      "Epoch 15/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3550 - acc: 0.8487 - val_loss: 0.3599 - val_acc: 0.8457\n",
      "Epoch 16/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3543 - acc: 0.8481 - val_loss: 0.3574 - val_acc: 0.8451\n",
      "Epoch 17/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3482 - acc: 0.8502 - val_loss: 0.3598 - val_acc: 0.8471\n",
      "Epoch 18/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3480 - acc: 0.8504 - val_loss: 0.3559 - val_acc: 0.8470\n",
      "Epoch 19/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3463 - acc: 0.8510 - val_loss: 0.3552 - val_acc: 0.8479\n",
      "Epoch 20/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3476 - acc: 0.8504 - val_loss: 0.3498 - val_acc: 0.8482\n",
      "Epoch 21/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3419 - acc: 0.8522 - val_loss: 0.3520 - val_acc: 0.8484\n",
      "Epoch 22/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3413 - acc: 0.8521 - val_loss: 0.3489 - val_acc: 0.8483\n",
      "Epoch 23/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3389 - acc: 0.8531 - val_loss: 0.3483 - val_acc: 0.8485\n",
      "Epoch 24/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3368 - acc: 0.8535 - val_loss: 0.3528 - val_acc: 0.8483\n",
      "Epoch 25/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3386 - acc: 0.8528 - val_loss: 0.3451 - val_acc: 0.8496\n",
      "Epoch 26/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3349 - acc: 0.8540 - val_loss: 0.3449 - val_acc: 0.8477\n",
      "Epoch 27/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3337 - acc: 0.8552 - val_loss: 0.3416 - val_acc: 0.8502\n",
      "Epoch 28/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3310 - acc: 0.8551 - val_loss: 0.3400 - val_acc: 0.8499\n",
      "Epoch 29/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3317 - acc: 0.8552 - val_loss: 0.3395 - val_acc: 0.8518\n",
      "Epoch 30/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3294 - acc: 0.8564 - val_loss: 0.3382 - val_acc: 0.8518\n",
      "Epoch 31/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3281 - acc: 0.8567 - val_loss: 0.3370 - val_acc: 0.8526\n",
      "Epoch 32/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3272 - acc: 0.8576 - val_loss: 0.3377 - val_acc: 0.8534\n",
      "Epoch 33/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3257 - acc: 0.8584 - val_loss: 0.3316 - val_acc: 0.8544\n",
      "Epoch 34/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3236 - acc: 0.8587 - val_loss: 0.3320 - val_acc: 0.8539\n",
      "Epoch 35/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3220 - acc: 0.8595 - val_loss: 0.3306 - val_acc: 0.8552\n",
      "Epoch 36/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3206 - acc: 0.8601 - val_loss: 0.3358 - val_acc: 0.8530\n",
      "Epoch 37/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3224 - acc: 0.8604 - val_loss: 0.3288 - val_acc: 0.8564\n",
      "Epoch 38/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3192 - acc: 0.8618 - val_loss: 0.3287 - val_acc: 0.8553\n",
      "Epoch 39/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3201 - acc: 0.8609 - val_loss: 0.3399 - val_acc: 0.8537\n",
      "Epoch 40/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3168 - acc: 0.8616 - val_loss: 0.3270 - val_acc: 0.8580\n",
      "Epoch 41/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3151 - acc: 0.8635 - val_loss: 0.3249 - val_acc: 0.8578\n",
      "Epoch 42/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3150 - acc: 0.8628 - val_loss: 0.3342 - val_acc: 0.8551\n",
      "Epoch 43/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3137 - acc: 0.8631 - val_loss: 0.3260 - val_acc: 0.8588\n",
      "Epoch 44/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3135 - acc: 0.8633 - val_loss: 0.3305 - val_acc: 0.8566\n",
      "Epoch 45/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3110 - acc: 0.8643 - val_loss: 0.3230 - val_acc: 0.8583\n",
      "Epoch 46/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3090 - acc: 0.8644 - val_loss: 0.3301 - val_acc: 0.8567\n",
      "Epoch 47/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3108 - acc: 0.8642 - val_loss: 0.3212 - val_acc: 0.8594\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3082 - acc: 0.8654 - val_loss: 0.3178 - val_acc: 0.8611\n",
      "Epoch 49/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3070 - acc: 0.8659 - val_loss: 0.3352 - val_acc: 0.8564\n",
      "Epoch 50/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3103 - acc: 0.8651 - val_loss: 0.3178 - val_acc: 0.8604\n",
      "Epoch 51/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3072 - acc: 0.8652 - val_loss: 0.3201 - val_acc: 0.8598\n",
      "Epoch 52/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3087 - acc: 0.8658 - val_loss: 0.3198 - val_acc: 0.8607\n",
      "Epoch 53/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3098 - acc: 0.8638 - val_loss: 0.3199 - val_acc: 0.8612\n",
      "Epoch 54/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3053 - acc: 0.8666 - val_loss: 0.3189 - val_acc: 0.8600\n",
      "Epoch 55/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3057 - acc: 0.8660 - val_loss: 0.3142 - val_acc: 0.8622\n",
      "Epoch 56/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3054 - acc: 0.8666 - val_loss: 0.3170 - val_acc: 0.8610\n",
      "Epoch 57/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3039 - acc: 0.8670 - val_loss: 0.3167 - val_acc: 0.8627\n",
      "Epoch 58/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3045 - acc: 0.8664 - val_loss: 0.3125 - val_acc: 0.8631\n",
      "Epoch 59/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3019 - acc: 0.8674 - val_loss: 0.3148 - val_acc: 0.8608\n",
      "Epoch 60/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3000 - acc: 0.8680 - val_loss: 0.3119 - val_acc: 0.8605\n",
      "Epoch 61/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3042 - acc: 0.8673 - val_loss: 0.3197 - val_acc: 0.8610\n",
      "Epoch 62/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2991 - acc: 0.8685 - val_loss: 0.3123 - val_acc: 0.8627\n",
      "Epoch 63/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2986 - acc: 0.8681 - val_loss: 0.3103 - val_acc: 0.8635\n",
      "Epoch 64/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2978 - acc: 0.8687 - val_loss: 0.3099 - val_acc: 0.8637\n",
      "Epoch 65/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2982 - acc: 0.8688 - val_loss: 0.3138 - val_acc: 0.8613\n",
      "Epoch 66/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.3006 - acc: 0.8674 - val_loss: 0.3189 - val_acc: 0.8618\n",
      "Epoch 67/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2999 - acc: 0.8682 - val_loss: 0.3158 - val_acc: 0.8620\n",
      "Epoch 68/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2973 - acc: 0.8688 - val_loss: 0.3065 - val_acc: 0.8632\n",
      "Epoch 69/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2962 - acc: 0.8681 - val_loss: 0.3085 - val_acc: 0.8628\n",
      "Epoch 70/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2954 - acc: 0.8700 - val_loss: 0.3086 - val_acc: 0.8631\n",
      "Epoch 71/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2960 - acc: 0.8700 - val_loss: 0.3069 - val_acc: 0.8643\n",
      "Epoch 72/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2966 - acc: 0.8700 - val_loss: 0.3097 - val_acc: 0.8632\n",
      "Epoch 73/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2959 - acc: 0.8702 - val_loss: 0.3028 - val_acc: 0.8646\n",
      "Epoch 74/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2941 - acc: 0.8708 - val_loss: 0.3074 - val_acc: 0.8626\n",
      "Epoch 75/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2966 - acc: 0.8692 - val_loss: 0.3113 - val_acc: 0.8627\n",
      "Epoch 76/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2968 - acc: 0.8689 - val_loss: 0.3118 - val_acc: 0.8618\n",
      "Epoch 77/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2950 - acc: 0.8694 - val_loss: 0.3088 - val_acc: 0.8632\n",
      "Epoch 78/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2928 - acc: 0.8704 - val_loss: 0.3021 - val_acc: 0.8652\n",
      "Epoch 79/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2953 - acc: 0.8693 - val_loss: 0.3011 - val_acc: 0.8653\n",
      "Epoch 80/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2929 - acc: 0.8703 - val_loss: 0.3038 - val_acc: 0.8654\n",
      "Epoch 81/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2906 - acc: 0.8704 - val_loss: 0.3063 - val_acc: 0.8648\n",
      "Epoch 82/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2923 - acc: 0.8698 - val_loss: 0.3074 - val_acc: 0.8639\n",
      "Epoch 83/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2951 - acc: 0.8694 - val_loss: 0.3022 - val_acc: 0.8637\n",
      "Epoch 84/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2907 - acc: 0.8712 - val_loss: 0.3049 - val_acc: 0.8632\n",
      "Epoch 85/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2940 - acc: 0.8695 - val_loss: 0.3010 - val_acc: 0.8654\n",
      "Epoch 86/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2892 - acc: 0.8714 - val_loss: 0.3005 - val_acc: 0.8643\n",
      "Epoch 87/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2892 - acc: 0.8706 - val_loss: 0.3054 - val_acc: 0.8633\n",
      "Epoch 88/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2899 - acc: 0.8708 - val_loss: 0.3049 - val_acc: 0.8642\n",
      "Epoch 89/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2906 - acc: 0.8706 - val_loss: 0.3090 - val_acc: 0.8616\n",
      "Epoch 90/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2908 - acc: 0.8700 - val_loss: 0.3037 - val_acc: 0.8626\n",
      "Epoch 91/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2889 - acc: 0.8714 - val_loss: 0.3031 - val_acc: 0.8645\n",
      "Epoch 92/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2886 - acc: 0.8705 - val_loss: 0.2987 - val_acc: 0.8646\n",
      "Epoch 93/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2897 - acc: 0.8704 - val_loss: 0.3128 - val_acc: 0.8620\n",
      "Epoch 94/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2862 - acc: 0.8715 - val_loss: 0.3052 - val_acc: 0.8621\n",
      "Epoch 95/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2881 - acc: 0.8713 - val_loss: 0.2995 - val_acc: 0.8645\n",
      "Epoch 96/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2872 - acc: 0.8721 - val_loss: 0.2985 - val_acc: 0.8649\n",
      "Epoch 97/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2864 - acc: 0.8719 - val_loss: 0.2971 - val_acc: 0.8658\n",
      "Epoch 98/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2856 - acc: 0.8722 - val_loss: 0.3114 - val_acc: 0.8608\n",
      "Epoch 99/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2915 - acc: 0.8694 - val_loss: 0.3018 - val_acc: 0.8659\n",
      "Epoch 100/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2855 - acc: 0.8717 - val_loss: 0.2964 - val_acc: 0.8654\n",
      "Epoch 101/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2876 - acc: 0.8716 - val_loss: 0.2974 - val_acc: 0.8655\n",
      "Epoch 102/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2854 - acc: 0.8722 - val_loss: 0.3012 - val_acc: 0.8643\n",
      "Epoch 103/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2864 - acc: 0.8721 - val_loss: 0.2981 - val_acc: 0.8655\n",
      "Epoch 104/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2851 - acc: 0.8711 - val_loss: 0.3021 - val_acc: 0.8655\n",
      "Epoch 105/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2843 - acc: 0.8728 - val_loss: 0.2983 - val_acc: 0.8660\n",
      "Epoch 106/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2840 - acc: 0.8734 - val_loss: 0.3013 - val_acc: 0.8645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2838 - acc: 0.8732 - val_loss: 0.2976 - val_acc: 0.8669\n",
      "Epoch 108/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2840 - acc: 0.8729 - val_loss: 0.2943 - val_acc: 0.8671\n",
      "Epoch 109/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2821 - acc: 0.8726 - val_loss: 0.2970 - val_acc: 0.8644\n",
      "Epoch 110/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2826 - acc: 0.8729 - val_loss: 0.2954 - val_acc: 0.8681\n",
      "Epoch 111/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2836 - acc: 0.8722 - val_loss: 0.2963 - val_acc: 0.8673\n",
      "Epoch 112/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2827 - acc: 0.8739 - val_loss: 0.2942 - val_acc: 0.8673\n",
      "Epoch 113/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2819 - acc: 0.8729 - val_loss: 0.2988 - val_acc: 0.8644\n",
      "Epoch 114/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2830 - acc: 0.8732 - val_loss: 0.2944 - val_acc: 0.8670\n",
      "Epoch 115/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2828 - acc: 0.8729 - val_loss: 0.3033 - val_acc: 0.8634\n",
      "Epoch 116/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2860 - acc: 0.8712 - val_loss: 0.3036 - val_acc: 0.8636\n",
      "Epoch 117/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2819 - acc: 0.8731 - val_loss: 0.3061 - val_acc: 0.8643\n",
      "Epoch 118/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2859 - acc: 0.8730 - val_loss: 0.3235 - val_acc: 0.8583\n",
      "Epoch 119/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2871 - acc: 0.8712 - val_loss: 0.2968 - val_acc: 0.8662\n",
      "Epoch 120/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2819 - acc: 0.8730 - val_loss: 0.2932 - val_acc: 0.8666\n",
      "Epoch 121/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2827 - acc: 0.8721 - val_loss: 0.2953 - val_acc: 0.8654\n",
      "Epoch 122/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2834 - acc: 0.8722 - val_loss: 0.2991 - val_acc: 0.8672\n",
      "Epoch 123/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2829 - acc: 0.8732 - val_loss: 0.2979 - val_acc: 0.8673\n",
      "Epoch 124/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2829 - acc: 0.8727 - val_loss: 0.3190 - val_acc: 0.8605\n",
      "Epoch 125/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2822 - acc: 0.8725 - val_loss: 0.2998 - val_acc: 0.8660\n",
      "Epoch 126/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2811 - acc: 0.8738 - val_loss: 0.2924 - val_acc: 0.8690\n",
      "Epoch 127/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2812 - acc: 0.8737 - val_loss: 0.2942 - val_acc: 0.8666\n",
      "Epoch 128/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2821 - acc: 0.8732 - val_loss: 0.2933 - val_acc: 0.8676\n",
      "Epoch 129/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2802 - acc: 0.8739 - val_loss: 0.2963 - val_acc: 0.8658\n",
      "Epoch 130/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2795 - acc: 0.8740 - val_loss: 0.2913 - val_acc: 0.8680\n",
      "Epoch 131/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2800 - acc: 0.8735 - val_loss: 0.3119 - val_acc: 0.8617\n",
      "Epoch 132/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2822 - acc: 0.8728 - val_loss: 0.2924 - val_acc: 0.8671\n",
      "Epoch 133/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2794 - acc: 0.8730 - val_loss: 0.2921 - val_acc: 0.8693\n",
      "Epoch 134/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2788 - acc: 0.8734 - val_loss: 0.2974 - val_acc: 0.8655\n",
      "Epoch 135/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2802 - acc: 0.8730 - val_loss: 0.2940 - val_acc: 0.8670\n",
      "Epoch 136/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2825 - acc: 0.8731 - val_loss: 0.2941 - val_acc: 0.8682\n",
      "Epoch 137/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2804 - acc: 0.8731 - val_loss: 0.2915 - val_acc: 0.8693\n",
      "Epoch 138/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2798 - acc: 0.8732 - val_loss: 0.2966 - val_acc: 0.8651\n",
      "Epoch 139/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2798 - acc: 0.8736 - val_loss: 0.2950 - val_acc: 0.8659\n",
      "Epoch 140/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2814 - acc: 0.8727 - val_loss: 0.2917 - val_acc: 0.8688\n",
      "Epoch 141/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2779 - acc: 0.8749 - val_loss: 0.2910 - val_acc: 0.8669\n",
      "Epoch 142/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2780 - acc: 0.8748 - val_loss: 0.2971 - val_acc: 0.8659\n",
      "Epoch 143/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2780 - acc: 0.8745 - val_loss: 0.2959 - val_acc: 0.8663\n",
      "Epoch 144/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2789 - acc: 0.8738 - val_loss: 0.2914 - val_acc: 0.8691\n",
      "Epoch 145/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2789 - acc: 0.8746 - val_loss: 0.2929 - val_acc: 0.8680\n",
      "Epoch 146/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2788 - acc: 0.8740 - val_loss: 0.2935 - val_acc: 0.8681\n",
      "Epoch 147/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2791 - acc: 0.8745 - val_loss: 0.2991 - val_acc: 0.8669\n",
      "Epoch 148/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2831 - acc: 0.8731 - val_loss: 0.2947 - val_acc: 0.8670\n",
      "Epoch 149/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2776 - acc: 0.8743 - val_loss: 0.2923 - val_acc: 0.8668\n",
      "Epoch 150/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2779 - acc: 0.8736 - val_loss: 0.2955 - val_acc: 0.8669\n",
      "Epoch 151/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2784 - acc: 0.8736 - val_loss: 0.2962 - val_acc: 0.8680\n",
      "Epoch 152/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2771 - acc: 0.8740 - val_loss: 0.2938 - val_acc: 0.8669\n",
      "Epoch 153/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2768 - acc: 0.8738 - val_loss: 0.2941 - val_acc: 0.8666\n",
      "Epoch 154/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2787 - acc: 0.8740 - val_loss: 0.2925 - val_acc: 0.8694\n",
      "Epoch 155/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2786 - acc: 0.8733 - val_loss: 0.2945 - val_acc: 0.8670\n",
      "Epoch 156/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2777 - acc: 0.8738 - val_loss: 0.2939 - val_acc: 0.8684\n",
      "Epoch 157/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2795 - acc: 0.8734 - val_loss: 0.2928 - val_acc: 0.8669\n",
      "Epoch 158/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2795 - acc: 0.8737 - val_loss: 0.2949 - val_acc: 0.8663\n",
      "Epoch 159/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2765 - acc: 0.8746 - val_loss: 0.3037 - val_acc: 0.8642\n",
      "Epoch 160/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2799 - acc: 0.8730 - val_loss: 0.2952 - val_acc: 0.8661\n",
      "Epoch 161/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2770 - acc: 0.8748 - val_loss: 0.2935 - val_acc: 0.8659\n",
      "Epoch 00161: early stopping\n",
      ">>>>Computing score th = 0.3\n",
      ">>>>Computing score th = 0.4\n",
      ">>>>Computing score th = 0.5\n",
      ">>>>Computing score th = 0.6\n",
      ">>>>Computing score th = 0.7\n",
      ">>>>Computing score th = 0.8\n",
      ">>>>Computing score th = 0.9\n",
      "[[0.86276, 0.5], [0.86039, 0.4], [0.85948, 0.6], [0.85645, 0.3], [0.8503, 0.7], [0.82815, 0.8], [0.78077, 0.9]]\n",
      "\n",
      "Accuracy: 86.28%\n",
      "Threshold: 0.5\n",
      "\n",
      "learning rate: 7.8e-06\n",
      "num_dense_layers: 5\n",
      "num_dense_nodes: 40\n",
      "activation: tanh\n",
      "dropout_rate: 0.1795762503458285\n",
      "decay rate: 2.241919476253268\n",
      "batch dimension: 8580\n",
      "\n",
      "KFold(n_splits=3, random_state=None, shuffle=False)\n",
      " Fold #1\n",
      ">>> Creating model...\n",
      ">>> input dim 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66666 samples, validate on 33334 samples\n",
      "Epoch 1/1000\n",
      "66666/66666 [==============================] - 0s 5us/step - loss: 0.2713 - acc: 0.8710 - val_loss: 0.2663 - val_acc: 0.8736\n",
      "Epoch 2/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2707 - acc: 0.8704 - val_loss: 0.2652 - val_acc: 0.8755\n",
      "Epoch 3/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2692 - acc: 0.8721 - val_loss: 0.2651 - val_acc: 0.8754\n",
      "Epoch 4/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2691 - acc: 0.8718 - val_loss: 0.2653 - val_acc: 0.8760\n",
      "Epoch 5/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2692 - acc: 0.8723 - val_loss: 0.2649 - val_acc: 0.8761\n",
      "Epoch 6/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2684 - acc: 0.8720 - val_loss: 0.2646 - val_acc: 0.8768\n",
      "Epoch 7/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2682 - acc: 0.8723 - val_loss: 0.2649 - val_acc: 0.8756\n",
      "Epoch 8/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2682 - acc: 0.8722 - val_loss: 0.2647 - val_acc: 0.8775\n",
      "Epoch 9/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2683 - acc: 0.8723 - val_loss: 0.2651 - val_acc: 0.8770\n",
      "Epoch 10/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8728 - val_loss: 0.2647 - val_acc: 0.8772\n",
      "Epoch 11/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8724 - val_loss: 0.2649 - val_acc: 0.8770\n",
      "Epoch 12/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2680 - acc: 0.8723 - val_loss: 0.2646 - val_acc: 0.8775\n",
      "Epoch 13/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8729 - val_loss: 0.2645 - val_acc: 0.8772\n",
      "Epoch 14/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8725 - val_loss: 0.2647 - val_acc: 0.8763\n",
      "Epoch 15/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2681 - acc: 0.8723 - val_loss: 0.2647 - val_acc: 0.8768\n",
      "Epoch 16/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8722 - val_loss: 0.2647 - val_acc: 0.8773\n",
      "Epoch 17/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2679 - acc: 0.8728 - val_loss: 0.2647 - val_acc: 0.8774\n",
      "Epoch 18/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2679 - acc: 0.8726 - val_loss: 0.2645 - val_acc: 0.8773\n",
      "Epoch 19/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2679 - acc: 0.8728 - val_loss: 0.2644 - val_acc: 0.8768\n",
      "Epoch 20/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2679 - acc: 0.8727 - val_loss: 0.2645 - val_acc: 0.8763\n",
      "Epoch 21/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2679 - acc: 0.8723 - val_loss: 0.2645 - val_acc: 0.8772\n",
      "Epoch 22/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8730 - val_loss: 0.2645 - val_acc: 0.8763\n",
      "Epoch 23/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2682 - acc: 0.8725 - val_loss: 0.2649 - val_acc: 0.8767\n",
      "Epoch 24/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2682 - acc: 0.8728 - val_loss: 0.2652 - val_acc: 0.8756\n",
      "Epoch 25/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2683 - acc: 0.8727 - val_loss: 0.2647 - val_acc: 0.8775\n",
      "Epoch 26/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8726 - val_loss: 0.2648 - val_acc: 0.8772\n",
      "Epoch 27/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2678 - acc: 0.8726 - val_loss: 0.2644 - val_acc: 0.8763\n",
      "Epoch 28/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8731 - val_loss: 0.2644 - val_acc: 0.8773\n",
      "Epoch 29/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2684 - acc: 0.8727 - val_loss: 0.2647 - val_acc: 0.8757\n",
      "Epoch 30/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8725 - val_loss: 0.2648 - val_acc: 0.8754\n",
      "Epoch 31/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2683 - acc: 0.8729 - val_loss: 0.2645 - val_acc: 0.8758\n",
      "Epoch 32/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8728 - val_loss: 0.2647 - val_acc: 0.8779\n",
      "Epoch 33/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8727 - val_loss: 0.2646 - val_acc: 0.8765\n",
      "Epoch 34/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8729 - val_loss: 0.2649 - val_acc: 0.8787\n",
      "Epoch 35/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2678 - acc: 0.8723 - val_loss: 0.2647 - val_acc: 0.8761\n",
      "Epoch 36/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2680 - acc: 0.8728 - val_loss: 0.2642 - val_acc: 0.8768\n",
      "Epoch 37/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8727 - val_loss: 0.2648 - val_acc: 0.8765\n",
      "Epoch 38/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2682 - acc: 0.8728 - val_loss: 0.2646 - val_acc: 0.8765\n",
      "Epoch 39/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2680 - acc: 0.8730 - val_loss: 0.2655 - val_acc: 0.8777\n",
      "Epoch 40/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2676 - acc: 0.8726 - val_loss: 0.2645 - val_acc: 0.8764\n",
      "Epoch 41/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8731 - val_loss: 0.2646 - val_acc: 0.8771\n",
      "Epoch 42/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2680 - acc: 0.8726 - val_loss: 0.2648 - val_acc: 0.8755\n",
      "Epoch 43/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8731 - val_loss: 0.2647 - val_acc: 0.8765\n",
      "Epoch 44/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8732 - val_loss: 0.2651 - val_acc: 0.8776\n",
      "Epoch 45/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8733 - val_loss: 0.2643 - val_acc: 0.8766\n",
      "Epoch 46/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8731 - val_loss: 0.2643 - val_acc: 0.8759\n",
      "Epoch 47/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2677 - acc: 0.8726 - val_loss: 0.2651 - val_acc: 0.8776\n",
      "Epoch 48/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8728 - val_loss: 0.2667 - val_acc: 0.8768\n",
      "Epoch 49/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2678 - acc: 0.8731 - val_loss: 0.2642 - val_acc: 0.8769\n",
      "Epoch 50/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2673 - acc: 0.8730 - val_loss: 0.2643 - val_acc: 0.8777\n",
      "Epoch 51/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2673 - acc: 0.8730 - val_loss: 0.2643 - val_acc: 0.8765\n",
      "Epoch 52/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8734 - val_loss: 0.2646 - val_acc: 0.8757\n",
      "Epoch 53/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8724 - val_loss: 0.2646 - val_acc: 0.8773\n",
      "Epoch 54/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2674 - acc: 0.8728 - val_loss: 0.2642 - val_acc: 0.8765\n",
      "Epoch 55/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2678 - acc: 0.8733 - val_loss: 0.2642 - val_acc: 0.8765\n",
      "Epoch 56/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2673 - acc: 0.8729 - val_loss: 0.2643 - val_acc: 0.8766\n",
      "Epoch 57/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2675 - acc: 0.8723 - val_loss: 0.2646 - val_acc: 0.8760\n",
      "Epoch 58/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2676 - acc: 0.8735 - val_loss: 0.2643 - val_acc: 0.8757\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2673 - acc: 0.8726 - val_loss: 0.2643 - val_acc: 0.8762\n",
      "Epoch 60/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8732 - val_loss: 0.2643 - val_acc: 0.8761\n",
      "Epoch 61/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8728 - val_loss: 0.2642 - val_acc: 0.8762\n",
      "Epoch 62/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2671 - acc: 0.8733 - val_loss: 0.2643 - val_acc: 0.8765\n",
      "Epoch 63/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2673 - acc: 0.8730 - val_loss: 0.2647 - val_acc: 0.8763\n",
      "Epoch 64/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2674 - acc: 0.8728 - val_loss: 0.2643 - val_acc: 0.8775\n",
      "Epoch 65/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8734 - val_loss: 0.2647 - val_acc: 0.8762\n",
      "Epoch 66/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2675 - acc: 0.8727 - val_loss: 0.2641 - val_acc: 0.8769\n",
      "Epoch 67/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8732 - val_loss: 0.2643 - val_acc: 0.8765\n",
      "Epoch 68/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2671 - acc: 0.8733 - val_loss: 0.2642 - val_acc: 0.8768\n",
      "Epoch 69/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2670 - acc: 0.8731 - val_loss: 0.2663 - val_acc: 0.8771\n",
      "Epoch 70/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2675 - acc: 0.8730 - val_loss: 0.2642 - val_acc: 0.8773\n",
      "Epoch 71/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2670 - acc: 0.8733 - val_loss: 0.2642 - val_acc: 0.8761\n",
      "Epoch 72/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2671 - acc: 0.8733 - val_loss: 0.2651 - val_acc: 0.8761\n",
      "Epoch 73/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2671 - acc: 0.8732 - val_loss: 0.2644 - val_acc: 0.8780\n",
      "Epoch 74/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2670 - acc: 0.8734 - val_loss: 0.2644 - val_acc: 0.8768\n",
      "Epoch 75/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2675 - acc: 0.8732 - val_loss: 0.2644 - val_acc: 0.8760\n",
      "Epoch 76/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2671 - acc: 0.8727 - val_loss: 0.2644 - val_acc: 0.8781\n",
      "Epoch 77/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2669 - acc: 0.8733 - val_loss: 0.2656 - val_acc: 0.8773\n",
      "Epoch 78/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2673 - acc: 0.8728 - val_loss: 0.2642 - val_acc: 0.8764\n",
      "Epoch 79/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8727 - val_loss: 0.2644 - val_acc: 0.8761\n",
      "Epoch 80/1000\n",
      "66666/66666 [==============================] - 0s 1us/step - loss: 0.2671 - acc: 0.8733 - val_loss: 0.2651 - val_acc: 0.8773\n",
      "Epoch 81/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2672 - acc: 0.8728 - val_loss: 0.2645 - val_acc: 0.8769\n",
      "Epoch 82/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2669 - acc: 0.8735 - val_loss: 0.2648 - val_acc: 0.8776\n",
      "Epoch 83/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2674 - acc: 0.8735 - val_loss: 0.2642 - val_acc: 0.8767\n",
      "Epoch 84/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2670 - acc: 0.8731 - val_loss: 0.2644 - val_acc: 0.8751\n",
      "Epoch 85/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2670 - acc: 0.8733 - val_loss: 0.2647 - val_acc: 0.8765\n",
      "Epoch 86/1000\n",
      "66666/66666 [==============================] - 0s 2us/step - loss: 0.2668 - acc: 0.8732 - val_loss: 0.2642 - val_acc: 0.8768\n",
      "Epoch 00086: early stopping\n",
      " Fold #2\n",
      ">>> Creating model...\n",
      ">>> input dim 15\n",
      "Train on 66667 samples, validate on 33333 samples\n",
      "Epoch 1/1000\n",
      "66667/66667 [==============================] - 0s 6us/step - loss: 0.9717 - acc: 0.7885 - val_loss: 0.9669 - val_acc: 0.8196\n",
      "Epoch 2/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.9177 - acc: 0.8275 - val_loss: 0.9075 - val_acc: 0.8243\n",
      "Epoch 3/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.8608 - acc: 0.8277 - val_loss: 0.8446 - val_acc: 0.8249\n",
      "Epoch 4/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.8114 - acc: 0.8262 - val_loss: 0.7925 - val_acc: 0.8236\n",
      "Epoch 5/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.7618 - acc: 0.8270 - val_loss: 0.7441 - val_acc: 0.8237\n",
      "Epoch 6/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.7158 - acc: 0.8274 - val_loss: 0.7042 - val_acc: 0.8262\n",
      "Epoch 7/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.6798 - acc: 0.8296 - val_loss: 0.6704 - val_acc: 0.8279\n",
      "Epoch 8/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.6494 - acc: 0.8287 - val_loss: 0.6381 - val_acc: 0.8259\n",
      "Epoch 9/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.6201 - acc: 0.8297 - val_loss: 0.6101 - val_acc: 0.8274\n",
      "Epoch 10/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.5931 - acc: 0.8314 - val_loss: 0.5820 - val_acc: 0.8299\n",
      "Epoch 11/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.5654 - acc: 0.8338 - val_loss: 0.5519 - val_acc: 0.8331\n",
      "Epoch 12/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.5313 - acc: 0.8357 - val_loss: 0.5169 - val_acc: 0.8337\n",
      "Epoch 13/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4958 - acc: 0.8370 - val_loss: 0.4927 - val_acc: 0.8320\n",
      "Epoch 14/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4793 - acc: 0.8363 - val_loss: 0.4788 - val_acc: 0.8363\n",
      "Epoch 15/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4656 - acc: 0.8421 - val_loss: 0.4702 - val_acc: 0.8375\n",
      "Epoch 16/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4570 - acc: 0.8426 - val_loss: 0.4618 - val_acc: 0.8388\n",
      "Epoch 17/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4505 - acc: 0.8431 - val_loss: 0.4556 - val_acc: 0.8384\n",
      "Epoch 18/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4447 - acc: 0.8434 - val_loss: 0.4507 - val_acc: 0.8391\n",
      "Epoch 19/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4401 - acc: 0.8439 - val_loss: 0.4459 - val_acc: 0.8389\n",
      "Epoch 20/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4362 - acc: 0.8434 - val_loss: 0.4422 - val_acc: 0.8395\n",
      "Epoch 21/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4323 - acc: 0.8436 - val_loss: 0.4385 - val_acc: 0.8392\n",
      "Epoch 22/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4288 - acc: 0.8437 - val_loss: 0.4354 - val_acc: 0.8406\n",
      "Epoch 23/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4257 - acc: 0.8441 - val_loss: 0.4317 - val_acc: 0.8397\n",
      "Epoch 24/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4224 - acc: 0.8441 - val_loss: 0.4289 - val_acc: 0.8406\n",
      "Epoch 25/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4197 - acc: 0.8442 - val_loss: 0.4260 - val_acc: 0.8407\n",
      "Epoch 26/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4173 - acc: 0.8447 - val_loss: 0.4233 - val_acc: 0.8406\n",
      "Epoch 27/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4150 - acc: 0.8441 - val_loss: 0.4216 - val_acc: 0.8416\n",
      "Epoch 28/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4127 - acc: 0.8451 - val_loss: 0.4188 - val_acc: 0.8405\n",
      "Epoch 29/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4103 - acc: 0.8448 - val_loss: 0.4170 - val_acc: 0.8417\n",
      "Epoch 30/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4082 - acc: 0.8451 - val_loss: 0.4147 - val_acc: 0.8406\n",
      "Epoch 31/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4063 - acc: 0.8449 - val_loss: 0.4131 - val_acc: 0.8416\n",
      "Epoch 32/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4045 - acc: 0.8452 - val_loss: 0.4110 - val_acc: 0.8410\n",
      "Epoch 33/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.4025 - acc: 0.8450 - val_loss: 0.4092 - val_acc: 0.8415\n",
      "Epoch 34/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.4008 - acc: 0.8453 - val_loss: 0.4074 - val_acc: 0.8416\n",
      "Epoch 35/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3993 - acc: 0.8455 - val_loss: 0.4058 - val_acc: 0.8414\n",
      "Epoch 36/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3976 - acc: 0.8455 - val_loss: 0.4048 - val_acc: 0.8416\n",
      "Epoch 37/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3961 - acc: 0.8454 - val_loss: 0.4031 - val_acc: 0.8416\n",
      "Epoch 38/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3947 - acc: 0.8457 - val_loss: 0.4020 - val_acc: 0.8417\n",
      "Epoch 39/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3935 - acc: 0.8456 - val_loss: 0.4007 - val_acc: 0.8418\n",
      "Epoch 40/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3922 - acc: 0.8457 - val_loss: 0.3997 - val_acc: 0.8414\n",
      "Epoch 41/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3910 - acc: 0.8454 - val_loss: 0.3987 - val_acc: 0.8418\n",
      "Epoch 42/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3900 - acc: 0.8458 - val_loss: 0.3976 - val_acc: 0.8415\n",
      "Epoch 43/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3890 - acc: 0.8460 - val_loss: 0.3966 - val_acc: 0.8417\n",
      "Epoch 44/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3879 - acc: 0.8457 - val_loss: 0.3956 - val_acc: 0.8416\n",
      "Epoch 45/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3868 - acc: 0.8459 - val_loss: 0.3949 - val_acc: 0.8416\n",
      "Epoch 46/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3861 - acc: 0.8460 - val_loss: 0.3938 - val_acc: 0.8416\n",
      "Epoch 47/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3850 - acc: 0.8461 - val_loss: 0.3931 - val_acc: 0.8416\n",
      "Epoch 48/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3841 - acc: 0.8460 - val_loss: 0.3925 - val_acc: 0.8414\n",
      "Epoch 49/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3833 - acc: 0.8463 - val_loss: 0.3915 - val_acc: 0.8415\n",
      "Epoch 50/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3823 - acc: 0.8463 - val_loss: 0.3909 - val_acc: 0.8416\n",
      "Epoch 51/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3816 - acc: 0.8464 - val_loss: 0.3901 - val_acc: 0.8415\n",
      "Epoch 52/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3810 - acc: 0.8464 - val_loss: 0.3895 - val_acc: 0.8413\n",
      "Epoch 53/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3802 - acc: 0.8464 - val_loss: 0.3890 - val_acc: 0.8417\n",
      "Epoch 54/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3796 - acc: 0.8465 - val_loss: 0.3885 - val_acc: 0.8416\n",
      "Epoch 55/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3791 - acc: 0.8465 - val_loss: 0.3879 - val_acc: 0.8416\n",
      "Epoch 56/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3783 - acc: 0.8465 - val_loss: 0.3873 - val_acc: 0.8415\n",
      "Epoch 57/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3777 - acc: 0.8466 - val_loss: 0.3868 - val_acc: 0.8414\n",
      "Epoch 58/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3773 - acc: 0.8466 - val_loss: 0.3864 - val_acc: 0.8413\n",
      "Epoch 59/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3768 - acc: 0.8464 - val_loss: 0.3862 - val_acc: 0.8415\n",
      "Epoch 60/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3763 - acc: 0.8466 - val_loss: 0.3854 - val_acc: 0.8414\n",
      "Epoch 61/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3758 - acc: 0.8466 - val_loss: 0.3851 - val_acc: 0.8412\n",
      "Epoch 62/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3752 - acc: 0.8467 - val_loss: 0.3846 - val_acc: 0.8414\n",
      "Epoch 63/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3747 - acc: 0.8467 - val_loss: 0.3843 - val_acc: 0.8413\n",
      "Epoch 64/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3742 - acc: 0.8469 - val_loss: 0.3840 - val_acc: 0.8415\n",
      "Epoch 65/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3739 - acc: 0.8472 - val_loss: 0.3834 - val_acc: 0.8416\n",
      "Epoch 66/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3738 - acc: 0.8467 - val_loss: 0.3835 - val_acc: 0.8414\n",
      "Epoch 67/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3730 - acc: 0.8470 - val_loss: 0.3825 - val_acc: 0.8414\n",
      "Epoch 68/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3722 - acc: 0.8473 - val_loss: 0.3821 - val_acc: 0.8416\n",
      "Epoch 69/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3717 - acc: 0.8473 - val_loss: 0.3819 - val_acc: 0.8419\n",
      "Epoch 70/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3713 - acc: 0.8473 - val_loss: 0.3816 - val_acc: 0.8418\n",
      "Epoch 71/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3709 - acc: 0.8473 - val_loss: 0.3809 - val_acc: 0.8418\n",
      "Epoch 72/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3700 - acc: 0.8474 - val_loss: 0.3802 - val_acc: 0.8416\n",
      "Epoch 73/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3696 - acc: 0.8473 - val_loss: 0.3801 - val_acc: 0.8415\n",
      "Epoch 74/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3691 - acc: 0.8472 - val_loss: 0.3800 - val_acc: 0.8416\n",
      "Epoch 75/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3686 - acc: 0.8476 - val_loss: 0.3794 - val_acc: 0.8417\n",
      "Epoch 76/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3679 - acc: 0.8478 - val_loss: 0.3787 - val_acc: 0.8419\n",
      "Epoch 77/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3674 - acc: 0.8480 - val_loss: 0.3784 - val_acc: 0.8418\n",
      "Epoch 78/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3669 - acc: 0.8481 - val_loss: 0.3779 - val_acc: 0.8421\n",
      "Epoch 79/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3666 - acc: 0.8481 - val_loss: 0.3778 - val_acc: 0.8422\n",
      "Epoch 80/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3660 - acc: 0.8481 - val_loss: 0.3771 - val_acc: 0.8422\n",
      "Epoch 81/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3657 - acc: 0.8482 - val_loss: 0.3768 - val_acc: 0.8422\n",
      "Epoch 82/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3653 - acc: 0.8483 - val_loss: 0.3765 - val_acc: 0.8423\n",
      "Epoch 83/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3651 - acc: 0.8483 - val_loss: 0.3766 - val_acc: 0.8429\n",
      "Epoch 84/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3650 - acc: 0.8483 - val_loss: 0.3756 - val_acc: 0.8427\n",
      "Epoch 85/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3648 - acc: 0.8483 - val_loss: 0.3756 - val_acc: 0.8426\n",
      "Epoch 86/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3646 - acc: 0.8483 - val_loss: 0.3756 - val_acc: 0.8422\n",
      "Epoch 87/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3642 - acc: 0.8482 - val_loss: 0.3750 - val_acc: 0.8431\n",
      "Epoch 88/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3636 - acc: 0.8483 - val_loss: 0.3746 - val_acc: 0.8433\n",
      "Epoch 89/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3632 - acc: 0.8484 - val_loss: 0.3743 - val_acc: 0.8432\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3630 - acc: 0.8484 - val_loss: 0.3741 - val_acc: 0.8434\n",
      "Epoch 91/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3626 - acc: 0.8487 - val_loss: 0.3737 - val_acc: 0.8433\n",
      "Epoch 92/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3627 - acc: 0.8486 - val_loss: 0.3735 - val_acc: 0.8431\n",
      "Epoch 93/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3626 - acc: 0.8485 - val_loss: 0.3733 - val_acc: 0.8435\n",
      "Epoch 94/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3621 - acc: 0.8484 - val_loss: 0.3733 - val_acc: 0.8431\n",
      "Epoch 95/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3620 - acc: 0.8485 - val_loss: 0.3728 - val_acc: 0.8434\n",
      "Epoch 96/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3618 - acc: 0.8482 - val_loss: 0.3733 - val_acc: 0.8436\n",
      "Epoch 97/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3615 - acc: 0.8483 - val_loss: 0.3725 - val_acc: 0.8435\n",
      "Epoch 98/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3612 - acc: 0.8484 - val_loss: 0.3722 - val_acc: 0.8431\n",
      "Epoch 99/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3608 - acc: 0.8486 - val_loss: 0.3720 - val_acc: 0.8433\n",
      "Epoch 100/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3606 - acc: 0.8486 - val_loss: 0.3719 - val_acc: 0.8433\n",
      "Epoch 101/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3605 - acc: 0.8486 - val_loss: 0.3720 - val_acc: 0.8432\n",
      "Epoch 102/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3605 - acc: 0.8489 - val_loss: 0.3715 - val_acc: 0.8429\n",
      "Epoch 103/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3601 - acc: 0.8486 - val_loss: 0.3719 - val_acc: 0.8436\n",
      "Epoch 104/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3602 - acc: 0.8485 - val_loss: 0.3717 - val_acc: 0.8434\n",
      "Epoch 105/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3599 - acc: 0.8485 - val_loss: 0.3709 - val_acc: 0.8429\n",
      "Epoch 106/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3596 - acc: 0.8486 - val_loss: 0.3713 - val_acc: 0.8432\n",
      "Epoch 107/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3595 - acc: 0.8487 - val_loss: 0.3706 - val_acc: 0.8431\n",
      "Epoch 108/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3594 - acc: 0.8487 - val_loss: 0.3708 - val_acc: 0.8430\n",
      "Epoch 109/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3590 - acc: 0.8484 - val_loss: 0.3707 - val_acc: 0.8432\n",
      "Epoch 110/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3587 - acc: 0.8484 - val_loss: 0.3701 - val_acc: 0.8431\n",
      "Epoch 111/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3586 - acc: 0.8486 - val_loss: 0.3700 - val_acc: 0.8431\n",
      "Epoch 112/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3583 - acc: 0.8484 - val_loss: 0.3698 - val_acc: 0.8429\n",
      "Epoch 113/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3580 - acc: 0.8486 - val_loss: 0.3695 - val_acc: 0.8429\n",
      "Epoch 114/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3579 - acc: 0.8487 - val_loss: 0.3693 - val_acc: 0.8426\n",
      "Epoch 115/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3579 - acc: 0.8487 - val_loss: 0.3692 - val_acc: 0.8428\n",
      "Epoch 116/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3576 - acc: 0.8487 - val_loss: 0.3692 - val_acc: 0.8428\n",
      "Epoch 117/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3576 - acc: 0.8487 - val_loss: 0.3690 - val_acc: 0.8428\n",
      "Epoch 118/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3574 - acc: 0.8489 - val_loss: 0.3694 - val_acc: 0.8430\n",
      "Epoch 119/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3572 - acc: 0.8487 - val_loss: 0.3689 - val_acc: 0.8429\n",
      "Epoch 120/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3569 - acc: 0.8487 - val_loss: 0.3683 - val_acc: 0.8429\n",
      "Epoch 121/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3567 - acc: 0.8491 - val_loss: 0.3682 - val_acc: 0.8426\n",
      "Epoch 122/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3567 - acc: 0.8486 - val_loss: 0.3681 - val_acc: 0.8432\n",
      "Epoch 123/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3563 - acc: 0.8486 - val_loss: 0.3679 - val_acc: 0.8431\n",
      "Epoch 124/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3561 - acc: 0.8487 - val_loss: 0.3679 - val_acc: 0.8427\n",
      "Epoch 125/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3560 - acc: 0.8486 - val_loss: 0.3676 - val_acc: 0.8429\n",
      "Epoch 126/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3558 - acc: 0.8485 - val_loss: 0.3676 - val_acc: 0.8434\n",
      "Epoch 127/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3555 - acc: 0.8485 - val_loss: 0.3679 - val_acc: 0.8433\n",
      "Epoch 128/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3556 - acc: 0.8485 - val_loss: 0.3673 - val_acc: 0.8433\n",
      "Epoch 129/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3552 - acc: 0.8487 - val_loss: 0.3668 - val_acc: 0.8428\n",
      "Epoch 130/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3551 - acc: 0.8488 - val_loss: 0.3666 - val_acc: 0.8431\n",
      "Epoch 131/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3547 - acc: 0.8490 - val_loss: 0.3667 - val_acc: 0.8434\n",
      "Epoch 132/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3546 - acc: 0.8488 - val_loss: 0.3664 - val_acc: 0.8431\n",
      "Epoch 133/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3548 - acc: 0.8488 - val_loss: 0.3661 - val_acc: 0.8433\n",
      "Epoch 134/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3543 - acc: 0.8488 - val_loss: 0.3666 - val_acc: 0.8435\n",
      "Epoch 135/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3541 - acc: 0.8487 - val_loss: 0.3657 - val_acc: 0.8433\n",
      "Epoch 136/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3540 - acc: 0.8488 - val_loss: 0.3659 - val_acc: 0.8433\n",
      "Epoch 137/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3537 - acc: 0.8490 - val_loss: 0.3653 - val_acc: 0.8434\n",
      "Epoch 138/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3535 - acc: 0.8488 - val_loss: 0.3656 - val_acc: 0.8437\n",
      "Epoch 139/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3536 - acc: 0.8486 - val_loss: 0.3655 - val_acc: 0.8436\n",
      "Epoch 140/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3535 - acc: 0.8487 - val_loss: 0.3651 - val_acc: 0.8433\n",
      "Epoch 141/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3534 - acc: 0.8487 - val_loss: 0.3652 - val_acc: 0.8437\n",
      "Epoch 142/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3534 - acc: 0.8487 - val_loss: 0.3647 - val_acc: 0.8438\n",
      "Epoch 143/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3527 - acc: 0.8490 - val_loss: 0.3644 - val_acc: 0.8437\n",
      "Epoch 144/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3525 - acc: 0.8490 - val_loss: 0.3647 - val_acc: 0.8444\n",
      "Epoch 145/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3522 - acc: 0.8488 - val_loss: 0.3642 - val_acc: 0.8439\n",
      "Epoch 146/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3523 - acc: 0.8491 - val_loss: 0.3638 - val_acc: 0.8434\n",
      "Epoch 147/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3522 - acc: 0.8489 - val_loss: 0.3639 - val_acc: 0.8438\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3518 - acc: 0.8489 - val_loss: 0.3637 - val_acc: 0.8439\n",
      "Epoch 149/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3517 - acc: 0.8487 - val_loss: 0.3639 - val_acc: 0.8436\n",
      "Epoch 150/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3518 - acc: 0.8488 - val_loss: 0.3634 - val_acc: 0.8432\n",
      "Epoch 151/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3517 - acc: 0.8488 - val_loss: 0.3631 - val_acc: 0.8433\n",
      "Epoch 152/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3513 - acc: 0.8487 - val_loss: 0.3636 - val_acc: 0.8438\n",
      "Epoch 153/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3514 - acc: 0.8487 - val_loss: 0.3639 - val_acc: 0.8443\n",
      "Epoch 154/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3514 - acc: 0.8487 - val_loss: 0.3631 - val_acc: 0.8442\n",
      "Epoch 155/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3513 - acc: 0.8489 - val_loss: 0.3629 - val_acc: 0.8440\n",
      "Epoch 156/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3511 - acc: 0.8489 - val_loss: 0.3624 - val_acc: 0.8438\n",
      "Epoch 157/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3507 - acc: 0.8489 - val_loss: 0.3622 - val_acc: 0.8438\n",
      "Epoch 158/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3503 - acc: 0.8489 - val_loss: 0.3620 - val_acc: 0.8443\n",
      "Epoch 159/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3500 - acc: 0.8491 - val_loss: 0.3619 - val_acc: 0.8439\n",
      "Epoch 160/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3501 - acc: 0.8493 - val_loss: 0.3617 - val_acc: 0.8441\n",
      "Epoch 161/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3498 - acc: 0.8490 - val_loss: 0.3614 - val_acc: 0.8441\n",
      "Epoch 162/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3497 - acc: 0.8489 - val_loss: 0.3614 - val_acc: 0.8443\n",
      "Epoch 163/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3495 - acc: 0.8493 - val_loss: 0.3611 - val_acc: 0.8438\n",
      "Epoch 164/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3494 - acc: 0.8494 - val_loss: 0.3613 - val_acc: 0.8446\n",
      "Epoch 165/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3495 - acc: 0.8494 - val_loss: 0.3613 - val_acc: 0.8445\n",
      "Epoch 166/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3494 - acc: 0.8490 - val_loss: 0.3609 - val_acc: 0.8444\n",
      "Epoch 167/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3490 - acc: 0.8493 - val_loss: 0.3608 - val_acc: 0.8444\n",
      "Epoch 168/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3489 - acc: 0.8495 - val_loss: 0.3606 - val_acc: 0.8447\n",
      "Epoch 169/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3489 - acc: 0.8495 - val_loss: 0.3612 - val_acc: 0.8448\n",
      "Epoch 170/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3491 - acc: 0.8494 - val_loss: 0.3606 - val_acc: 0.8441\n",
      "Epoch 171/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3488 - acc: 0.8499 - val_loss: 0.3602 - val_acc: 0.8446\n",
      "Epoch 172/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3484 - acc: 0.8497 - val_loss: 0.3612 - val_acc: 0.8450\n",
      "Epoch 173/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3485 - acc: 0.8497 - val_loss: 0.3601 - val_acc: 0.8448\n",
      "Epoch 174/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3480 - acc: 0.8498 - val_loss: 0.3596 - val_acc: 0.8449\n",
      "Epoch 175/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3476 - acc: 0.8497 - val_loss: 0.3595 - val_acc: 0.8450\n",
      "Epoch 176/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3476 - acc: 0.8497 - val_loss: 0.3600 - val_acc: 0.8450\n",
      "Epoch 177/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3479 - acc: 0.8499 - val_loss: 0.3598 - val_acc: 0.8449\n",
      "Epoch 178/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3476 - acc: 0.8500 - val_loss: 0.3599 - val_acc: 0.8448\n",
      "Epoch 179/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3474 - acc: 0.8500 - val_loss: 0.3590 - val_acc: 0.8452\n",
      "Epoch 180/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3470 - acc: 0.8502 - val_loss: 0.3593 - val_acc: 0.8453\n",
      "Epoch 181/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3471 - acc: 0.8502 - val_loss: 0.3591 - val_acc: 0.8452\n",
      "Epoch 182/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3470 - acc: 0.8498 - val_loss: 0.3589 - val_acc: 0.8452\n",
      "Epoch 183/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3469 - acc: 0.8504 - val_loss: 0.3587 - val_acc: 0.8452\n",
      "Epoch 184/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3468 - acc: 0.8502 - val_loss: 0.3593 - val_acc: 0.8450\n",
      "Epoch 185/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3467 - acc: 0.8501 - val_loss: 0.3586 - val_acc: 0.8452\n",
      "Epoch 186/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3463 - acc: 0.8502 - val_loss: 0.3586 - val_acc: 0.8453\n",
      "Epoch 187/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3464 - acc: 0.8501 - val_loss: 0.3585 - val_acc: 0.8453\n",
      "Epoch 188/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3462 - acc: 0.8501 - val_loss: 0.3585 - val_acc: 0.8453\n",
      "Epoch 189/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3460 - acc: 0.8500 - val_loss: 0.3583 - val_acc: 0.8453\n",
      "Epoch 190/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3460 - acc: 0.8503 - val_loss: 0.3582 - val_acc: 0.8455\n",
      "Epoch 191/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3458 - acc: 0.8503 - val_loss: 0.3582 - val_acc: 0.8456\n",
      "Epoch 192/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3458 - acc: 0.8505 - val_loss: 0.3581 - val_acc: 0.8454\n",
      "Epoch 193/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3455 - acc: 0.8508 - val_loss: 0.3583 - val_acc: 0.8456\n",
      "Epoch 194/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3458 - acc: 0.8504 - val_loss: 0.3576 - val_acc: 0.8455\n",
      "Epoch 195/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3453 - acc: 0.8506 - val_loss: 0.3576 - val_acc: 0.8452\n",
      "Epoch 196/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3452 - acc: 0.8505 - val_loss: 0.3575 - val_acc: 0.8456\n",
      "Epoch 197/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3448 - acc: 0.8507 - val_loss: 0.3573 - val_acc: 0.8457\n",
      "Epoch 198/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3447 - acc: 0.8507 - val_loss: 0.3572 - val_acc: 0.8453\n",
      "Epoch 199/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3446 - acc: 0.8506 - val_loss: 0.3571 - val_acc: 0.8459\n",
      "Epoch 200/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3444 - acc: 0.8507 - val_loss: 0.3572 - val_acc: 0.8458\n",
      "Epoch 201/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3443 - acc: 0.8506 - val_loss: 0.3570 - val_acc: 0.8458\n",
      "Epoch 202/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3443 - acc: 0.8511 - val_loss: 0.3569 - val_acc: 0.8455\n",
      "Epoch 203/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3439 - acc: 0.8511 - val_loss: 0.3572 - val_acc: 0.8459\n",
      "Epoch 204/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3441 - acc: 0.8507 - val_loss: 0.3572 - val_acc: 0.8456\n",
      "Epoch 205/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3440 - acc: 0.8506 - val_loss: 0.3563 - val_acc: 0.8461\n",
      "Epoch 206/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3436 - acc: 0.8513 - val_loss: 0.3561 - val_acc: 0.8456\n",
      "Epoch 207/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3437 - acc: 0.8507 - val_loss: 0.3560 - val_acc: 0.8461\n",
      "Epoch 208/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3437 - acc: 0.8508 - val_loss: 0.3561 - val_acc: 0.8459\n",
      "Epoch 209/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3435 - acc: 0.8508 - val_loss: 0.3559 - val_acc: 0.8458\n",
      "Epoch 210/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3430 - acc: 0.8511 - val_loss: 0.3558 - val_acc: 0.8459\n",
      "Epoch 211/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3427 - acc: 0.8513 - val_loss: 0.3559 - val_acc: 0.8458\n",
      "Epoch 212/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3427 - acc: 0.8511 - val_loss: 0.3554 - val_acc: 0.8461\n",
      "Epoch 213/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3425 - acc: 0.8514 - val_loss: 0.3552 - val_acc: 0.8457\n",
      "Epoch 214/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3425 - acc: 0.8512 - val_loss: 0.3552 - val_acc: 0.8459\n",
      "Epoch 215/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3426 - acc: 0.8515 - val_loss: 0.3550 - val_acc: 0.8459\n",
      "Epoch 216/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3422 - acc: 0.8511 - val_loss: 0.3551 - val_acc: 0.8460\n",
      "Epoch 217/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3419 - acc: 0.8511 - val_loss: 0.3549 - val_acc: 0.8457\n",
      "Epoch 218/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3417 - acc: 0.8515 - val_loss: 0.3548 - val_acc: 0.8455\n",
      "Epoch 219/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3416 - acc: 0.8517 - val_loss: 0.3547 - val_acc: 0.8458\n",
      "Epoch 220/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3415 - acc: 0.8517 - val_loss: 0.3545 - val_acc: 0.8455\n",
      "Epoch 221/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3412 - acc: 0.8518 - val_loss: 0.3546 - val_acc: 0.8458\n",
      "Epoch 222/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3411 - acc: 0.8514 - val_loss: 0.3544 - val_acc: 0.8458\n",
      "Epoch 223/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3410 - acc: 0.8519 - val_loss: 0.3542 - val_acc: 0.8456\n",
      "Epoch 224/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3408 - acc: 0.8519 - val_loss: 0.3541 - val_acc: 0.8455\n",
      "Epoch 225/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3408 - acc: 0.8518 - val_loss: 0.3541 - val_acc: 0.8458\n",
      "Epoch 226/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3409 - acc: 0.8519 - val_loss: 0.3541 - val_acc: 0.8460\n",
      "Epoch 227/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3407 - acc: 0.8516 - val_loss: 0.3538 - val_acc: 0.8455\n",
      "Epoch 228/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3404 - acc: 0.8519 - val_loss: 0.3540 - val_acc: 0.8456\n",
      "Epoch 229/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3405 - acc: 0.8518 - val_loss: 0.3537 - val_acc: 0.8453\n",
      "Epoch 230/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3404 - acc: 0.8515 - val_loss: 0.3535 - val_acc: 0.8455\n",
      "Epoch 231/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3400 - acc: 0.8519 - val_loss: 0.3534 - val_acc: 0.8456\n",
      "Epoch 232/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3397 - acc: 0.8521 - val_loss: 0.3535 - val_acc: 0.8457\n",
      "Epoch 233/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3400 - acc: 0.8519 - val_loss: 0.3536 - val_acc: 0.8456\n",
      "Epoch 234/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3399 - acc: 0.8520 - val_loss: 0.3536 - val_acc: 0.8455\n",
      "Epoch 235/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3396 - acc: 0.8523 - val_loss: 0.3529 - val_acc: 0.8459\n",
      "Epoch 236/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3393 - acc: 0.8524 - val_loss: 0.3532 - val_acc: 0.8460\n",
      "Epoch 237/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3392 - acc: 0.8524 - val_loss: 0.3530 - val_acc: 0.8463\n",
      "Epoch 238/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3390 - acc: 0.8527 - val_loss: 0.3529 - val_acc: 0.8462\n",
      "Epoch 239/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3390 - acc: 0.8524 - val_loss: 0.3526 - val_acc: 0.8462\n",
      "Epoch 240/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3388 - acc: 0.8523 - val_loss: 0.3524 - val_acc: 0.8459\n",
      "Epoch 241/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3388 - acc: 0.8526 - val_loss: 0.3527 - val_acc: 0.8464\n",
      "Epoch 242/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3386 - acc: 0.8534 - val_loss: 0.3538 - val_acc: 0.8459\n",
      "Epoch 243/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3387 - acc: 0.8520 - val_loss: 0.3537 - val_acc: 0.8462\n",
      "Epoch 244/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3389 - acc: 0.8524 - val_loss: 0.3522 - val_acc: 0.8464\n",
      "Epoch 245/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3385 - acc: 0.8525 - val_loss: 0.3520 - val_acc: 0.8470\n",
      "Epoch 246/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3382 - acc: 0.8527 - val_loss: 0.3520 - val_acc: 0.8466\n",
      "Epoch 247/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3384 - acc: 0.8527 - val_loss: 0.3520 - val_acc: 0.8468\n",
      "Epoch 248/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3385 - acc: 0.8525 - val_loss: 0.3523 - val_acc: 0.8468\n",
      "Epoch 249/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3379 - acc: 0.8529 - val_loss: 0.3525 - val_acc: 0.8468\n",
      "Epoch 250/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3379 - acc: 0.8527 - val_loss: 0.3522 - val_acc: 0.8466\n",
      "Epoch 251/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3375 - acc: 0.8534 - val_loss: 0.3514 - val_acc: 0.8471\n",
      "Epoch 252/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3371 - acc: 0.8530 - val_loss: 0.3515 - val_acc: 0.8472\n",
      "Epoch 253/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3375 - acc: 0.8529 - val_loss: 0.3516 - val_acc: 0.8471\n",
      "Epoch 254/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3371 - acc: 0.8532 - val_loss: 0.3512 - val_acc: 0.8470\n",
      "Epoch 255/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3373 - acc: 0.8533 - val_loss: 0.3508 - val_acc: 0.8471\n",
      "Epoch 256/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3369 - acc: 0.8532 - val_loss: 0.3505 - val_acc: 0.8473\n",
      "Epoch 257/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3370 - acc: 0.8534 - val_loss: 0.3507 - val_acc: 0.8471\n",
      "Epoch 258/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3368 - acc: 0.8535 - val_loss: 0.3515 - val_acc: 0.8474\n",
      "Epoch 259/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3368 - acc: 0.8533 - val_loss: 0.3502 - val_acc: 0.8473\n",
      "Epoch 260/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3364 - acc: 0.8534 - val_loss: 0.3506 - val_acc: 0.8478\n",
      "Epoch 261/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3359 - acc: 0.8536 - val_loss: 0.3503 - val_acc: 0.8474\n",
      "Epoch 262/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3359 - acc: 0.8534 - val_loss: 0.3511 - val_acc: 0.8472\n",
      "Epoch 263/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3360 - acc: 0.8535 - val_loss: 0.3514 - val_acc: 0.8474\n",
      "Epoch 264/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3361 - acc: 0.8537 - val_loss: 0.3512 - val_acc: 0.8473\n",
      "Epoch 265/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3357 - acc: 0.8536 - val_loss: 0.3493 - val_acc: 0.8473\n",
      "Epoch 266/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3354 - acc: 0.8535 - val_loss: 0.3503 - val_acc: 0.8480\n",
      "Epoch 267/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3352 - acc: 0.8539 - val_loss: 0.3493 - val_acc: 0.8475\n",
      "Epoch 268/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3348 - acc: 0.8538 - val_loss: 0.3493 - val_acc: 0.8480\n",
      "Epoch 269/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3347 - acc: 0.8539 - val_loss: 0.3493 - val_acc: 0.8479\n",
      "Epoch 270/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3348 - acc: 0.8536 - val_loss: 0.3490 - val_acc: 0.8480\n",
      "Epoch 271/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3344 - acc: 0.8542 - val_loss: 0.3505 - val_acc: 0.8478\n",
      "Epoch 272/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3346 - acc: 0.8536 - val_loss: 0.3494 - val_acc: 0.8478\n",
      "Epoch 273/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3341 - acc: 0.8538 - val_loss: 0.3488 - val_acc: 0.8477\n",
      "Epoch 274/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3339 - acc: 0.8541 - val_loss: 0.3492 - val_acc: 0.8481\n",
      "Epoch 275/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3339 - acc: 0.8539 - val_loss: 0.3488 - val_acc: 0.8481\n",
      "Epoch 276/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3338 - acc: 0.8538 - val_loss: 0.3478 - val_acc: 0.8482\n",
      "Epoch 277/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3339 - acc: 0.8539 - val_loss: 0.3486 - val_acc: 0.8480\n",
      "Epoch 278/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3334 - acc: 0.8541 - val_loss: 0.3475 - val_acc: 0.8482\n",
      "Epoch 279/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3331 - acc: 0.8543 - val_loss: 0.3476 - val_acc: 0.8479\n",
      "Epoch 280/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3329 - acc: 0.8541 - val_loss: 0.3475 - val_acc: 0.8481\n",
      "Epoch 281/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3327 - acc: 0.8541 - val_loss: 0.3476 - val_acc: 0.8482\n",
      "Epoch 282/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3326 - acc: 0.8540 - val_loss: 0.3472 - val_acc: 0.8475\n",
      "Epoch 283/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3324 - acc: 0.8546 - val_loss: 0.3475 - val_acc: 0.8472\n",
      "Epoch 284/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3328 - acc: 0.8541 - val_loss: 0.3486 - val_acc: 0.8476\n",
      "Epoch 285/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3328 - acc: 0.8538 - val_loss: 0.3472 - val_acc: 0.8476\n",
      "Epoch 286/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3321 - acc: 0.8538 - val_loss: 0.3474 - val_acc: 0.8473\n",
      "Epoch 287/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3320 - acc: 0.8535 - val_loss: 0.3463 - val_acc: 0.8486\n",
      "Epoch 288/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3314 - acc: 0.8540 - val_loss: 0.3459 - val_acc: 0.8481\n",
      "Epoch 289/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3313 - acc: 0.8543 - val_loss: 0.3462 - val_acc: 0.8485\n",
      "Epoch 290/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3313 - acc: 0.8548 - val_loss: 0.3457 - val_acc: 0.8487\n",
      "Epoch 291/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3311 - acc: 0.8543 - val_loss: 0.3452 - val_acc: 0.8487\n",
      "Epoch 292/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3306 - acc: 0.8551 - val_loss: 0.3456 - val_acc: 0.8486\n",
      "Epoch 293/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3307 - acc: 0.8549 - val_loss: 0.3459 - val_acc: 0.8483\n",
      "Epoch 294/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3308 - acc: 0.8553 - val_loss: 0.3447 - val_acc: 0.8483\n",
      "Epoch 295/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3305 - acc: 0.8553 - val_loss: 0.3448 - val_acc: 0.8488\n",
      "Epoch 296/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3301 - acc: 0.8555 - val_loss: 0.3442 - val_acc: 0.8489\n",
      "Epoch 297/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3297 - acc: 0.8556 - val_loss: 0.3445 - val_acc: 0.8492\n",
      "Epoch 298/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3299 - acc: 0.8552 - val_loss: 0.3445 - val_acc: 0.8494\n",
      "Epoch 299/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3296 - acc: 0.8554 - val_loss: 0.3437 - val_acc: 0.8489\n",
      "Epoch 300/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3296 - acc: 0.8560 - val_loss: 0.3439 - val_acc: 0.8491\n",
      "Epoch 301/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3296 - acc: 0.8551 - val_loss: 0.3438 - val_acc: 0.8497\n",
      "Epoch 302/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3297 - acc: 0.8551 - val_loss: 0.3441 - val_acc: 0.8493\n",
      "Epoch 303/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3293 - acc: 0.8558 - val_loss: 0.3428 - val_acc: 0.8495\n",
      "Epoch 304/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3284 - acc: 0.8559 - val_loss: 0.3428 - val_acc: 0.8503\n",
      "Epoch 305/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3281 - acc: 0.8569 - val_loss: 0.3422 - val_acc: 0.8502\n",
      "Epoch 306/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3280 - acc: 0.8568 - val_loss: 0.3425 - val_acc: 0.8501\n",
      "Epoch 307/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3281 - acc: 0.8564 - val_loss: 0.3418 - val_acc: 0.8501\n",
      "Epoch 308/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3281 - acc: 0.8566 - val_loss: 0.3433 - val_acc: 0.8500\n",
      "Epoch 309/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3277 - acc: 0.8566 - val_loss: 0.3414 - val_acc: 0.8505\n",
      "Epoch 310/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3271 - acc: 0.8567 - val_loss: 0.3411 - val_acc: 0.8503\n",
      "Epoch 311/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3268 - acc: 0.8572 - val_loss: 0.3406 - val_acc: 0.8503\n",
      "Epoch 312/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3264 - acc: 0.8571 - val_loss: 0.3418 - val_acc: 0.8501\n",
      "Epoch 313/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3269 - acc: 0.8570 - val_loss: 0.3414 - val_acc: 0.8507\n",
      "Epoch 314/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3268 - acc: 0.8575 - val_loss: 0.3407 - val_acc: 0.8510\n",
      "Epoch 315/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3268 - acc: 0.8573 - val_loss: 0.3406 - val_acc: 0.8500\n",
      "Epoch 316/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3259 - acc: 0.8579 - val_loss: 0.3406 - val_acc: 0.8514\n",
      "Epoch 317/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3257 - acc: 0.8575 - val_loss: 0.3402 - val_acc: 0.8515\n",
      "Epoch 318/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3252 - acc: 0.8586 - val_loss: 0.3395 - val_acc: 0.8509\n",
      "Epoch 319/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3252 - acc: 0.8587 - val_loss: 0.3395 - val_acc: 0.8507\n",
      "Epoch 320/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3251 - acc: 0.8583 - val_loss: 0.3394 - val_acc: 0.8518\n",
      "Epoch 321/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3251 - acc: 0.8577 - val_loss: 0.3388 - val_acc: 0.8514\n",
      "Epoch 322/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3256 - acc: 0.8583 - val_loss: 0.3388 - val_acc: 0.8513\n",
      "Epoch 323/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3250 - acc: 0.8584 - val_loss: 0.3395 - val_acc: 0.8516\n",
      "Epoch 324/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3243 - acc: 0.8586 - val_loss: 0.3387 - val_acc: 0.8513\n",
      "Epoch 325/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3242 - acc: 0.8585 - val_loss: 0.3390 - val_acc: 0.8521\n",
      "Epoch 326/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3235 - acc: 0.8586 - val_loss: 0.3389 - val_acc: 0.8519\n",
      "Epoch 327/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3234 - acc: 0.8588 - val_loss: 0.3384 - val_acc: 0.8522\n",
      "Epoch 328/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3239 - acc: 0.8583 - val_loss: 0.3378 - val_acc: 0.8514\n",
      "Epoch 329/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3233 - acc: 0.8582 - val_loss: 0.3398 - val_acc: 0.8512\n",
      "Epoch 330/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3234 - acc: 0.8588 - val_loss: 0.3373 - val_acc: 0.8521\n",
      "Epoch 331/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3230 - acc: 0.8586 - val_loss: 0.3373 - val_acc: 0.8524\n",
      "Epoch 332/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3223 - acc: 0.8592 - val_loss: 0.3367 - val_acc: 0.8522\n",
      "Epoch 333/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3220 - acc: 0.8593 - val_loss: 0.3366 - val_acc: 0.8525\n",
      "Epoch 334/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3219 - acc: 0.8595 - val_loss: 0.3373 - val_acc: 0.8518\n",
      "Epoch 335/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3218 - acc: 0.8592 - val_loss: 0.3362 - val_acc: 0.8529\n",
      "Epoch 336/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3218 - acc: 0.8594 - val_loss: 0.3365 - val_acc: 0.8521\n",
      "Epoch 337/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3216 - acc: 0.8593 - val_loss: 0.3359 - val_acc: 0.8525\n",
      "Epoch 338/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3208 - acc: 0.8596 - val_loss: 0.3356 - val_acc: 0.8521\n",
      "Epoch 339/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3204 - acc: 0.8593 - val_loss: 0.3355 - val_acc: 0.8532\n",
      "Epoch 340/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3206 - acc: 0.8594 - val_loss: 0.3354 - val_acc: 0.8527\n",
      "Epoch 341/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3206 - acc: 0.8593 - val_loss: 0.3355 - val_acc: 0.8527\n",
      "Epoch 342/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3201 - acc: 0.8596 - val_loss: 0.3346 - val_acc: 0.8531\n",
      "Epoch 343/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3197 - acc: 0.8598 - val_loss: 0.3349 - val_acc: 0.8520\n",
      "Epoch 344/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3200 - acc: 0.8594 - val_loss: 0.3366 - val_acc: 0.8524\n",
      "Epoch 345/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3200 - acc: 0.8596 - val_loss: 0.3352 - val_acc: 0.8529\n",
      "Epoch 346/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3195 - acc: 0.8602 - val_loss: 0.3349 - val_acc: 0.8530\n",
      "Epoch 347/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3192 - acc: 0.8596 - val_loss: 0.3342 - val_acc: 0.8536\n",
      "Epoch 348/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3188 - acc: 0.8604 - val_loss: 0.3338 - val_acc: 0.8533\n",
      "Epoch 349/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3187 - acc: 0.8603 - val_loss: 0.3337 - val_acc: 0.8526\n",
      "Epoch 350/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3182 - acc: 0.8604 - val_loss: 0.3331 - val_acc: 0.8534\n",
      "Epoch 351/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3181 - acc: 0.8601 - val_loss: 0.3334 - val_acc: 0.8534\n",
      "Epoch 352/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3189 - acc: 0.8601 - val_loss: 0.3340 - val_acc: 0.8535\n",
      "Epoch 353/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3185 - acc: 0.8606 - val_loss: 0.3344 - val_acc: 0.8530\n",
      "Epoch 354/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3180 - acc: 0.8605 - val_loss: 0.3323 - val_acc: 0.8537\n",
      "Epoch 355/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3177 - acc: 0.8606 - val_loss: 0.3330 - val_acc: 0.8533\n",
      "Epoch 356/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3175 - acc: 0.8612 - val_loss: 0.3325 - val_acc: 0.8537\n",
      "Epoch 357/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3172 - acc: 0.8612 - val_loss: 0.3321 - val_acc: 0.8539\n",
      "Epoch 358/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3171 - acc: 0.8615 - val_loss: 0.3327 - val_acc: 0.8529\n",
      "Epoch 359/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3173 - acc: 0.8610 - val_loss: 0.3317 - val_acc: 0.8537\n",
      "Epoch 360/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3165 - acc: 0.8619 - val_loss: 0.3318 - val_acc: 0.8546\n",
      "Epoch 361/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3160 - acc: 0.8618 - val_loss: 0.3312 - val_acc: 0.8546\n",
      "Epoch 362/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3157 - acc: 0.8618 - val_loss: 0.3316 - val_acc: 0.8542\n",
      "Epoch 363/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3158 - acc: 0.8621 - val_loss: 0.3308 - val_acc: 0.8544\n",
      "Epoch 364/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3155 - acc: 0.8624 - val_loss: 0.3306 - val_acc: 0.8543\n",
      "Epoch 365/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3155 - acc: 0.8616 - val_loss: 0.3300 - val_acc: 0.8555\n",
      "Epoch 366/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3155 - acc: 0.8623 - val_loss: 0.3302 - val_acc: 0.8553\n",
      "Epoch 367/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3157 - acc: 0.8620 - val_loss: 0.3309 - val_acc: 0.8548\n",
      "Epoch 368/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3153 - acc: 0.8621 - val_loss: 0.3295 - val_acc: 0.8562\n",
      "Epoch 369/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3144 - acc: 0.8630 - val_loss: 0.3290 - val_acc: 0.8560\n",
      "Epoch 370/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3143 - acc: 0.8628 - val_loss: 0.3288 - val_acc: 0.8560\n",
      "Epoch 371/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3139 - acc: 0.8629 - val_loss: 0.3290 - val_acc: 0.8555\n",
      "Epoch 372/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3140 - acc: 0.8630 - val_loss: 0.3289 - val_acc: 0.8566\n",
      "Epoch 373/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3137 - acc: 0.8629 - val_loss: 0.3285 - val_acc: 0.8564\n",
      "Epoch 374/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3134 - acc: 0.8627 - val_loss: 0.3286 - val_acc: 0.8561\n",
      "Epoch 375/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3135 - acc: 0.8629 - val_loss: 0.3282 - val_acc: 0.8571\n",
      "Epoch 376/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3130 - acc: 0.8632 - val_loss: 0.3293 - val_acc: 0.8560\n",
      "Epoch 377/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3132 - acc: 0.8629 - val_loss: 0.3275 - val_acc: 0.8566\n",
      "Epoch 378/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3124 - acc: 0.8628 - val_loss: 0.3276 - val_acc: 0.8555\n",
      "Epoch 379/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3120 - acc: 0.8635 - val_loss: 0.3269 - val_acc: 0.8560\n",
      "Epoch 380/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3114 - acc: 0.8635 - val_loss: 0.3269 - val_acc: 0.8565\n",
      "Epoch 381/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3113 - acc: 0.8631 - val_loss: 0.3278 - val_acc: 0.8553\n",
      "Epoch 382/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3112 - acc: 0.8638 - val_loss: 0.3271 - val_acc: 0.8563\n",
      "Epoch 383/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3104 - acc: 0.8636 - val_loss: 0.3263 - val_acc: 0.8565\n",
      "Epoch 384/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3106 - acc: 0.8632 - val_loss: 0.3259 - val_acc: 0.8563\n",
      "Epoch 385/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3101 - acc: 0.8637 - val_loss: 0.3267 - val_acc: 0.8560\n",
      "Epoch 386/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3100 - acc: 0.8640 - val_loss: 0.3266 - val_acc: 0.8569\n",
      "Epoch 387/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3108 - acc: 0.8635 - val_loss: 0.3265 - val_acc: 0.8552\n",
      "Epoch 388/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3095 - acc: 0.8633 - val_loss: 0.3256 - val_acc: 0.8557\n",
      "Epoch 389/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3100 - acc: 0.8637 - val_loss: 0.3261 - val_acc: 0.8564\n",
      "Epoch 390/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3094 - acc: 0.8637 - val_loss: 0.3254 - val_acc: 0.8560\n",
      "Epoch 391/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3087 - acc: 0.8643 - val_loss: 0.3246 - val_acc: 0.8570\n",
      "Epoch 392/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3083 - acc: 0.8649 - val_loss: 0.3244 - val_acc: 0.8577\n",
      "Epoch 393/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3075 - acc: 0.8651 - val_loss: 0.3242 - val_acc: 0.8567\n",
      "Epoch 394/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3077 - acc: 0.8649 - val_loss: 0.3240 - val_acc: 0.8573\n",
      "Epoch 395/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3079 - acc: 0.8651 - val_loss: 0.3239 - val_acc: 0.8572\n",
      "Epoch 396/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3074 - acc: 0.8654 - val_loss: 0.3236 - val_acc: 0.8569\n",
      "Epoch 397/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3069 - acc: 0.8654 - val_loss: 0.3230 - val_acc: 0.8571\n",
      "Epoch 398/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3064 - acc: 0.8653 - val_loss: 0.3235 - val_acc: 0.8567\n",
      "Epoch 399/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.3071 - acc: 0.8650 - val_loss: 0.3228 - val_acc: 0.8575\n",
      "Epoch 400/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3067 - acc: 0.8664 - val_loss: 0.3232 - val_acc: 0.8575\n",
      "Epoch 401/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3070 - acc: 0.8656 - val_loss: 0.3236 - val_acc: 0.8565\n",
      "Epoch 402/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3064 - acc: 0.8656 - val_loss: 0.3218 - val_acc: 0.8573\n",
      "Epoch 403/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3059 - acc: 0.8652 - val_loss: 0.3222 - val_acc: 0.8578\n",
      "Epoch 404/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3070 - acc: 0.8655 - val_loss: 0.3228 - val_acc: 0.8570\n",
      "Epoch 405/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3055 - acc: 0.8665 - val_loss: 0.3212 - val_acc: 0.8584\n",
      "Epoch 406/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3049 - acc: 0.8663 - val_loss: 0.3209 - val_acc: 0.8577\n",
      "Epoch 407/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3044 - acc: 0.8661 - val_loss: 0.3216 - val_acc: 0.8574\n",
      "Epoch 408/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3042 - acc: 0.8666 - val_loss: 0.3206 - val_acc: 0.8581\n",
      "Epoch 409/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3041 - acc: 0.8669 - val_loss: 0.3213 - val_acc: 0.8587\n",
      "Epoch 410/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3044 - acc: 0.8664 - val_loss: 0.3211 - val_acc: 0.8591\n",
      "Epoch 411/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3044 - acc: 0.8662 - val_loss: 0.3207 - val_acc: 0.8575\n",
      "Epoch 412/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3042 - acc: 0.8661 - val_loss: 0.3198 - val_acc: 0.8583\n",
      "Epoch 413/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3038 - acc: 0.8670 - val_loss: 0.3201 - val_acc: 0.8585\n",
      "Epoch 414/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3029 - acc: 0.8677 - val_loss: 0.3201 - val_acc: 0.8581\n",
      "Epoch 415/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3028 - acc: 0.8674 - val_loss: 0.3197 - val_acc: 0.8590\n",
      "Epoch 416/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3023 - acc: 0.8680 - val_loss: 0.3210 - val_acc: 0.8582\n",
      "Epoch 417/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3030 - acc: 0.8667 - val_loss: 0.3215 - val_acc: 0.8582\n",
      "Epoch 418/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3032 - acc: 0.8668 - val_loss: 0.3215 - val_acc: 0.8583\n",
      "Epoch 419/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3035 - acc: 0.8664 - val_loss: 0.3214 - val_acc: 0.8574\n",
      "Epoch 420/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3044 - acc: 0.8663 - val_loss: 0.3201 - val_acc: 0.8587\n",
      "Epoch 421/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3033 - acc: 0.8670 - val_loss: 0.3194 - val_acc: 0.8590\n",
      "Epoch 422/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3035 - acc: 0.8664 - val_loss: 0.3187 - val_acc: 0.8590\n",
      "Epoch 423/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3020 - acc: 0.8676 - val_loss: 0.3191 - val_acc: 0.8586\n",
      "Epoch 424/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3014 - acc: 0.8678 - val_loss: 0.3183 - val_acc: 0.8596\n",
      "Epoch 425/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3014 - acc: 0.8678 - val_loss: 0.3188 - val_acc: 0.8589\n",
      "Epoch 426/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3006 - acc: 0.8682 - val_loss: 0.3183 - val_acc: 0.8590\n",
      "Epoch 427/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3000 - acc: 0.8684 - val_loss: 0.3176 - val_acc: 0.8593\n",
      "Epoch 428/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2997 - acc: 0.8680 - val_loss: 0.3183 - val_acc: 0.8595\n",
      "Epoch 429/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.3000 - acc: 0.8682 - val_loss: 0.3182 - val_acc: 0.8590\n",
      "Epoch 430/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2997 - acc: 0.8685 - val_loss: 0.3170 - val_acc: 0.8597\n",
      "Epoch 431/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2997 - acc: 0.8685 - val_loss: 0.3169 - val_acc: 0.8596\n",
      "Epoch 432/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2994 - acc: 0.8687 - val_loss: 0.3165 - val_acc: 0.8596\n",
      "Epoch 433/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2991 - acc: 0.8689 - val_loss: 0.3166 - val_acc: 0.8600\n",
      "Epoch 434/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2993 - acc: 0.8680 - val_loss: 0.3172 - val_acc: 0.8592\n",
      "Epoch 435/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2998 - acc: 0.8677 - val_loss: 0.3181 - val_acc: 0.8593\n",
      "Epoch 436/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2997 - acc: 0.8681 - val_loss: 0.3193 - val_acc: 0.8593\n",
      "Epoch 437/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2996 - acc: 0.8682 - val_loss: 0.3179 - val_acc: 0.8595\n",
      "Epoch 438/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2986 - acc: 0.8690 - val_loss: 0.3166 - val_acc: 0.8609\n",
      "Epoch 439/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2982 - acc: 0.8688 - val_loss: 0.3171 - val_acc: 0.8592\n",
      "Epoch 440/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2982 - acc: 0.8689 - val_loss: 0.3164 - val_acc: 0.8602\n",
      "Epoch 441/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2979 - acc: 0.8693 - val_loss: 0.3161 - val_acc: 0.8600\n",
      "Epoch 442/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2974 - acc: 0.8687 - val_loss: 0.3159 - val_acc: 0.8597\n",
      "Epoch 443/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2975 - acc: 0.8694 - val_loss: 0.3159 - val_acc: 0.8600\n",
      "Epoch 444/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2982 - acc: 0.8693 - val_loss: 0.3157 - val_acc: 0.8607\n",
      "Epoch 445/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2982 - acc: 0.8689 - val_loss: 0.3155 - val_acc: 0.8603\n",
      "Epoch 446/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2978 - acc: 0.8689 - val_loss: 0.3160 - val_acc: 0.8603\n",
      "Epoch 447/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2970 - acc: 0.8683 - val_loss: 0.3152 - val_acc: 0.8603\n",
      "Epoch 448/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2966 - acc: 0.8686 - val_loss: 0.3145 - val_acc: 0.8607\n",
      "Epoch 449/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2958 - acc: 0.8695 - val_loss: 0.3141 - val_acc: 0.8605\n",
      "Epoch 450/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2956 - acc: 0.8701 - val_loss: 0.3142 - val_acc: 0.8602\n",
      "Epoch 451/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2960 - acc: 0.8696 - val_loss: 0.3161 - val_acc: 0.8605\n",
      "Epoch 452/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2967 - acc: 0.8689 - val_loss: 0.3145 - val_acc: 0.8602\n",
      "Epoch 453/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2960 - acc: 0.8689 - val_loss: 0.3140 - val_acc: 0.8609\n",
      "Epoch 454/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2961 - acc: 0.8693 - val_loss: 0.3153 - val_acc: 0.8604\n",
      "Epoch 455/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2960 - acc: 0.8690 - val_loss: 0.3136 - val_acc: 0.8610\n",
      "Epoch 456/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2959 - acc: 0.8690 - val_loss: 0.3142 - val_acc: 0.8599\n",
      "Epoch 457/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2956 - acc: 0.8689 - val_loss: 0.3142 - val_acc: 0.8607\n",
      "Epoch 458/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2947 - acc: 0.8693 - val_loss: 0.3136 - val_acc: 0.8605\n",
      "Epoch 459/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2950 - acc: 0.8693 - val_loss: 0.3142 - val_acc: 0.8599\n",
      "Epoch 460/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2949 - acc: 0.8694 - val_loss: 0.3149 - val_acc: 0.8607\n",
      "Epoch 461/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2956 - acc: 0.8692 - val_loss: 0.3165 - val_acc: 0.8604\n",
      "Epoch 462/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2960 - acc: 0.8690 - val_loss: 0.3169 - val_acc: 0.8594\n",
      "Epoch 463/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2958 - acc: 0.8688 - val_loss: 0.3138 - val_acc: 0.8608\n",
      "Epoch 464/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2952 - acc: 0.8689 - val_loss: 0.3136 - val_acc: 0.8617\n",
      "Epoch 465/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2957 - acc: 0.8688 - val_loss: 0.3149 - val_acc: 0.8608\n",
      "Epoch 466/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2944 - acc: 0.8691 - val_loss: 0.3125 - val_acc: 0.8615\n",
      "Epoch 467/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2936 - acc: 0.8692 - val_loss: 0.3126 - val_acc: 0.8618\n",
      "Epoch 468/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2936 - acc: 0.8700 - val_loss: 0.3124 - val_acc: 0.8611\n",
      "Epoch 469/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2936 - acc: 0.8690 - val_loss: 0.3124 - val_acc: 0.8615\n",
      "Epoch 470/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2948 - acc: 0.8701 - val_loss: 0.3147 - val_acc: 0.8597\n",
      "Epoch 471/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2939 - acc: 0.8687 - val_loss: 0.3163 - val_acc: 0.8598\n",
      "Epoch 472/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2947 - acc: 0.8686 - val_loss: 0.3129 - val_acc: 0.8607\n",
      "Epoch 473/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2927 - acc: 0.8695 - val_loss: 0.3127 - val_acc: 0.8617\n",
      "Epoch 474/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2934 - acc: 0.8698 - val_loss: 0.3122 - val_acc: 0.8604\n",
      "Epoch 475/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2931 - acc: 0.8693 - val_loss: 0.3129 - val_acc: 0.8612\n",
      "Epoch 476/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2935 - acc: 0.8689 - val_loss: 0.3153 - val_acc: 0.8593\n",
      "Epoch 477/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2942 - acc: 0.8690 - val_loss: 0.3145 - val_acc: 0.8591\n",
      "Epoch 478/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2945 - acc: 0.8691 - val_loss: 0.3159 - val_acc: 0.8595\n",
      "Epoch 479/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2943 - acc: 0.8691 - val_loss: 0.3127 - val_acc: 0.8599\n",
      "Epoch 480/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2935 - acc: 0.8689 - val_loss: 0.3140 - val_acc: 0.8604\n",
      "Epoch 481/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2929 - acc: 0.8696 - val_loss: 0.3123 - val_acc: 0.8593\n",
      "Epoch 482/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2916 - acc: 0.8699 - val_loss: 0.3116 - val_acc: 0.8604\n",
      "Epoch 483/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2919 - acc: 0.8696 - val_loss: 0.3126 - val_acc: 0.8603\n",
      "Epoch 484/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2927 - acc: 0.8695 - val_loss: 0.3138 - val_acc: 0.8606\n",
      "Epoch 485/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2925 - acc: 0.8693 - val_loss: 0.3118 - val_acc: 0.8607\n",
      "Epoch 486/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2923 - acc: 0.8694 - val_loss: 0.3125 - val_acc: 0.8599\n",
      "Epoch 487/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2916 - acc: 0.8692 - val_loss: 0.3102 - val_acc: 0.8615\n",
      "Epoch 488/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2906 - acc: 0.8695 - val_loss: 0.3106 - val_acc: 0.8606\n",
      "Epoch 489/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2904 - acc: 0.8703 - val_loss: 0.3103 - val_acc: 0.8611\n",
      "Epoch 490/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2902 - acc: 0.8704 - val_loss: 0.3101 - val_acc: 0.8613\n",
      "Epoch 491/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2900 - acc: 0.8703 - val_loss: 0.3101 - val_acc: 0.8608\n",
      "Epoch 492/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2903 - acc: 0.8707 - val_loss: 0.3100 - val_acc: 0.8610\n",
      "Epoch 493/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2896 - acc: 0.8706 - val_loss: 0.3102 - val_acc: 0.8605\n",
      "Epoch 494/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2902 - acc: 0.8698 - val_loss: 0.3093 - val_acc: 0.8604\n",
      "Epoch 495/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2902 - acc: 0.8696 - val_loss: 0.3104 - val_acc: 0.8606\n",
      "Epoch 496/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2910 - acc: 0.8699 - val_loss: 0.3117 - val_acc: 0.8611\n",
      "Epoch 497/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2904 - acc: 0.8700 - val_loss: 0.3097 - val_acc: 0.8612\n",
      "Epoch 498/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2900 - acc: 0.8703 - val_loss: 0.3127 - val_acc: 0.8595\n",
      "Epoch 499/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2904 - acc: 0.8696 - val_loss: 0.3091 - val_acc: 0.8611\n",
      "Epoch 500/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2891 - acc: 0.8705 - val_loss: 0.3091 - val_acc: 0.8605\n",
      "Epoch 501/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2898 - acc: 0.8701 - val_loss: 0.3097 - val_acc: 0.8607\n",
      "Epoch 502/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2893 - acc: 0.8705 - val_loss: 0.3103 - val_acc: 0.8606\n",
      "Epoch 503/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2892 - acc: 0.8705 - val_loss: 0.3089 - val_acc: 0.8609\n",
      "Epoch 504/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2888 - acc: 0.8703 - val_loss: 0.3096 - val_acc: 0.8609\n",
      "Epoch 505/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2898 - acc: 0.8698 - val_loss: 0.3084 - val_acc: 0.8607\n",
      "Epoch 506/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2896 - acc: 0.8700 - val_loss: 0.3084 - val_acc: 0.8604\n",
      "Epoch 507/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2894 - acc: 0.8705 - val_loss: 0.3084 - val_acc: 0.8606\n",
      "Epoch 508/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2900 - acc: 0.8698 - val_loss: 0.3085 - val_acc: 0.8608\n",
      "Epoch 509/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2888 - acc: 0.8704 - val_loss: 0.3081 - val_acc: 0.8603\n",
      "Epoch 510/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2892 - acc: 0.8705 - val_loss: 0.3086 - val_acc: 0.8609\n",
      "Epoch 511/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2887 - acc: 0.8701 - val_loss: 0.3090 - val_acc: 0.8610\n",
      "Epoch 512/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2880 - acc: 0.8704 - val_loss: 0.3083 - val_acc: 0.8605\n",
      "Epoch 513/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2886 - acc: 0.8702 - val_loss: 0.3104 - val_acc: 0.8605\n",
      "Epoch 514/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2896 - acc: 0.8697 - val_loss: 0.3088 - val_acc: 0.8616\n",
      "Epoch 515/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2888 - acc: 0.8706 - val_loss: 0.3120 - val_acc: 0.8599\n",
      "Epoch 516/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2909 - acc: 0.8691 - val_loss: 0.3090 - val_acc: 0.8609\n",
      "Epoch 517/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2896 - acc: 0.8701 - val_loss: 0.3095 - val_acc: 0.8600\n",
      "Epoch 518/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2888 - acc: 0.8702 - val_loss: 0.3089 - val_acc: 0.8607\n",
      "Epoch 519/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2878 - acc: 0.8707 - val_loss: 0.3077 - val_acc: 0.8603\n",
      "Epoch 520/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2891 - acc: 0.8693 - val_loss: 0.3100 - val_acc: 0.8617\n",
      "Epoch 521/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2894 - acc: 0.8700 - val_loss: 0.3085 - val_acc: 0.8607\n",
      "Epoch 522/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2885 - acc: 0.8697 - val_loss: 0.3075 - val_acc: 0.8611\n",
      "Epoch 523/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2883 - acc: 0.8707 - val_loss: 0.3086 - val_acc: 0.8600\n",
      "Epoch 524/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2878 - acc: 0.8703 - val_loss: 0.3068 - val_acc: 0.8600\n",
      "Epoch 525/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2869 - acc: 0.8702 - val_loss: 0.3067 - val_acc: 0.8613\n",
      "Epoch 526/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2873 - acc: 0.8699 - val_loss: 0.3071 - val_acc: 0.8602\n",
      "Epoch 527/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2871 - acc: 0.8701 - val_loss: 0.3074 - val_acc: 0.8606\n",
      "Epoch 528/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2868 - acc: 0.8705 - val_loss: 0.3074 - val_acc: 0.8608\n",
      "Epoch 529/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2865 - acc: 0.8706 - val_loss: 0.3092 - val_acc: 0.8604\n",
      "Epoch 530/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2874 - acc: 0.8698 - val_loss: 0.3072 - val_acc: 0.8605\n",
      "Epoch 531/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2861 - acc: 0.8708 - val_loss: 0.3057 - val_acc: 0.8613\n",
      "Epoch 532/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2863 - acc: 0.8710 - val_loss: 0.3068 - val_acc: 0.8613\n",
      "Epoch 533/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2865 - acc: 0.8708 - val_loss: 0.3065 - val_acc: 0.8615\n",
      "Epoch 534/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2859 - acc: 0.8703 - val_loss: 0.3060 - val_acc: 0.8617\n",
      "Epoch 535/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2865 - acc: 0.8704 - val_loss: 0.3082 - val_acc: 0.8617\n",
      "Epoch 536/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2876 - acc: 0.8702 - val_loss: 0.3075 - val_acc: 0.8609\n",
      "Epoch 537/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2858 - acc: 0.8704 - val_loss: 0.3065 - val_acc: 0.8601\n",
      "Epoch 538/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2859 - acc: 0.8700 - val_loss: 0.3055 - val_acc: 0.8604\n",
      "Epoch 539/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2856 - acc: 0.8706 - val_loss: 0.3060 - val_acc: 0.8614\n",
      "Epoch 540/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2861 - acc: 0.8707 - val_loss: 0.3052 - val_acc: 0.8613\n",
      "Epoch 541/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2861 - acc: 0.8702 - val_loss: 0.3060 - val_acc: 0.8603\n",
      "Epoch 542/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2869 - acc: 0.8701 - val_loss: 0.3068 - val_acc: 0.8603\n",
      "Epoch 543/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2861 - acc: 0.8710 - val_loss: 0.3060 - val_acc: 0.8602\n",
      "Epoch 544/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2854 - acc: 0.8703 - val_loss: 0.3045 - val_acc: 0.8604\n",
      "Epoch 545/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2848 - acc: 0.8705 - val_loss: 0.3055 - val_acc: 0.8617\n",
      "Epoch 546/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2847 - acc: 0.8712 - val_loss: 0.3048 - val_acc: 0.8611\n",
      "Epoch 547/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2850 - acc: 0.8713 - val_loss: 0.3047 - val_acc: 0.8607\n",
      "Epoch 548/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2850 - acc: 0.8710 - val_loss: 0.3066 - val_acc: 0.8602\n",
      "Epoch 549/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2851 - acc: 0.8715 - val_loss: 0.3048 - val_acc: 0.8607\n",
      "Epoch 550/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2852 - acc: 0.8703 - val_loss: 0.3060 - val_acc: 0.8613\n",
      "Epoch 551/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2855 - acc: 0.8709 - val_loss: 0.3054 - val_acc: 0.8607\n",
      "Epoch 552/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2848 - acc: 0.8709 - val_loss: 0.3049 - val_acc: 0.8599\n",
      "Epoch 553/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2843 - acc: 0.8712 - val_loss: 0.3040 - val_acc: 0.8612\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2841 - acc: 0.8708 - val_loss: 0.3047 - val_acc: 0.8602\n",
      "Epoch 555/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2846 - acc: 0.8705 - val_loss: 0.3060 - val_acc: 0.8620\n",
      "Epoch 556/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2842 - acc: 0.8708 - val_loss: 0.3041 - val_acc: 0.8621\n",
      "Epoch 557/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2850 - acc: 0.8707 - val_loss: 0.3041 - val_acc: 0.8604\n",
      "Epoch 558/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2842 - acc: 0.8704 - val_loss: 0.3037 - val_acc: 0.8618\n",
      "Epoch 559/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2841 - acc: 0.8715 - val_loss: 0.3049 - val_acc: 0.8616\n",
      "Epoch 560/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2845 - acc: 0.8718 - val_loss: 0.3055 - val_acc: 0.8612\n",
      "Epoch 561/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2845 - acc: 0.8714 - val_loss: 0.3041 - val_acc: 0.8613\n",
      "Epoch 562/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2837 - acc: 0.8711 - val_loss: 0.3054 - val_acc: 0.8615\n",
      "Epoch 563/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2849 - acc: 0.8710 - val_loss: 0.3055 - val_acc: 0.8612\n",
      "Epoch 564/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2841 - acc: 0.8712 - val_loss: 0.3041 - val_acc: 0.8612\n",
      "Epoch 565/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2839 - acc: 0.8717 - val_loss: 0.3051 - val_acc: 0.8603\n",
      "Epoch 566/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2834 - acc: 0.8712 - val_loss: 0.3035 - val_acc: 0.8619\n",
      "Epoch 567/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2836 - acc: 0.8714 - val_loss: 0.3066 - val_acc: 0.8609\n",
      "Epoch 568/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2854 - acc: 0.8703 - val_loss: 0.3060 - val_acc: 0.8602\n",
      "Epoch 569/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2851 - acc: 0.8710 - val_loss: 0.3056 - val_acc: 0.8613\n",
      "Epoch 570/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2861 - acc: 0.8706 - val_loss: 0.3037 - val_acc: 0.8624\n",
      "Epoch 571/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2843 - acc: 0.8718 - val_loss: 0.3025 - val_acc: 0.8614\n",
      "Epoch 572/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2839 - acc: 0.8711 - val_loss: 0.3044 - val_acc: 0.8597\n",
      "Epoch 573/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2825 - acc: 0.8715 - val_loss: 0.3031 - val_acc: 0.8618\n",
      "Epoch 574/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2827 - acc: 0.8714 - val_loss: 0.3026 - val_acc: 0.8624\n",
      "Epoch 575/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2835 - acc: 0.8711 - val_loss: 0.3025 - val_acc: 0.8618\n",
      "Epoch 576/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2832 - acc: 0.8701 - val_loss: 0.3031 - val_acc: 0.8612\n",
      "Epoch 577/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2826 - acc: 0.8712 - val_loss: 0.3028 - val_acc: 0.8620\n",
      "Epoch 578/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2822 - acc: 0.8710 - val_loss: 0.3025 - val_acc: 0.8606\n",
      "Epoch 579/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2825 - acc: 0.8713 - val_loss: 0.3023 - val_acc: 0.8625\n",
      "Epoch 580/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2820 - acc: 0.8716 - val_loss: 0.3028 - val_acc: 0.8620\n",
      "Epoch 581/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2823 - acc: 0.8714 - val_loss: 0.3023 - val_acc: 0.8625\n",
      "Epoch 582/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2829 - acc: 0.8716 - val_loss: 0.3017 - val_acc: 0.8611\n",
      "Epoch 583/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2829 - acc: 0.8713 - val_loss: 0.3054 - val_acc: 0.8615\n",
      "Epoch 584/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2832 - acc: 0.8714 - val_loss: 0.3038 - val_acc: 0.8617\n",
      "Epoch 585/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2822 - acc: 0.8713 - val_loss: 0.3024 - val_acc: 0.8617\n",
      "Epoch 586/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2830 - acc: 0.8713 - val_loss: 0.3029 - val_acc: 0.8615\n",
      "Epoch 587/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2823 - acc: 0.8712 - val_loss: 0.3031 - val_acc: 0.8622\n",
      "Epoch 588/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2819 - acc: 0.8724 - val_loss: 0.3031 - val_acc: 0.8609\n",
      "Epoch 589/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2828 - acc: 0.8710 - val_loss: 0.3052 - val_acc: 0.8600\n",
      "Epoch 590/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2836 - acc: 0.8709 - val_loss: 0.3032 - val_acc: 0.8617\n",
      "Epoch 591/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2830 - acc: 0.8712 - val_loss: 0.3018 - val_acc: 0.8633\n",
      "Epoch 592/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2830 - acc: 0.8715 - val_loss: 0.3016 - val_acc: 0.8612\n",
      "Epoch 593/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2850 - acc: 0.8703 - val_loss: 0.3018 - val_acc: 0.8614\n",
      "Epoch 594/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2827 - acc: 0.8712 - val_loss: 0.3011 - val_acc: 0.8622\n",
      "Epoch 595/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2809 - acc: 0.8720 - val_loss: 0.3023 - val_acc: 0.8622\n",
      "Epoch 596/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2811 - acc: 0.8718 - val_loss: 0.3005 - val_acc: 0.8624\n",
      "Epoch 597/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2812 - acc: 0.8726 - val_loss: 0.3006 - val_acc: 0.8623\n",
      "Epoch 598/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2807 - acc: 0.8714 - val_loss: 0.3012 - val_acc: 0.8620\n",
      "Epoch 599/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2822 - acc: 0.8714 - val_loss: 0.3053 - val_acc: 0.8603\n",
      "Epoch 600/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2811 - acc: 0.8719 - val_loss: 0.3011 - val_acc: 0.8629\n",
      "Epoch 601/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2801 - acc: 0.8722 - val_loss: 0.3015 - val_acc: 0.8615\n",
      "Epoch 602/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2801 - acc: 0.8720 - val_loss: 0.3009 - val_acc: 0.8626\n",
      "Epoch 603/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2806 - acc: 0.8717 - val_loss: 0.3004 - val_acc: 0.8615\n",
      "Epoch 604/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2802 - acc: 0.8721 - val_loss: 0.3005 - val_acc: 0.8616\n",
      "Epoch 605/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2802 - acc: 0.8715 - val_loss: 0.2998 - val_acc: 0.8618\n",
      "Epoch 606/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2801 - acc: 0.8719 - val_loss: 0.3008 - val_acc: 0.8627\n",
      "Epoch 607/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2801 - acc: 0.8719 - val_loss: 0.3010 - val_acc: 0.8624\n",
      "Epoch 608/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2801 - acc: 0.8719 - val_loss: 0.3014 - val_acc: 0.8628\n",
      "Epoch 609/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2804 - acc: 0.8712 - val_loss: 0.3032 - val_acc: 0.8611\n",
      "Epoch 610/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2813 - acc: 0.8717 - val_loss: 0.3014 - val_acc: 0.8631\n",
      "Epoch 611/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2810 - acc: 0.8718 - val_loss: 0.3017 - val_acc: 0.8617\n",
      "Epoch 612/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2800 - acc: 0.8724 - val_loss: 0.3002 - val_acc: 0.8628\n",
      "Epoch 613/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2796 - acc: 0.8726 - val_loss: 0.2998 - val_acc: 0.8629\n",
      "Epoch 614/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2792 - acc: 0.8724 - val_loss: 0.2992 - val_acc: 0.8626\n",
      "Epoch 615/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2789 - acc: 0.8725 - val_loss: 0.2999 - val_acc: 0.8626\n",
      "Epoch 616/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2791 - acc: 0.8720 - val_loss: 0.3005 - val_acc: 0.8624\n",
      "Epoch 617/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2794 - acc: 0.8724 - val_loss: 0.3032 - val_acc: 0.8609\n",
      "Epoch 618/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2801 - acc: 0.8714 - val_loss: 0.2999 - val_acc: 0.8629\n",
      "Epoch 619/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2790 - acc: 0.8723 - val_loss: 0.3007 - val_acc: 0.8632\n",
      "Epoch 620/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2803 - acc: 0.8720 - val_loss: 0.2992 - val_acc: 0.8628\n",
      "Epoch 621/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2793 - acc: 0.8720 - val_loss: 0.2999 - val_acc: 0.8615\n",
      "Epoch 622/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2792 - acc: 0.8724 - val_loss: 0.2995 - val_acc: 0.8618\n",
      "Epoch 623/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2787 - acc: 0.8725 - val_loss: 0.2996 - val_acc: 0.8628\n",
      "Epoch 624/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2805 - acc: 0.8716 - val_loss: 0.2998 - val_acc: 0.8622\n",
      "Epoch 625/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2799 - acc: 0.8728 - val_loss: 0.2995 - val_acc: 0.8625\n",
      "Epoch 626/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2788 - acc: 0.8722 - val_loss: 0.2992 - val_acc: 0.8626\n",
      "Epoch 627/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2784 - acc: 0.8718 - val_loss: 0.2985 - val_acc: 0.8627\n",
      "Epoch 628/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2788 - acc: 0.8724 - val_loss: 0.3003 - val_acc: 0.8630\n",
      "Epoch 629/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2790 - acc: 0.8722 - val_loss: 0.2997 - val_acc: 0.8635\n",
      "Epoch 630/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2784 - acc: 0.8722 - val_loss: 0.2988 - val_acc: 0.8633\n",
      "Epoch 631/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2784 - acc: 0.8720 - val_loss: 0.2985 - val_acc: 0.8626\n",
      "Epoch 632/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2777 - acc: 0.8723 - val_loss: 0.2993 - val_acc: 0.8620\n",
      "Epoch 633/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2778 - acc: 0.8731 - val_loss: 0.2982 - val_acc: 0.8626\n",
      "Epoch 634/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2784 - acc: 0.8729 - val_loss: 0.2991 - val_acc: 0.8630\n",
      "Epoch 635/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2784 - acc: 0.8729 - val_loss: 0.2995 - val_acc: 0.8616\n",
      "Epoch 636/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2777 - acc: 0.8721 - val_loss: 0.2979 - val_acc: 0.8635\n",
      "Epoch 637/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2777 - acc: 0.8728 - val_loss: 0.3000 - val_acc: 0.8627\n",
      "Epoch 638/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2788 - acc: 0.8718 - val_loss: 0.2975 - val_acc: 0.8632\n",
      "Epoch 639/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2774 - acc: 0.8720 - val_loss: 0.2975 - val_acc: 0.8633\n",
      "Epoch 640/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2771 - acc: 0.8731 - val_loss: 0.2978 - val_acc: 0.8629\n",
      "Epoch 641/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2773 - acc: 0.8720 - val_loss: 0.2978 - val_acc: 0.8628\n",
      "Epoch 642/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2771 - acc: 0.8732 - val_loss: 0.2985 - val_acc: 0.8628\n",
      "Epoch 643/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2778 - acc: 0.8727 - val_loss: 0.2971 - val_acc: 0.8631\n",
      "Epoch 644/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2769 - acc: 0.8732 - val_loss: 0.2977 - val_acc: 0.8636\n",
      "Epoch 645/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2766 - acc: 0.8731 - val_loss: 0.2973 - val_acc: 0.8634\n",
      "Epoch 646/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2767 - acc: 0.8732 - val_loss: 0.2975 - val_acc: 0.8624\n",
      "Epoch 647/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2770 - acc: 0.8732 - val_loss: 0.2984 - val_acc: 0.8623\n",
      "Epoch 648/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2769 - acc: 0.8735 - val_loss: 0.2971 - val_acc: 0.8630\n",
      "Epoch 649/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2763 - acc: 0.8732 - val_loss: 0.2966 - val_acc: 0.8633\n",
      "Epoch 650/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2764 - acc: 0.8733 - val_loss: 0.2991 - val_acc: 0.8630\n",
      "Epoch 651/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2770 - acc: 0.8730 - val_loss: 0.2976 - val_acc: 0.8632\n",
      "Epoch 652/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2772 - acc: 0.8730 - val_loss: 0.2974 - val_acc: 0.8634\n",
      "Epoch 653/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2768 - acc: 0.8730 - val_loss: 0.2979 - val_acc: 0.8622\n",
      "Epoch 654/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2767 - acc: 0.8728 - val_loss: 0.2969 - val_acc: 0.8626\n",
      "Epoch 655/1000\n",
      "66667/66667 [==============================] - 0s 3us/step - loss: 0.2770 - acc: 0.8728 - val_loss: 0.2970 - val_acc: 0.8631\n",
      "Epoch 656/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2766 - acc: 0.8732 - val_loss: 0.2976 - val_acc: 0.8633\n",
      "Epoch 657/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2779 - acc: 0.8724 - val_loss: 0.3004 - val_acc: 0.8619\n",
      "Epoch 658/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2778 - acc: 0.8727 - val_loss: 0.2990 - val_acc: 0.8620\n",
      "Epoch 659/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2767 - acc: 0.8725 - val_loss: 0.2966 - val_acc: 0.8629\n",
      "Epoch 660/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2776 - acc: 0.8724 - val_loss: 0.2973 - val_acc: 0.8633\n",
      "Epoch 661/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2779 - acc: 0.8726 - val_loss: 0.3030 - val_acc: 0.8603\n",
      "Epoch 662/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2806 - acc: 0.8718 - val_loss: 0.3059 - val_acc: 0.8584\n",
      "Epoch 663/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2794 - acc: 0.8722 - val_loss: 0.2997 - val_acc: 0.8619\n",
      "Epoch 664/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2775 - acc: 0.8729 - val_loss: 0.2977 - val_acc: 0.8623\n",
      "Epoch 665/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2775 - acc: 0.8724 - val_loss: 0.2996 - val_acc: 0.8621\n",
      "Epoch 666/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2771 - acc: 0.8723 - val_loss: 0.2973 - val_acc: 0.8639\n",
      "Epoch 667/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2764 - acc: 0.8729 - val_loss: 0.3009 - val_acc: 0.8616\n",
      "Epoch 668/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2782 - acc: 0.8718 - val_loss: 0.2965 - val_acc: 0.8633\n",
      "Epoch 669/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2763 - acc: 0.8720 - val_loss: 0.2973 - val_acc: 0.8624\n",
      "Epoch 670/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2759 - acc: 0.8730 - val_loss: 0.2965 - val_acc: 0.8630\n",
      "Epoch 671/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2750 - acc: 0.8732 - val_loss: 0.2965 - val_acc: 0.8638\n",
      "Epoch 672/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2757 - acc: 0.8730 - val_loss: 0.2956 - val_acc: 0.8631\n",
      "Epoch 673/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2760 - acc: 0.8730 - val_loss: 0.2979 - val_acc: 0.8612\n",
      "Epoch 674/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2754 - acc: 0.8732 - val_loss: 0.2962 - val_acc: 0.8629\n",
      "Epoch 675/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2750 - acc: 0.8730 - val_loss: 0.2961 - val_acc: 0.8631\n",
      "Epoch 676/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2751 - acc: 0.8732 - val_loss: 0.2955 - val_acc: 0.8632\n",
      "Epoch 677/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2763 - acc: 0.8728 - val_loss: 0.2966 - val_acc: 0.8630\n",
      "Epoch 678/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2762 - acc: 0.8728 - val_loss: 0.2967 - val_acc: 0.8628\n",
      "Epoch 679/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2760 - acc: 0.8730 - val_loss: 0.2998 - val_acc: 0.8615\n",
      "Epoch 680/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2774 - acc: 0.8726 - val_loss: 0.3005 - val_acc: 0.8623\n",
      "Epoch 681/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2771 - acc: 0.8722 - val_loss: 0.2977 - val_acc: 0.8621\n",
      "Epoch 682/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2762 - acc: 0.8722 - val_loss: 0.2953 - val_acc: 0.8633\n",
      "Epoch 683/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2751 - acc: 0.8732 - val_loss: 0.2972 - val_acc: 0.8632\n",
      "Epoch 684/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2772 - acc: 0.8726 - val_loss: 0.2972 - val_acc: 0.8629\n",
      "Epoch 685/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2778 - acc: 0.8725 - val_loss: 0.2976 - val_acc: 0.8625\n",
      "Epoch 686/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2776 - acc: 0.8717 - val_loss: 0.2968 - val_acc: 0.8625\n",
      "Epoch 687/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2762 - acc: 0.8719 - val_loss: 0.2960 - val_acc: 0.8623\n",
      "Epoch 688/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2754 - acc: 0.8732 - val_loss: 0.2976 - val_acc: 0.8623\n",
      "Epoch 689/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2758 - acc: 0.8723 - val_loss: 0.2974 - val_acc: 0.8623\n",
      "Epoch 690/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2755 - acc: 0.8724 - val_loss: 0.2965 - val_acc: 0.8627\n",
      "Epoch 691/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2750 - acc: 0.8728 - val_loss: 0.2963 - val_acc: 0.8631\n",
      "Epoch 692/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2749 - acc: 0.8729 - val_loss: 0.2947 - val_acc: 0.8633\n",
      "Epoch 693/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2746 - acc: 0.8732 - val_loss: 0.2952 - val_acc: 0.8625\n",
      "Epoch 694/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2740 - acc: 0.8726 - val_loss: 0.2951 - val_acc: 0.8630\n",
      "Epoch 695/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2743 - acc: 0.8729 - val_loss: 0.2953 - val_acc: 0.8622\n",
      "Epoch 696/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2747 - acc: 0.8725 - val_loss: 0.2958 - val_acc: 0.8625\n",
      "Epoch 697/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2739 - acc: 0.8737 - val_loss: 0.2949 - val_acc: 0.8635\n",
      "Epoch 698/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2744 - acc: 0.8732 - val_loss: 0.2957 - val_acc: 0.8625\n",
      "Epoch 699/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2739 - acc: 0.8732 - val_loss: 0.2961 - val_acc: 0.8630\n",
      "Epoch 700/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2739 - acc: 0.8734 - val_loss: 0.2963 - val_acc: 0.8630\n",
      "Epoch 701/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2747 - acc: 0.8730 - val_loss: 0.2943 - val_acc: 0.8630\n",
      "Epoch 702/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2740 - acc: 0.8731 - val_loss: 0.2952 - val_acc: 0.8634\n",
      "Epoch 703/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2738 - acc: 0.8732 - val_loss: 0.2944 - val_acc: 0.8627\n",
      "Epoch 704/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2730 - acc: 0.8730 - val_loss: 0.2947 - val_acc: 0.8630\n",
      "Epoch 705/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2731 - acc: 0.8734 - val_loss: 0.2948 - val_acc: 0.8633\n",
      "Epoch 706/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2731 - acc: 0.8736 - val_loss: 0.2950 - val_acc: 0.8629\n",
      "Epoch 707/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2732 - acc: 0.8727 - val_loss: 0.2948 - val_acc: 0.8634\n",
      "Epoch 708/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2753 - acc: 0.8729 - val_loss: 0.2963 - val_acc: 0.8628\n",
      "Epoch 709/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2742 - acc: 0.8728 - val_loss: 0.2956 - val_acc: 0.8632\n",
      "Epoch 710/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2737 - acc: 0.8733 - val_loss: 0.2952 - val_acc: 0.8635\n",
      "Epoch 711/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2740 - acc: 0.8731 - val_loss: 0.2966 - val_acc: 0.8626\n",
      "Epoch 712/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2741 - acc: 0.8733 - val_loss: 0.2954 - val_acc: 0.8630\n",
      "Epoch 713/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2737 - acc: 0.8727 - val_loss: 0.2942 - val_acc: 0.8631\n",
      "Epoch 714/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2731 - acc: 0.8726 - val_loss: 0.2942 - val_acc: 0.8635\n",
      "Epoch 715/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2745 - acc: 0.8730 - val_loss: 0.2949 - val_acc: 0.8622\n",
      "Epoch 716/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2738 - acc: 0.8728 - val_loss: 0.2947 - val_acc: 0.8632\n",
      "Epoch 717/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2740 - acc: 0.8724 - val_loss: 0.2941 - val_acc: 0.8626\n",
      "Epoch 718/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2736 - acc: 0.8725 - val_loss: 0.2935 - val_acc: 0.8625\n",
      "Epoch 719/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2730 - acc: 0.8733 - val_loss: 0.2939 - val_acc: 0.8638\n",
      "Epoch 720/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2732 - acc: 0.8729 - val_loss: 0.2938 - val_acc: 0.8633\n",
      "Epoch 721/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2729 - acc: 0.8728 - val_loss: 0.2951 - val_acc: 0.8625\n",
      "Epoch 722/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2740 - acc: 0.8727 - val_loss: 0.2951 - val_acc: 0.8636\n",
      "Epoch 723/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2740 - acc: 0.8729 - val_loss: 0.2979 - val_acc: 0.8614\n",
      "Epoch 724/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2740 - acc: 0.8724 - val_loss: 0.2945 - val_acc: 0.8620\n",
      "Epoch 725/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2728 - acc: 0.8727 - val_loss: 0.2943 - val_acc: 0.8632\n",
      "Epoch 726/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2727 - acc: 0.8731 - val_loss: 0.2938 - val_acc: 0.8632\n",
      "Epoch 727/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2728 - acc: 0.8730 - val_loss: 0.2943 - val_acc: 0.8626\n",
      "Epoch 728/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2726 - acc: 0.8729 - val_loss: 0.2957 - val_acc: 0.8632\n",
      "Epoch 729/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2745 - acc: 0.8719 - val_loss: 0.2971 - val_acc: 0.8622\n",
      "Epoch 730/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2744 - acc: 0.8722 - val_loss: 0.2935 - val_acc: 0.8626\n",
      "Epoch 731/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2727 - acc: 0.8725 - val_loss: 0.2945 - val_acc: 0.8628\n",
      "Epoch 732/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2727 - acc: 0.8720 - val_loss: 0.2945 - val_acc: 0.8635\n",
      "Epoch 733/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2720 - acc: 0.8732 - val_loss: 0.2944 - val_acc: 0.8634\n",
      "Epoch 734/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2720 - acc: 0.8731 - val_loss: 0.2935 - val_acc: 0.8629\n",
      "Epoch 735/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2717 - acc: 0.8736 - val_loss: 0.2932 - val_acc: 0.8630\n",
      "Epoch 736/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2718 - acc: 0.8728 - val_loss: 0.2946 - val_acc: 0.8627\n",
      "Epoch 737/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2714 - acc: 0.8736 - val_loss: 0.2928 - val_acc: 0.8630\n",
      "Epoch 738/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2714 - acc: 0.8736 - val_loss: 0.2939 - val_acc: 0.8626\n",
      "Epoch 739/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2721 - acc: 0.8730 - val_loss: 0.2934 - val_acc: 0.8631\n",
      "Epoch 740/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2718 - acc: 0.8727 - val_loss: 0.2930 - val_acc: 0.8631\n",
      "Epoch 741/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2724 - acc: 0.8729 - val_loss: 0.2945 - val_acc: 0.8619\n",
      "Epoch 742/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2723 - acc: 0.8731 - val_loss: 0.2925 - val_acc: 0.8627\n",
      "Epoch 743/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2716 - acc: 0.8733 - val_loss: 0.2959 - val_acc: 0.8632\n",
      "Epoch 744/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2729 - acc: 0.8729 - val_loss: 0.2937 - val_acc: 0.8623\n",
      "Epoch 745/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2725 - acc: 0.8736 - val_loss: 0.2945 - val_acc: 0.8622\n",
      "Epoch 746/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2720 - acc: 0.8730 - val_loss: 0.2930 - val_acc: 0.8627\n",
      "Epoch 747/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2716 - acc: 0.8732 - val_loss: 0.2924 - val_acc: 0.8630\n",
      "Epoch 748/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2724 - acc: 0.8734 - val_loss: 0.2944 - val_acc: 0.8632\n",
      "Epoch 749/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2717 - acc: 0.8731 - val_loss: 0.2926 - val_acc: 0.8627\n",
      "Epoch 750/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2723 - acc: 0.8728 - val_loss: 0.2924 - val_acc: 0.8631\n",
      "Epoch 751/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2710 - acc: 0.8733 - val_loss: 0.2939 - val_acc: 0.8629\n",
      "Epoch 752/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2710 - acc: 0.8737 - val_loss: 0.2929 - val_acc: 0.8630\n",
      "Epoch 753/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2714 - acc: 0.8728 - val_loss: 0.2940 - val_acc: 0.8626\n",
      "Epoch 754/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2725 - acc: 0.8728 - val_loss: 0.2932 - val_acc: 0.8633\n",
      "Epoch 755/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2712 - acc: 0.8731 - val_loss: 0.2927 - val_acc: 0.8635\n",
      "Epoch 756/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2705 - acc: 0.8729 - val_loss: 0.2923 - val_acc: 0.8623\n",
      "Epoch 757/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2706 - acc: 0.8735 - val_loss: 0.2923 - val_acc: 0.8632\n",
      "Epoch 758/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2707 - acc: 0.8729 - val_loss: 0.2925 - val_acc: 0.8633\n",
      "Epoch 759/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2715 - acc: 0.8737 - val_loss: 0.2961 - val_acc: 0.8627\n",
      "Epoch 760/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2718 - acc: 0.8725 - val_loss: 0.2935 - val_acc: 0.8628\n",
      "Epoch 761/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2714 - acc: 0.8735 - val_loss: 0.2926 - val_acc: 0.8627\n",
      "Epoch 762/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2708 - acc: 0.8736 - val_loss: 0.2937 - val_acc: 0.8630\n",
      "Epoch 763/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2709 - acc: 0.8730 - val_loss: 0.2920 - val_acc: 0.8626\n",
      "Epoch 764/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2703 - acc: 0.8738 - val_loss: 0.2924 - val_acc: 0.8633\n",
      "Epoch 765/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2705 - acc: 0.8734 - val_loss: 0.2939 - val_acc: 0.8619\n",
      "Epoch 766/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2707 - acc: 0.8739 - val_loss: 0.2921 - val_acc: 0.8636\n",
      "Epoch 767/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2714 - acc: 0.8728 - val_loss: 0.2935 - val_acc: 0.8624\n",
      "Epoch 768/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2720 - acc: 0.8732 - val_loss: 0.2934 - val_acc: 0.8624\n",
      "Epoch 769/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2720 - acc: 0.8732 - val_loss: 0.2926 - val_acc: 0.8629\n",
      "Epoch 770/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2700 - acc: 0.8734 - val_loss: 0.2929 - val_acc: 0.8632\n",
      "Epoch 771/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2706 - acc: 0.8727 - val_loss: 0.2919 - val_acc: 0.8635\n",
      "Epoch 772/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2698 - acc: 0.8735 - val_loss: 0.2921 - val_acc: 0.8627\n",
      "Epoch 773/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2701 - acc: 0.8731 - val_loss: 0.2917 - val_acc: 0.8634\n",
      "Epoch 774/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2705 - acc: 0.8740 - val_loss: 0.2917 - val_acc: 0.8628\n",
      "Epoch 775/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2704 - acc: 0.8735 - val_loss: 0.2917 - val_acc: 0.8638\n",
      "Epoch 776/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2700 - acc: 0.8732 - val_loss: 0.2921 - val_acc: 0.8635\n",
      "Epoch 777/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2705 - acc: 0.8732 - val_loss: 0.2920 - val_acc: 0.8636\n",
      "Epoch 778/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2705 - acc: 0.8730 - val_loss: 0.2925 - val_acc: 0.8623\n",
      "Epoch 779/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2698 - acc: 0.8735 - val_loss: 0.2914 - val_acc: 0.8637\n",
      "Epoch 780/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2694 - acc: 0.8735 - val_loss: 0.2908 - val_acc: 0.8638\n",
      "Epoch 781/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2695 - acc: 0.8736 - val_loss: 0.2920 - val_acc: 0.8632\n",
      "Epoch 782/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2699 - acc: 0.8736 - val_loss: 0.2911 - val_acc: 0.8639\n",
      "Epoch 783/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2696 - acc: 0.8743 - val_loss: 0.2912 - val_acc: 0.8634\n",
      "Epoch 784/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2700 - acc: 0.8732 - val_loss: 0.2919 - val_acc: 0.8633\n",
      "Epoch 785/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2708 - acc: 0.8726 - val_loss: 0.2920 - val_acc: 0.8626\n",
      "Epoch 786/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2698 - acc: 0.8734 - val_loss: 0.2924 - val_acc: 0.8630\n",
      "Epoch 787/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2697 - acc: 0.8731 - val_loss: 0.2916 - val_acc: 0.8630\n",
      "Epoch 788/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2694 - acc: 0.8734 - val_loss: 0.2917 - val_acc: 0.8636\n",
      "Epoch 789/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2697 - acc: 0.8743 - val_loss: 0.2927 - val_acc: 0.8631\n",
      "Epoch 790/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2697 - acc: 0.8737 - val_loss: 0.2908 - val_acc: 0.8640\n",
      "Epoch 791/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2694 - acc: 0.8740 - val_loss: 0.2918 - val_acc: 0.8644\n",
      "Epoch 792/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2692 - acc: 0.8740 - val_loss: 0.2913 - val_acc: 0.8633\n",
      "Epoch 793/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2695 - acc: 0.8740 - val_loss: 0.2916 - val_acc: 0.8628\n",
      "Epoch 794/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2694 - acc: 0.8735 - val_loss: 0.2920 - val_acc: 0.8630\n",
      "Epoch 795/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2697 - acc: 0.8733 - val_loss: 0.2909 - val_acc: 0.8633\n",
      "Epoch 796/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2693 - acc: 0.8734 - val_loss: 0.2912 - val_acc: 0.8636\n",
      "Epoch 797/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2692 - acc: 0.8741 - val_loss: 0.2907 - val_acc: 0.8637\n",
      "Epoch 798/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2694 - acc: 0.8741 - val_loss: 0.2918 - val_acc: 0.8630\n",
      "Epoch 799/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2696 - acc: 0.8738 - val_loss: 0.2912 - val_acc: 0.8638\n",
      "Epoch 800/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2698 - acc: 0.8738 - val_loss: 0.2923 - val_acc: 0.8631\n",
      "Epoch 801/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2693 - acc: 0.8736 - val_loss: 0.2912 - val_acc: 0.8627\n",
      "Epoch 802/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2696 - acc: 0.8733 - val_loss: 0.2913 - val_acc: 0.8637\n",
      "Epoch 803/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2699 - acc: 0.8739 - val_loss: 0.2906 - val_acc: 0.8639\n",
      "Epoch 804/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2692 - acc: 0.8741 - val_loss: 0.2929 - val_acc: 0.8630\n",
      "Epoch 805/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2696 - acc: 0.8741 - val_loss: 0.2908 - val_acc: 0.8633\n",
      "Epoch 806/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2701 - acc: 0.8735 - val_loss: 0.2911 - val_acc: 0.8625\n",
      "Epoch 807/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2710 - acc: 0.8741 - val_loss: 0.2934 - val_acc: 0.8619\n",
      "Epoch 808/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2715 - acc: 0.8720 - val_loss: 0.2936 - val_acc: 0.8624\n",
      "Epoch 809/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2702 - acc: 0.8736 - val_loss: 0.2911 - val_acc: 0.8641\n",
      "Epoch 810/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8738 - val_loss: 0.2901 - val_acc: 0.8638\n",
      "Epoch 811/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2689 - acc: 0.8737 - val_loss: 0.2920 - val_acc: 0.8640\n",
      "Epoch 812/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2695 - acc: 0.8742 - val_loss: 0.2934 - val_acc: 0.8640\n",
      "Epoch 813/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2696 - acc: 0.8739 - val_loss: 0.2905 - val_acc: 0.8641\n",
      "Epoch 814/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2688 - acc: 0.8744 - val_loss: 0.2906 - val_acc: 0.8633\n",
      "Epoch 815/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2688 - acc: 0.8740 - val_loss: 0.2915 - val_acc: 0.8631\n",
      "Epoch 816/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2691 - acc: 0.8739 - val_loss: 0.2900 - val_acc: 0.8637\n",
      "Epoch 817/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2693 - acc: 0.8732 - val_loss: 0.2910 - val_acc: 0.8642\n",
      "Epoch 818/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8740 - val_loss: 0.2903 - val_acc: 0.8638\n",
      "Epoch 819/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2680 - acc: 0.8737 - val_loss: 0.2898 - val_acc: 0.8637\n",
      "Epoch 820/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2689 - acc: 0.8732 - val_loss: 0.2915 - val_acc: 0.8634\n",
      "Epoch 821/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2692 - acc: 0.8732 - val_loss: 0.2917 - val_acc: 0.8635\n",
      "Epoch 822/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2683 - acc: 0.8740 - val_loss: 0.2904 - val_acc: 0.8635\n",
      "Epoch 823/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8740 - val_loss: 0.2911 - val_acc: 0.8632\n",
      "Epoch 824/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8739 - val_loss: 0.2920 - val_acc: 0.8632\n",
      "Epoch 825/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8738 - val_loss: 0.2919 - val_acc: 0.8629\n",
      "Epoch 826/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2695 - acc: 0.8739 - val_loss: 0.2912 - val_acc: 0.8637\n",
      "Epoch 827/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2692 - acc: 0.8732 - val_loss: 0.2909 - val_acc: 0.8639\n",
      "Epoch 828/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8737 - val_loss: 0.2940 - val_acc: 0.8625\n",
      "Epoch 829/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2696 - acc: 0.8738 - val_loss: 0.2902 - val_acc: 0.8632\n",
      "Epoch 830/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2688 - acc: 0.8735 - val_loss: 0.2906 - val_acc: 0.8634\n",
      "Epoch 831/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2689 - acc: 0.8741 - val_loss: 0.2946 - val_acc: 0.8629\n",
      "Epoch 832/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2701 - acc: 0.8737 - val_loss: 0.2905 - val_acc: 0.8642\n",
      "Epoch 833/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2690 - acc: 0.8739 - val_loss: 0.2924 - val_acc: 0.8631\n",
      "Epoch 834/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2688 - acc: 0.8736 - val_loss: 0.2895 - val_acc: 0.8636\n",
      "Epoch 835/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2686 - acc: 0.8736 - val_loss: 0.2926 - val_acc: 0.8627\n",
      "Epoch 836/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2695 - acc: 0.8735 - val_loss: 0.2909 - val_acc: 0.8636\n",
      "Epoch 837/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2683 - acc: 0.8739 - val_loss: 0.2897 - val_acc: 0.8642\n",
      "Epoch 838/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8741 - val_loss: 0.2913 - val_acc: 0.8625\n",
      "Epoch 839/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2683 - acc: 0.8736 - val_loss: 0.2896 - val_acc: 0.8635\n",
      "Epoch 840/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2678 - acc: 0.8743 - val_loss: 0.2914 - val_acc: 0.8630\n",
      "Epoch 841/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8741 - val_loss: 0.2907 - val_acc: 0.8643\n",
      "Epoch 842/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2682 - acc: 0.8740 - val_loss: 0.2907 - val_acc: 0.8628\n",
      "Epoch 843/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2682 - acc: 0.8732 - val_loss: 0.2892 - val_acc: 0.8637\n",
      "Epoch 844/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2679 - acc: 0.8742 - val_loss: 0.2914 - val_acc: 0.8636\n",
      "Epoch 845/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2696 - acc: 0.8746 - val_loss: 0.2923 - val_acc: 0.8630\n",
      "Epoch 846/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2685 - acc: 0.8739 - val_loss: 0.2898 - val_acc: 0.8635\n",
      "Epoch 847/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2675 - acc: 0.8736 - val_loss: 0.2891 - val_acc: 0.8635\n",
      "Epoch 848/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2666 - acc: 0.8741 - val_loss: 0.2908 - val_acc: 0.8627\n",
      "Epoch 849/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2670 - acc: 0.8741 - val_loss: 0.2892 - val_acc: 0.8630\n",
      "Epoch 850/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2667 - acc: 0.8743 - val_loss: 0.2893 - val_acc: 0.8631\n",
      "Epoch 851/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2674 - acc: 0.8744 - val_loss: 0.2891 - val_acc: 0.8646\n",
      "Epoch 852/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2674 - acc: 0.8736 - val_loss: 0.2895 - val_acc: 0.8637\n",
      "Epoch 853/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2676 - acc: 0.8745 - val_loss: 0.2893 - val_acc: 0.8639\n",
      "Epoch 854/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2673 - acc: 0.8737 - val_loss: 0.2900 - val_acc: 0.8629\n",
      "Epoch 855/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2675 - acc: 0.8740 - val_loss: 0.2896 - val_acc: 0.8630\n",
      "Epoch 856/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2670 - acc: 0.8742 - val_loss: 0.2895 - val_acc: 0.8635\n",
      "Epoch 857/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2669 - acc: 0.8741 - val_loss: 0.2902 - val_acc: 0.8630\n",
      "Epoch 858/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2674 - acc: 0.8743 - val_loss: 0.2904 - val_acc: 0.8635\n",
      "Epoch 859/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2668 - acc: 0.8744 - val_loss: 0.2900 - val_acc: 0.8636\n",
      "Epoch 860/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8742 - val_loss: 0.2893 - val_acc: 0.8629\n",
      "Epoch 861/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2678 - acc: 0.8745 - val_loss: 0.2890 - val_acc: 0.8640\n",
      "Epoch 862/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2680 - acc: 0.8736 - val_loss: 0.2899 - val_acc: 0.8622\n",
      "Epoch 863/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2673 - acc: 0.8739 - val_loss: 0.2913 - val_acc: 0.8626\n",
      "Epoch 864/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8740 - val_loss: 0.2892 - val_acc: 0.8632\n",
      "Epoch 865/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2680 - acc: 0.8735 - val_loss: 0.2904 - val_acc: 0.8620\n",
      "Epoch 866/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2671 - acc: 0.8744 - val_loss: 0.2896 - val_acc: 0.8646\n",
      "Epoch 867/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2673 - acc: 0.8741 - val_loss: 0.2900 - val_acc: 0.8632\n",
      "Epoch 868/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2676 - acc: 0.8737 - val_loss: 0.2913 - val_acc: 0.8629\n",
      "Epoch 869/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2681 - acc: 0.8740 - val_loss: 0.2888 - val_acc: 0.8631\n",
      "Epoch 870/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2666 - acc: 0.8749 - val_loss: 0.2883 - val_acc: 0.8636\n",
      "Epoch 871/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2663 - acc: 0.8743 - val_loss: 0.2899 - val_acc: 0.8623\n",
      "Epoch 872/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2671 - acc: 0.8733 - val_loss: 0.2886 - val_acc: 0.8628\n",
      "Epoch 873/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2669 - acc: 0.8737 - val_loss: 0.2897 - val_acc: 0.8635\n",
      "Epoch 874/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2674 - acc: 0.8742 - val_loss: 0.2892 - val_acc: 0.8638\n",
      "Epoch 875/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2671 - acc: 0.8737 - val_loss: 0.2909 - val_acc: 0.8625\n",
      "Epoch 876/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2678 - acc: 0.8738 - val_loss: 0.2899 - val_acc: 0.8630\n",
      "Epoch 877/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2677 - acc: 0.8742 - val_loss: 0.2886 - val_acc: 0.8632\n",
      "Epoch 878/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2671 - acc: 0.8745 - val_loss: 0.2919 - val_acc: 0.8621\n",
      "Epoch 879/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2667 - acc: 0.8737 - val_loss: 0.2882 - val_acc: 0.8639\n",
      "Epoch 880/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2665 - acc: 0.8737 - val_loss: 0.2888 - val_acc: 0.8631\n",
      "Epoch 881/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2673 - acc: 0.8737 - val_loss: 0.2898 - val_acc: 0.8632\n",
      "Epoch 882/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2668 - acc: 0.8745 - val_loss: 0.2886 - val_acc: 0.8634\n",
      "Epoch 883/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2657 - acc: 0.8747 - val_loss: 0.2888 - val_acc: 0.8645\n",
      "Epoch 884/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2665 - acc: 0.8743 - val_loss: 0.2887 - val_acc: 0.8638\n",
      "Epoch 885/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2669 - acc: 0.8746 - val_loss: 0.2904 - val_acc: 0.8632\n",
      "Epoch 886/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2669 - acc: 0.8742 - val_loss: 0.2880 - val_acc: 0.8643\n",
      "Epoch 887/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2663 - acc: 0.8739 - val_loss: 0.2886 - val_acc: 0.8631\n",
      "Epoch 888/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2666 - acc: 0.8742 - val_loss: 0.2880 - val_acc: 0.8633\n",
      "Epoch 889/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2667 - acc: 0.8740 - val_loss: 0.2903 - val_acc: 0.8631\n",
      "Epoch 890/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2676 - acc: 0.8741 - val_loss: 0.2900 - val_acc: 0.8626\n",
      "Epoch 891/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2675 - acc: 0.8748 - val_loss: 0.2902 - val_acc: 0.8632\n",
      "Epoch 892/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2664 - acc: 0.8743 - val_loss: 0.2876 - val_acc: 0.8646\n",
      "Epoch 893/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2655 - acc: 0.8747 - val_loss: 0.2881 - val_acc: 0.8635\n",
      "Epoch 894/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2659 - acc: 0.8741 - val_loss: 0.2887 - val_acc: 0.8626\n",
      "Epoch 895/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2653 - acc: 0.8742 - val_loss: 0.2879 - val_acc: 0.8634\n",
      "Epoch 896/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2653 - acc: 0.8746 - val_loss: 0.2880 - val_acc: 0.8636\n",
      "Epoch 897/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2651 - acc: 0.8749 - val_loss: 0.2877 - val_acc: 0.8638\n",
      "Epoch 898/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2651 - acc: 0.8747 - val_loss: 0.2886 - val_acc: 0.8639\n",
      "Epoch 899/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2652 - acc: 0.8746 - val_loss: 0.2876 - val_acc: 0.8633\n",
      "Epoch 900/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2657 - acc: 0.8740 - val_loss: 0.2875 - val_acc: 0.8641\n",
      "Epoch 901/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2660 - acc: 0.8743 - val_loss: 0.2896 - val_acc: 0.8630\n",
      "Epoch 902/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2659 - acc: 0.8747 - val_loss: 0.2880 - val_acc: 0.8628\n",
      "Epoch 903/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2653 - acc: 0.8740 - val_loss: 0.2877 - val_acc: 0.8641\n",
      "Epoch 904/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2649 - acc: 0.8754 - val_loss: 0.2878 - val_acc: 0.8630\n",
      "Epoch 905/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2659 - acc: 0.8748 - val_loss: 0.2908 - val_acc: 0.8634\n",
      "Epoch 906/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2672 - acc: 0.8737 - val_loss: 0.2880 - val_acc: 0.8643\n",
      "Epoch 907/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2656 - acc: 0.8748 - val_loss: 0.2878 - val_acc: 0.8639\n",
      "Epoch 908/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2656 - acc: 0.8744 - val_loss: 0.2886 - val_acc: 0.8635\n",
      "Epoch 909/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2655 - acc: 0.8739 - val_loss: 0.2887 - val_acc: 0.8641\n",
      "Epoch 910/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2662 - acc: 0.8744 - val_loss: 0.2877 - val_acc: 0.8638\n",
      "Epoch 911/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2656 - acc: 0.8749 - val_loss: 0.2875 - val_acc: 0.8632\n",
      "Epoch 912/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2668 - acc: 0.8740 - val_loss: 0.2909 - val_acc: 0.8631\n",
      "Epoch 913/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2669 - acc: 0.8738 - val_loss: 0.2890 - val_acc: 0.8635\n",
      "Epoch 914/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2664 - acc: 0.8738 - val_loss: 0.2871 - val_acc: 0.8634\n",
      "Epoch 915/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2655 - acc: 0.8745 - val_loss: 0.2876 - val_acc: 0.8634\n",
      "Epoch 916/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2651 - acc: 0.8747 - val_loss: 0.2893 - val_acc: 0.8636\n",
      "Epoch 917/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2658 - acc: 0.8743 - val_loss: 0.2881 - val_acc: 0.8633\n",
      "Epoch 918/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2663 - acc: 0.8742 - val_loss: 0.2880 - val_acc: 0.8643\n",
      "Epoch 919/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2651 - acc: 0.8745 - val_loss: 0.2873 - val_acc: 0.8633\n",
      "Epoch 920/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2657 - acc: 0.8741 - val_loss: 0.2880 - val_acc: 0.8634\n",
      "Epoch 921/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2659 - acc: 0.8744 - val_loss: 0.2870 - val_acc: 0.8630\n",
      "Epoch 922/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2654 - acc: 0.8741 - val_loss: 0.2882 - val_acc: 0.8639\n",
      "Epoch 923/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2670 - acc: 0.8740 - val_loss: 0.2874 - val_acc: 0.8647\n",
      "Epoch 924/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2664 - acc: 0.8741 - val_loss: 0.2910 - val_acc: 0.8626\n",
      "Epoch 925/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2664 - acc: 0.8749 - val_loss: 0.2885 - val_acc: 0.8636\n",
      "Epoch 926/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2660 - acc: 0.8734 - val_loss: 0.2870 - val_acc: 0.8635\n",
      "Epoch 927/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2657 - acc: 0.8743 - val_loss: 0.2886 - val_acc: 0.8630\n",
      "Epoch 928/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2649 - acc: 0.8747 - val_loss: 0.2870 - val_acc: 0.8635\n",
      "Epoch 929/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2647 - acc: 0.8743 - val_loss: 0.2889 - val_acc: 0.8631\n",
      "Epoch 930/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2653 - acc: 0.8742 - val_loss: 0.2866 - val_acc: 0.8645\n",
      "Epoch 931/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2645 - acc: 0.8741 - val_loss: 0.2874 - val_acc: 0.8626\n",
      "Epoch 932/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2645 - acc: 0.8746 - val_loss: 0.2869 - val_acc: 0.8639\n",
      "Epoch 933/1000\n",
      "66667/66667 [==============================] - 0s 2us/step - loss: 0.2656 - acc: 0.8740 - val_loss: 0.2903 - val_acc: 0.8630\n",
      "Epoch 934/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2659 - acc: 0.8734 - val_loss: 0.2867 - val_acc: 0.8635\n",
      "Epoch 935/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2647 - acc: 0.8745 - val_loss: 0.2872 - val_acc: 0.8646\n",
      "Epoch 936/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2660 - acc: 0.8742 - val_loss: 0.2889 - val_acc: 0.8627\n",
      "Epoch 937/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2647 - acc: 0.8744 - val_loss: 0.2872 - val_acc: 0.8626\n",
      "Epoch 938/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2648 - acc: 0.8743 - val_loss: 0.2888 - val_acc: 0.8629\n",
      "Epoch 939/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2646 - acc: 0.8740 - val_loss: 0.2867 - val_acc: 0.8639\n",
      "Epoch 940/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2644 - acc: 0.8749 - val_loss: 0.2884 - val_acc: 0.8635\n",
      "Epoch 941/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2652 - acc: 0.8737 - val_loss: 0.2874 - val_acc: 0.8634\n",
      "Epoch 942/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2647 - acc: 0.8742 - val_loss: 0.2867 - val_acc: 0.8640\n",
      "Epoch 943/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2646 - acc: 0.8743 - val_loss: 0.2883 - val_acc: 0.8626\n",
      "Epoch 944/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2657 - acc: 0.8740 - val_loss: 0.2892 - val_acc: 0.8622\n",
      "Epoch 945/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2657 - acc: 0.8737 - val_loss: 0.2880 - val_acc: 0.8632\n",
      "Epoch 946/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2663 - acc: 0.8740 - val_loss: 0.2874 - val_acc: 0.8630\n",
      "Epoch 947/1000\n",
      "66667/66667 [==============================] - ETA: 0s - loss: 0.2645 - acc: 0.874 - 0s 1us/step - loss: 0.2645 - acc: 0.8744 - val_loss: 0.2880 - val_acc: 0.8629\n",
      "Epoch 948/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2641 - acc: 0.8743 - val_loss: 0.2878 - val_acc: 0.8632\n",
      "Epoch 949/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2641 - acc: 0.8742 - val_loss: 0.2868 - val_acc: 0.8639\n",
      "Epoch 950/1000\n",
      "66667/66667 [==============================] - 0s 1us/step - loss: 0.2638 - acc: 0.8747 - val_loss: 0.2867 - val_acc: 0.8632\n",
      "Epoch 00950: early stopping\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'dense_4_input' with dtype float and shape [?,15]\n\t [[Node: dense_4_input = Placeholder[dtype=DT_FLOAT, shape=[?,15], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d25399303002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                             x0=default_parameters)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> Final Results: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6d4c55eb46d1>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, num_dense_layers, num_dense_nodes, activation, dropout_rate, decay_rate, batch_dimension)\u001b[0m\n\u001b[1;32m     42\u001b[0m         history = model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=1, epochs = 1000, batch_size=batch_dimension, class_weight=class_weights,shuffle=True,\n\u001b[1;32m     43\u001b[0m                             callbacks=[early_stop])\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mpredi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moos_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moos_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1493\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'dense_4_input' with dtype float and shape [?,15]\n\t [[Node: dense_4_input = Placeholder[dtype=DT_FLOAT, shape=[?,15], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "default_parameters = [1e-3, 2, 10, 'relu', 0.0, 1e-5, 1024]\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=5000,\n",
    "                            x0=default_parameters)\n",
    "\n",
    "print(\">>> Final Results: \")\n",
    "print()\n",
    "print(\"BEST RESULTS: {0}\".format(search_result.x))\n",
    "print()\n",
    "print(\"BEST ACCURACY: {0}\".format(-search_result.fun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_names = ['learning_rate', 'num_dense_nodes', 'num_dense_layers', 'dropout_rate', 'decay_rate', 'batch_dimension']\n",
    "fig, ax = plot_objective(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1, ax = plot_evaluations(result=search_result, dimension_names=dim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
